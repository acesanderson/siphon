{
  "message": {
    "message_type": "text",
    "role": "assistant",
    "content": "# Siphon\n\nSiphon is a data ingestion and enrichment pipeline designed to process content from heterogeneous sources, such as local files and online URLs, and standardize it into a unified, queryable format. The system's primary output is a `ProcessedContent` object, which encapsulates a source's canonical URI, its LLM-ready context, and AI-generated metadata. The architecture is designed for extensibility and operational resilience, featuring a local-first caching mechanism that allows for offline operation with eventual consistency to a central PostgreSQL database.\n\nThe core processing pipeline consists of four main stages. First, an input source string (e.g., a file path or URL) is parsed by a factory method that selects a source-specific URI handler. This produces a typed `URI` object, which contains a `SourceType` enum that dictates the subsequent processing path. Second, this `URI` object is passed to a `Context` factory, which dynamically selects the corresponding `Context` class to retrieve and standardize the raw content into a clean text format (`llm_context`) and extract source-specific metadata. Third, the resulting `Context` object is used to generate enrichments via LLM calls. A `SyntheticData` factory dispatches this task to a source-specific class, which generates a title, description, and summary using predefined prompts and the `conduit` library for asynchronous model interaction. Finally, the `URI`, `Context`, and `SyntheticData` objects are aggregated into a final `ProcessedContent` object for persistence.\n\nThe persistence layer is architected around a primary PostgreSQL database and a local SQLite fallback cache. This design ensures that the data ingestion process is not interrupted by network unavailability. When content is processed, it is first written to the local SQLite database. An attempt is then made to write to PostgreSQL. If this fails, the content is added to a synchronization queue managed by the `FallbackCacheManager`. This manager opportunistically processes the queue, writing pending items to PostgreSQL when the connection is restored. This architecture prioritizes data capture resilience over immediate data consistency. Data in PostgreSQL is stored in a JSONB column, with separate `pgvector` columns for embeddings to facilitate semantic search.\n\nThe codebase includes several distinct modules to support the main pipeline. The `siphon/uri/`, `siphon/context/`, and `siphon/synthetic_data/` directories use a dynamic loading pattern based on the `SourceType` enum. To add support for a new source, one must implement corresponding URI, Context, and SyntheticData classes and place them in the correct directories; the system will automatically discover and integrate them. The `siphon/ingestion/` module contains the low-level logic for retrieving data from each source type (e.g., audio transcription, GitHub repository flattening, YouTube transcript downloading). The `siphon/collections/` module provides a fluent, monadic query interface (`SiphonQuery`) for interacting with the persisted data library. A suite of auxiliary CLI scripts in `siphon/scripts/` provides standalone functionality that builds upon or complements the core ingestion pipeline, such as code flattening, audio recording, and multi-document synthesis."
  },
  "request": {
    "model": "gemini-2.5-pro-preview-05-06",
    "messages": [
      {
        "message_type": "text",
        "role": "user",
        "content": "Write a technical README for this codebase. Follow these requirements strictly:\n\n1. NO icons, emojis, or decorative elements\n2. NO sections for Contributing, License, Acknowledgments, or similar boilerplate\n3. NO installation instructions unless the setup is non-trivial\n4. NO marketing language or enthusiasm\n\nStructure:\n- Start with a direct technical description (2-4 paragraphs)\n- Explain what the code does, how it works architecturally, and why key decisions were made\n- Use precise technical terminology\n- Focus on information density - every sentence should add value\n\nThe description should cover:\n- The core purpose and role in the larger system (if applicable)\n- Main technical phases/components and how they interact\n- Key implementation details (concurrency model, error handling, data flow)\n- Rationale for non-obvious technical decisions (e.g., \"uses X instead of Y because Z constraint\")\n- Any footguns, limitations, or \"tried this approach first but failed because...\" context\n- Dependencies on or relationships with other parts of the system\n- Intended use case and operational context\n\nWrite in a matter-of-fact tone as if documenting for your future self. Assume the reader is technically competent. Be specific about what the code actually does rather than what problems it theoretically solves.\n\nDo not include: usage examples, quickstart guides, or feature lists unless the code has genuine complexity that requires explanation.\n\nHere's the code project:\n<code>\n<?xml version=\"1.0\" ?>\n<project name=\"siphon-project\">\n  <directory_tree>\n    <directory name=\"siphon-project\">\n      <directory name=\"src\">\n        <directory name=\"siphon\">\n          <file name=\"__init__.py\" path=\"src/siphon/__init__.py\"/>\n          <directory name=\"scripts\">\n            <file name=\"peek_cli.py\" path=\"src/siphon/scripts/peek_cli.py\"/>\n            <file name=\"play_cli.py\" path=\"src/siphon/scripts/play_cli.py\"/>\n            <file name=\"obsidian_cli.py\" path=\"src/siphon/scripts/obsidian_cli.py\"/>\n            <file name=\"youtube_cli.py\" path=\"src/siphon/scripts/youtube_cli.py\"/>\n            <file name=\"ocr_cli.py\" path=\"src/siphon/scripts/ocr_cli.py\"/>\n            <file name=\"survey_cli.py\" path=\"src/siphon/scripts/survey_cli.py\"/>\n            <file name=\"record_cli.py\" path=\"src/siphon/scripts/record_cli.py\"/>\n            <file name=\"test_sourcetype.py\" path=\"src/siphon/scripts/test_sourcetype.py\"/>\n            <file name=\"flatten_cli.py\" path=\"src/siphon/scripts/flatten_cli.py\"/>\n            <file name=\"siphonserver_cli.py\" path=\"src/siphon/scripts/siphonserver_cli.py\"/>\n            <file name=\"summarize_cli.py\" path=\"src/siphon/scripts/summarize_cli.py\"/>\n            <directory name=\"flatten\">\n              <file name=\"generate_docs.py\" path=\"src/siphon/scripts/flatten/generate_docs.py\"/>\n            </directory>\n          </directory>\n          <directory name=\"cli\">\n            <file name=\"cli_params.py\" path=\"src/siphon/cli/cli_params.py\"/>\n            <file name=\"siphon_cli.py\" path=\"src/siphon/cli/siphon_cli.py\"/>\n            <file name=\"implicit_input.py\" path=\"src/siphon/cli/implicit_input.py\"/>\n          </directory>\n          <directory name=\"tests\">\n            <file name=\"test_context.py\" path=\"src/siphon/tests/test_context.py\"/>\n            <file name=\"test_youtube.py\" path=\"src/siphon/tests/test_youtube.py\"/>\n            <file name=\"test_display.py\" path=\"src/siphon/tests/test_display.py\"/>\n            <file name=\"test_minimal_siphon.py\" path=\"src/siphon/tests/test_minimal_siphon.py\"/>\n            <file name=\"test_uri.py\" path=\"src/siphon/tests/test_uri.py\"/>\n            <file name=\"test_postgres.py\" path=\"src/siphon/tests/test_postgres.py\"/>\n            <file name=\"test_local.py\" path=\"src/siphon/tests/test_local.py\"/>\n            <file name=\"test_integration.py\" path=\"src/siphon/tests/test_integration.py\"/>\n            <directory name=\"audio\">\n              <file name=\"test_audio.py\" path=\"src/siphon/tests/audio/test_audio.py\"/>\n              <file name=\"test_transcribe.py\" path=\"src/siphon/tests/audio/test_transcribe.py\"/>\n              <file name=\"test_diarize.py\" path=\"src/siphon/tests/audio/test_diarize.py\"/>\n            </directory>\n            <directory name=\"fixtures\">\n              <file name=\"assets.py\" path=\"src/siphon/tests/fixtures/assets.py\"/>\n              <file name=\"example_files.py\" path=\"src/siphon/tests/fixtures/example_files.py\"/>\n              <file name=\"example_ProcessedContent.py\" path=\"src/siphon/tests/fixtures/example_ProcessedContent.py\"/>\n            </directory>\n            <directory name=\"data\">\n              <file name=\"test_URI.py\" path=\"src/siphon/tests/data/test_URI.py\"/>\n            </directory>\n          </directory>\n          <directory name=\"collections\">\n            <file name=\"test_collections.py\" path=\"src/siphon/collections/test_collections.py\"/>\n            <directory name=\"corpus\">\n              <file name=\"__init__.py\" path=\"src/siphon/collections/corpus/__init__.py\"/>\n              <file name=\"siphon_corpus.py\" path=\"src/siphon/collections/corpus/siphon_corpus.py\"/>\n              <file name=\"sourdough.py\" path=\"src/siphon/collections/corpus/sourdough.py\"/>\n            </directory>\n            <directory name=\"query\">\n              <file name=\"snapshot.py\" path=\"src/siphon/collections/query/snapshot.py\"/>\n              <file name=\"siphon_query.py\" path=\"src/siphon/collections/query/siphon_query.py\"/>\n            </directory>\n          </directory>\n          <directory name=\"logs\">\n            <file name=\"logging_config.py\" path=\"src/siphon/logs/logging_config.py\"/>\n          </directory>\n          <directory name=\"database\">\n            <directory name=\"postgres\">\n              <file name=\"migrate.py\" path=\"src/siphon/database/postgres/migrate.py\"/>\n              <file name=\"PGRES_processed_content.py\" path=\"src/siphon/database/postgres/PGRES_processed_content.py\"/>\n            </directory>\n            <directory name=\"sqlite\">\n              <file name=\"manual_sync.py\" path=\"src/siphon/database/sqlite/manual_sync.py\"/>\n              <file name=\"snapshot_sqlite3.py\" path=\"src/siphon/database/sqlite/snapshot_sqlite3.py\"/>\n            </directory>\n            <directory name=\"chroma\">\n              <file name=\"chroma_curate.py\" path=\"src/siphon/database/chroma/chroma_curate.py\"/>\n            </directory>\n            <directory name=\"pgvector\">\n              <file name=\"vector_db.py\" path=\"src/siphon/database/pgvector/vector_db.py\"/>\n            </directory>\n            <directory name=\"fallback\">\n              <file name=\"fallback_manager.py\" path=\"src/siphon/database/fallback/fallback_manager.py\"/>\n              <file name=\"sync_queue.py\" path=\"src/siphon/database/fallback/sync_queue.py\"/>\n            </directory>\n          </directory>\n          <directory name=\"data\">\n            <file name=\"context.py\" path=\"src/siphon/data/context.py\"/>\n            <file name=\"processed_content_display.py\" path=\"src/siphon/data/processed_content_display.py\"/>\n            <file name=\"synthetic_data.py\" path=\"src/siphon/data/synthetic_data.py\"/>\n            <file name=\"processed_content.py\" path=\"src/siphon/data/processed_content.py\"/>\n            <file name=\"context_call.py\" path=\"src/siphon/data/context_call.py\"/>\n            <file name=\"uri.py\" path=\"src/siphon/data/uri.py\"/>\n            <directory name=\"type-definitions\">\n              <file name=\"uri_schemes.py\" path=\"src/siphon/data/type-definitions/uri_schemes.py\"/>\n              <file name=\"extensions.py\" path=\"src/siphon/data/type-definitions/extensions.py\"/>\n              <file name=\"source_type.py\" path=\"src/siphon/data/type-definitions/source_type.py\"/>\n            </directory>\n            <directory name=\"type_definitions\">\n              <file name=\"uri_schemes.py\" path=\"src/siphon/data/type_definitions/uri_schemes.py\"/>\n              <file name=\"extensions.py\" path=\"src/siphon/data/type_definitions/extensions.py\"/>\n              <file name=\"source_type.py\" path=\"src/siphon/data/type_definitions/source_type.py\"/>\n            </directory>\n          </directory>\n          <directory name=\"context\">\n            <file name=\"context_classes.py\" path=\"src/siphon/context/context_classes.py\"/>\n            <directory name=\"classes\">\n              <file name=\"image_context.py\" path=\"src/siphon/context/classes/image_context.py\"/>\n              <file name=\"video_context.py\" path=\"src/siphon/context/classes/video_context.py\"/>\n              <file name=\"youtube_context.py\" path=\"src/siphon/context/classes/youtube_context.py\"/>\n              <file name=\"github_context.py\" path=\"src/siphon/context/classes/github_context.py\"/>\n              <file name=\"obsidian_context.py\" path=\"src/siphon/context/classes/obsidian_context.py\"/>\n              <file name=\"audio_context.py\" path=\"src/siphon/context/classes/audio_context.py\"/>\n              <file name=\"doc_context.py\" path=\"src/siphon/context/classes/doc_context.py\"/>\n              <file name=\"text_context.py\" path=\"src/siphon/context/classes/text_context.py\"/>\n              <file name=\"email_context.py\" path=\"src/siphon/context/classes/email_context.py\"/>\n              <file name=\"drive_context.py\" path=\"src/siphon/context/classes/drive_context.py\"/>\n              <file name=\"article_context.py\" path=\"src/siphon/context/classes/article_context.py\"/>\n            </directory>\n          </directory>\n          <directory name=\"main\">\n            <file name=\"siphon.py\" path=\"src/siphon/main/siphon.py\"/>\n          </directory>\n          <directory name=\"ingestion\">\n            <file name=\"online_context.py\" path=\"src/siphon/ingestion/online_context.py\"/>\n            <file name=\"file_context.py\" path=\"src/siphon/ingestion/file_context.py\"/>\n            <file name=\"retrieve.py\" path=\"src/siphon/ingestion/retrieve.py\"/>\n            <directory name=\"obsidian\">\n              <file name=\"vault.py\" path=\"src/siphon/ingestion/obsidian/vault.py\"/>\n            </directory>\n            <directory name=\"article\">\n              <file name=\"retrieve_article.py\" path=\"src/siphon/ingestion/article/retrieve_article.py\"/>\n              <file name=\"download_article.py\" path=\"src/siphon/ingestion/article/download_article.py\"/>\n            </directory>\n            <directory name=\"youtube\">\n              <file name=\"retrieve_youtube.py\" path=\"src/siphon/ingestion/youtube/retrieve_youtube.py\"/>\n              <file name=\"download_youtube_transcript.py\" path=\"src/siphon/ingestion/youtube/download_youtube_transcript.py\"/>\n              <file name=\"format_youtube.py\" path=\"src/siphon/ingestion/youtube/format_youtube.py\"/>\n            </directory>\n            <directory name=\"audio\">\n              <file name=\"transcribe.py\" path=\"src/siphon/ingestion/audio/transcribe.py\"/>\n              <file name=\"clean.py\" path=\"src/siphon/ingestion/audio/clean.py\"/>\n              <file name=\"local_transcript.py\" path=\"src/siphon/ingestion/audio/local_transcript.py\"/>\n              <file name=\"format.py\" path=\"src/siphon/ingestion/audio/format.py\"/>\n              <file name=\"combine.py\" path=\"src/siphon/ingestion/audio/combine.py\"/>\n              <file name=\"convert.py\" path=\"src/siphon/ingestion/audio/convert.py\"/>\n              <file name=\"diarize.py\" path=\"src/siphon/ingestion/audio/diarize.py\"/>\n              <file name=\"retrieve_audio.py\" path=\"src/siphon/ingestion/audio/retrieve_audio.py\"/>\n              <file name=\"openai_transcript.py\" path=\"src/siphon/ingestion/audio/openai_transcript.py\"/>\n            </directory>\n            <directory name=\"image\">\n              <file name=\"describe_image_with_cloud.py\" path=\"src/siphon/ingestion/image/describe_image_with_cloud.py\"/>\n              <file name=\"describe_image_with_ollama.py\" path=\"src/siphon/ingestion/image/describe_image_with_ollama.py\"/>\n              <file name=\"retrieve_image.py\" path=\"src/siphon/ingestion/image/retrieve_image.py\"/>\n            </directory>\n            <directory name=\"googledrive\">\n              <file name=\"retrieve_google_doc.py\" path=\"src/siphon/ingestion/googledrive/retrieve_google_doc.py\"/>\n            </directory>\n            <directory name=\"github\">\n              <file name=\"flatten_xml.py\" path=\"src/siphon/ingestion/github/flatten_xml.py\"/>\n              <file name=\"flatten_directory.py\" path=\"src/siphon/ingestion/github/flatten_directory.py\"/>\n              <file name=\"flatten_url.py\" path=\"src/siphon/ingestion/github/flatten_url.py\"/>\n              <file name=\"retrieve_github.py\" path=\"src/siphon/ingestion/github/retrieve_github.py\"/>\n            </directory>\n            <directory name=\"arxiv\">\n              <file name=\"retrieve_arxiv.py\" path=\"src/siphon/ingestion/arxiv/retrieve_arxiv.py\"/>\n            </directory>\n          </directory>\n          <directory name=\"prompts\">\n            <file name=\"synthetic_data_prompts.py\" path=\"src/siphon/prompts/synthetic_data_prompts.py\"/>\n          </directory>\n          <directory name=\"uri\">\n            <file name=\"uri_classes.py\" path=\"src/siphon/uri/uri_classes.py\"/>\n            <directory name=\"classes\">\n              <file name=\"doc_uri.py\" path=\"src/siphon/uri/classes/doc_uri.py\"/>\n              <file name=\"obsidian_uri.py\" path=\"src/siphon/uri/classes/obsidian_uri.py\"/>\n              <file name=\"email_uri.py\" path=\"src/siphon/uri/classes/email_uri.py\"/>\n              <file name=\"github_uri.py\" path=\"src/siphon/uri/classes/github_uri.py\"/>\n              <file name=\"image_uri.py\" path=\"src/siphon/uri/classes/image_uri.py\"/>\n              <file name=\"video_uri.py\" path=\"src/siphon/uri/classes/video_uri.py\"/>\n              <file name=\"drive_uri.py\" path=\"src/siphon/uri/classes/drive_uri.py\"/>\n              <file name=\"audio_uri.py\" path=\"src/siphon/uri/classes/audio_uri.py\"/>\n              <file name=\"youtube_uri.py\" path=\"src/siphon/uri/classes/youtube_uri.py\"/>\n              <file name=\"article_uri.py\" path=\"src/siphon/uri/classes/article_uri.py\"/>\n              <file name=\"text_uri.py\" path=\"src/siphon/uri/classes/text_uri.py\"/>\n            </directory>\n          </directory>\n          <directory name=\"synthetic_data\">\n            <file name=\"synthetic_data_classes.py\" path=\"src/siphon/synthetic_data/synthetic_data_classes.py\"/>\n            <directory name=\"classes\">\n              <file name=\"email_synthetic_data.py\" path=\"src/siphon/synthetic_data/classes/email_synthetic_data.py\"/>\n              <file name=\"github_synthetic_data.py\" path=\"src/siphon/synthetic_data/classes/github_synthetic_data.py\"/>\n              <file name=\"obsidian_synthetic_data.py\" path=\"src/siphon/synthetic_data/classes/obsidian_synthetic_data.py\"/>\n              <file name=\"article_synthetic_data.py\" path=\"src/siphon/synthetic_data/classes/article_synthetic_data.py\"/>\n              <file name=\"audio_synthetic_data.py\" path=\"src/siphon/synthetic_data/classes/audio_synthetic_data.py\"/>\n              <file name=\"youtube_synthetic_data.py\" path=\"src/siphon/synthetic_data/classes/youtube_synthetic_data.py\"/>\n              <file name=\"video_synthetic_data.py\" path=\"src/siphon/synthetic_data/classes/video_synthetic_data.py\"/>\n              <file name=\"image_synthetic_data.py\" path=\"src/siphon/synthetic_data/classes/image_synthetic_data.py\"/>\n              <file name=\"text_synthetic_data.py\" path=\"src/siphon/synthetic_data/classes/text_synthetic_data.py\"/>\n              <file name=\"drive_synthetic_data.py\" path=\"src/siphon/synthetic_data/classes/drive_synthetic_data.py\"/>\n              <file name=\"doc_synthetic_data.py\" path=\"src/siphon/synthetic_data/classes/doc_synthetic_data.py\"/>\n            </directory>\n          </directory>\n          <directory name=\"enrich\">\n            <file name=\"generate_title.py\" path=\"src/siphon/enrich/generate_title.py\"/>\n            <file name=\"enrich.py\" path=\"src/siphon/enrich/enrich.py\"/>\n            <file name=\"generate_summary.py\" path=\"src/siphon/enrich/generate_summary.py\"/>\n            <file name=\"test_runner.py\" path=\"src/siphon/enrich/test_runner.py\"/>\n            <file name=\"generate_description.py\" path=\"src/siphon/enrich/generate_description.py\"/>\n          </directory>\n        </directory>\n      </directory>\n    </directory>\n  </directory_tree>\n  <file_contents>\n    <file path=\"src/siphon/__init__.py\">from siphon.main.siphon import siphon\n\n__all__ = [&quot;siphon&quot;]\n</file>\n    <file path=\"src/siphon/scripts/peek_cli.py\">import argparse\nfrom pathlib import Path\n\n\ndef display_image_file(filename: str | Path):\n    &quot;&quot;&quot;\n    Display a base64-encoded image using chafa.\n    Your mileage may vary depending on the terminal and chafa version.\n    &quot;&quot;&quot;\n    # Coerce to Path\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    import subprocess, base64, os\n\n    if not filename.exists():\n        raise FileNotFoundError(f&quot;File {filename} does not exist.&quot;)\n\n    # Read binary image file and convert to base64\n    with open(filename, &quot;rb&quot;) as file:  # Note the &quot;rb&quot; for binary mode\n        image_data = file.read()\n\n    try:\n        cmd = [&quot;chafa&quot;, &quot;-&quot;]\n\n        # If in tmux or SSH, force text mode for consistency\n        if (\n            os.environ.get(&quot;TMUX&quot;)\n            or os.environ.get(&quot;SSH_CLIENT&quot;)\n            or os.environ.get(&quot;SSH_CONNECTION&quot;)\n        ):\n            cmd.extend([&quot;--format&quot;, &quot;symbols&quot;, &quot;--symbols&quot;, &quot;block&quot;])\n        process = subprocess.Popen(cmd, stdin=subprocess.PIPE)\n        process.communicate(input=image_data)\n    except Exception as e:\n        print(f&quot;Error displaying image: {e}&quot;)\n\n\ndef test_display_image_file():\n    from siphon.tests.fixtures.example_files import example_image_file\n\n    display_image_file(example_image_file)\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=&quot;Display a base64-encoded image using chafa.&quot;\n    )\n    parser.add_argument(\n        &quot;filename&quot;, help=&quot;Path to the file containing the base64 image content.&quot;\n    )\n    args = parser.parse_args()\n\n    filename = args.filename\n\n    display_image_file(filename)\n\n\nif __name__ == &quot;__main__&quot;:\n    # test_display_image_file()\n    main()\n</file>\n    <file path=\"src/siphon/scripts/play_cli.py\">from pathlib import Path\nfrom pydub import AudioSegment\nfrom pydub.playback import play\nimport argparse, sys\n\n\ndef play_mp3(mp3_file_path: str | Path):\n    &quot;&quot;&quot;\n    Play an MP3 file using pydub.\n    &quot;&quot;&quot;\n    # Load the MP3 file\n    audio = AudioSegment.from_mp3(mp3_file_path)\n    # Play the audio\n    play(audio)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=&quot;Play an MP3 file.&quot;)\n    parser.add_argument(&quot;mp3_file&quot;, type=str, help=&quot;Path to the MP3 file to play.&quot;)\n    args = parser.parse_args()\n    # Detect if no input; if so, print help message and exit\n    if len(sys.argv) == 1:\n        parser.print_help(sys.stderr)\n        sys.exit(1)\n\n    mp3_file_path = Path(args.mp3_file)\n    if not mp3_file_path.is_file():\n        print(f&quot;Error: The file '{mp3_file_path}' does not exist.&quot;)\n        return\n    if mp3_file_path.suffix != &quot;.mp3&quot;:\n        raise FileError(f&quot;Error: The file '{mp3_file_path}' is not an MP3 file.&quot;)\n    play_mp3(mp3_file_path)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</file>\n    <file path=\"src/siphon/scripts/obsidian_cli.py\">&quot;&quot;&quot;\nObsidian as a SourceType implementation is pending; in the meanwhile a convenience (non-persistant) CLI for Obsidian is provided.\nImmediate use:\n- return context from a set of daily notes for a given date range\n&quot;&quot;&quot;\n\nfrom pathlib import Path\nimport argparse\n\nobsidian_vault = Path(&quot;~/MorphyMobile&quot;).expanduser()\n\n# Glob all date notes (filename: YYYY-MM-DD.md) in the vault\ndate_notes = sorted(obsidian_vault.glob(&quot;202[0-9]-[0-1][0-9]-[0-3][0-9].md&quot;))\n\n\ndef get_date_notes(start_date, end_date) -&gt; str:\n    &quot;&quot;&quot;Get date notes between start_date and end_date (inclusive).&quot;&quot;&quot;\n    selected_notes = []\n    for note in date_notes:\n        note_date_str = note.stem  # Get the filename without extension\n        if start_date &lt;= note_date_str &lt;= end_date:\n            selected_notes.append(note)\n    llm_context = &quot;&quot;\n    for note in selected_notes:\n        llm_context += f&quot;\\n\\n# {note.stem}\\n&quot;\n        llm_context += note.read_text()\n    return llm_context\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=&quot;Fetch Obsidian date notes for a given date range.&quot;\n    )\n    # Print usage example\n    parser.epilog = &quot;Example usage: python obsidian_cli.py 2023-10-01 2023-10-07&quot;\n    parser.add_argument(&quot;start_date&quot;, type=str, help=&quot;Start date in YYYY-MM-DD format&quot;)\n    parser.add_argument(&quot;end_date&quot;, type=str, help=&quot;End date in YYYY-MM-DD format&quot;)\n    args = parser.parse_args()\n\n    context = get_date_notes(args.start_date, args.end_date)\n    print(context)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</file>\n    <file path=\"src/siphon/scripts/youtube_cli.py\">&quot;&quot;&quot;\nYouTube CLI tool for handling playlists and channels.\nExtends Siphon functionality without modifying core YouTubeURI class.\n\nUsage:\n   # Regular video (existing functionality)\n   python scripts/youtube_cli.py &quot;https://youtube.com/watch?v=abc123&quot;\n\n   # Playlist (siphon all videos, return combined transcript)\n   python scripts/youtube_cli.py &quot;https://youtube.com/playlist?list=PLxxx&quot;\n\n   # Channel (return video URL list)\n   python scripts/youtube_cli.py &quot;https://youtube.com/@channelname&quot;\n&quot;&quot;&quot;\n\nimport argparse\nimport os\nimport re\nimport sys\nimport time\nfrom googleapiclient.discovery import build\nfrom siphon.main.siphon import siphon\n\n\ndef get_youtube_service():\n    &quot;&quot;&quot;Initialize YouTube Data API client.&quot;&quot;&quot;\n    api_key = os.getenv(&quot;YOUTUBE_API_KEY&quot;)\n    if not api_key:\n        print(&quot;Error: YOUTUBE_API_KEY environment variable not set&quot;)\n        sys.exit(1)\n\n    # NOTE: May need to handle API client initialization errors\n    return build(&quot;youtube&quot;, &quot;v3&quot;, developerKey=api_key)\n\n\ndef extract_playlist_id(url):\n    &quot;&quot;&quot;Extract playlist ID from YouTube playlist URL.&quot;&quot;&quot;\n    # NOTE: This regex might not catch all playlist URL variations\n    match = re.search(r&quot;[?&amp;]list=([a-zA-Z0-9_-]+)&quot;, url)\n    return match.group(1) if match else None\n\n\ndef extract_channel_info(url):\n    &quot;&quot;&quot;Extract channel identifier from various YouTube channel URL formats.&quot;&quot;&quot;\n    # NOTE: YouTube channel URL formats may change - these patterns might need updates\n\n    # @username format\n    if &quot;@&quot; in url:\n        match = re.search(r&quot;@([a-zA-Z0-9_-]+)&quot;, url)\n        return (&quot;username&quot;, match.group(1)) if match else (None, None)\n\n    # /channel/UC... format\n    if &quot;/channel/&quot; in url:\n        match = re.search(r&quot;/channel/([a-zA-Z0-9_-]+)&quot;, url)\n        return (&quot;channel_id&quot;, match.group(1)) if match else (None, None)\n\n    # /c/customname or /user/username formats\n    if &quot;/c/&quot; in url or &quot;/user/&quot; in url:\n        match = re.search(r&quot;/(?:c|user)/([a-zA-Z0-9_-]+)&quot;, url)\n        return (&quot;username&quot;, match.group(1)) if match else (None, None)\n\n    return (None, None)\n\n\ndef get_channel_uploads_playlist(service, channel_type, channel_value):\n    &quot;&quot;&quot;Get the uploads playlist ID for a channel.&quot;&quot;&quot;\n    try:\n        if channel_type == &quot;channel_id&quot;:\n            request = service.channels().list(part=&quot;contentDetails&quot;, id=channel_value)\n        else:  # username - need to search first to get channel ID\n            # Search for the channel to get the actual channel ID\n            search_request = service.search().list(\n                part=&quot;snippet&quot;, q=channel_value, type=&quot;channel&quot;, maxResults=1\n            )\n            search_response = search_request.execute()\n\n            if not search_response[&quot;items&quot;]:\n                print(f&quot;Error: Channel '{channel_value}' not found&quot;)\n                return None\n\n            channel_id = search_response[&quot;items&quot;][0][&quot;snippet&quot;][&quot;channelId&quot;]\n            request = service.channels().list(part=&quot;contentDetails&quot;, id=channel_id)\n\n        response = request.execute()\n\n        if not response[&quot;items&quot;]:\n            print(f&quot;Error: Channel not found&quot;)\n            return None\n\n        uploads_playlist = response[&quot;items&quot;][0][&quot;contentDetails&quot;][&quot;relatedPlaylists&quot;][\n            &quot;uploads&quot;\n        ]\n        return uploads_playlist\n\n    except Exception as e:\n        print(f&quot;Error fetching channel info: {e}&quot;)\n        return None\n\n\ndef get_playlist_videos(service, playlist_id, max_results=100):\n    &quot;&quot;&quot;Get video IDs from a playlist.&quot;&quot;&quot;\n    videos = []\n    next_page_token = None\n\n    while len(videos) &lt; max_results:\n        try:\n            request = service.playlistItems().list(\n                part=&quot;snippet&quot;,\n                playlistId=playlist_id,\n                maxResults=min(\n                    50, max_results - len(videos)\n                ),  # API max is 50 per request\n                pageToken=next_page_token,\n            )\n            response = request.execute()\n\n            for item in response[&quot;items&quot;]:\n                # NOTE: Video might be private/deleted - snippet might not have videoId\n                video_id = item[&quot;snippet&quot;][&quot;resourceId&quot;].get(&quot;videoId&quot;)\n                title = item[&quot;snippet&quot;][&quot;title&quot;]\n                if video_id:\n                    videos.append({&quot;id&quot;: video_id, &quot;title&quot;: title})\n\n            next_page_token = response.get(&quot;nextPageToken&quot;)\n            if not next_page_token:\n                break\n\n            time.sleep(1)  # Rate limiting\n\n        except Exception as e:\n            print(f&quot;Error fetching playlist videos: {e}&quot;)\n            break\n\n    return videos[:max_results]\n\n\ndef handle_playlist(url):\n    &quot;&quot;&quot;Process YouTube playlist - siphon all videos and return combined transcript.&quot;&quot;&quot;\n    playlist_id = extract_playlist_id(url)\n    if not playlist_id:\n        print(&quot;Error: Could not extract playlist ID from URL&quot;)\n        return\n\n    service = get_youtube_service()\n    videos = get_playlist_videos(service, playlist_id, max_results=20)\n\n    if not videos:\n        print(&quot;Error: No videos found in playlist&quot;)\n        return\n\n    print(f&quot;Processing {len(videos)} videos from playlist...&quot;)\n    combined_transcript = []\n\n    for video in videos:\n        video_url = f&quot;https://youtube.com/watch?v={video['id']}&quot;\n        print(f&quot;Processing: {video['title']}&quot;)\n\n        try:\n            processed_content = siphon(video_url)\n            separator = f&quot;=== {video['title']} ({video_url}) ===&quot;\n            combined_transcript.append(separator)\n            combined_transcript.append(processed_content.context)\n            combined_transcript.append(&quot;&quot;)  # Empty line between videos\n\n        except Exception as e:\n            print(f&quot;Error processing {video_url} - {video['title']}: {e}&quot;)\n            continue\n\n        time.sleep(1)  # Rate limiting\n\n    # Print combined result\n    print(&quot;\\n&quot; + &quot;=&quot; * 80)\n    print(&quot;COMBINED PLAYLIST TRANSCRIPT&quot;)\n    print(&quot;=&quot; * 80 + &quot;\\n&quot;)\n    print(&quot;\\n&quot;.join(combined_transcript))\n\n\ndef handle_channel(url):\n    &quot;&quot;&quot;Process YouTube channel - return list of video URLs.&quot;&quot;&quot;\n    channel_type, channel_value = extract_channel_info(url)\n    if not channel_type:\n        print(&quot;Error: Could not extract channel info from URL&quot;)\n        return\n\n    service = get_youtube_service()\n    uploads_playlist = get_channel_uploads_playlist(\n        service, channel_type, channel_value\n    )\n\n    if not uploads_playlist:\n        return\n\n    videos = get_playlist_videos(service, uploads_playlist, max_results=100)\n\n    print(f&quot;Found {len(videos)} videos in channel:&quot;)\n    for video in videos:\n        print(f&quot;https://youtube.com/watch?v={video['id']}&quot;)\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=&quot;YouTube playlist and channel processor&quot;\n    )\n    parser.add_argument(&quot;url&quot;, help=&quot;YouTube video, playlist, or channel URL&quot;)\n    args = parser.parse_args()\n\n    url = args.url.strip()\n    # Detect if no input; if so, print help and exit\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(0)\n\n    if &quot;playlist?list=&quot; in url:\n        handle_playlist(url)\n    elif any(x in url for x in [&quot;@&quot;, &quot;/channel/&quot;, &quot;/c/&quot;, &quot;/user/&quot;]):\n        handle_channel(url)\n    else:\n        # Regular video - just siphon it\n        try:\n            processed_content = siphon(url)\n            processed_content.pretty_print()\n        except Exception as e:\n            print(f&quot;Error processing video: {e}&quot;)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</file>\n    <file path=\"src/siphon/scripts/ocr_cli.py\">from PIL import Image\nfrom io import BytesIO\nimport argparse, pytesseract, base64\nfrom rich.console import Console\n\nconsole = Console()\n\n\ndef grab_image_from_clipboard() -&gt; str | None:\n    &quot;&quot;&quot;\n    Attempt to grab image from clipboard; return tuple of mime_type and base64.\n    &quot;&quot;&quot;\n    import os\n\n    if &quot;SSH_CLIENT&quot; in os.environ or &quot;SSH_TTY&quot; in os.environ:\n        console.print(&quot;Image paste not available over SSH.&quot;, style=&quot;red&quot;)\n        return\n\n    import warnings\n    from PIL import ImageGrab\n    import base64, io\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(&quot;ignore&quot;)  # Suppress PIL warnings\n        image = ImageGrab.grabclipboard()\n\n    if image:\n        buffer = io.BytesIO()\n        image.save(buffer, format=&quot;PNG&quot;)\n        img_base64 = base64.b64encode(buffer.getvalue()).decode()\n        # Save for next query\n        console.print(&quot;Image captured!&quot;, style=&quot;green&quot;)\n        # Build our ImageMessage\n        return img_base64\n    else:\n        console.print(&quot;No image detected.&quot;, style=&quot;red&quot;)\n        import sys\n\n        sys.exit()\n\n\ndef grab_image_from_file(image_file: str) -&gt; str | None:\n    &quot;&quot;&quot;\n    Attempt to grab image from a file; return base64 encoded string.\n    &quot;&quot;&quot;\n    import os\n\n    if not os.path.exists(image_file):\n        console.print(f&quot;File {image_file} does not exist.&quot;, style=&quot;red&quot;)\n        return None\n\n    with open(image_file, &quot;rb&quot;) as f:\n        img_bytes = f.read()\n        img_base64 = base64.b64encode(img_bytes).decode()\n        console.print(&quot;Image loaded from file!&quot;, style=&quot;green&quot;)\n        return img_base64\n\n\ndef extract_text_from_base64(base64_string):\n    # Remove data URL prefix if present\n    if base64_string.startswith(&quot;data:image&quot;):\n        base64_string = base64_string.split(&quot;,&quot;)[1]\n\n    # Decode base64 to bytes\n    image_bytes = base64.b64decode(base64_string)\n\n    # Convert bytes to PIL Image\n    image = Image.open(BytesIO(image_bytes))\n\n    # Extract text using pytesseract\n    text = pytesseract.image_to_string(image)\n\n    return text.strip()\n\n\ndef text_example():\n    from siphon.tests.fixtures.example_files import example_ocr_file\n\n    # Convert example_ocr_file (a path to a PNG file) to base64\n    with open(example_ocr_file, &quot;rb&quot;) as f:\n        base64_png = base64.b64encode(f.read()).decode()\n\n    # Usage\n    extracted_text = extract_text_from_base64(base64_png)\n    print(extracted_text)\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=&quot;OCR from clipboard or base64 string. Default is clipboard.&quot;\n    )\n    # add an imagefile positional required argument\n    parser.add_argument(\n        &quot;filename&quot;,\n        nargs=&quot;?&quot;,\n        help=&quot;Path to an image file to perform OCR on. If not provided, will use clipboard.&quot;,\n    )\n    args = parser.parse_args()\n\n    if args.filename:\n        image_content = grab_image_from_file(args.filename)\n    else:\n        image_content = grab_image_from_clipboard()\n    if image_content:\n        extracted_text = extract_text_from_base64(image_content)\n        print(extracted_text)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</file>\n    <file path=\"src/siphon/scripts/survey_cli.py\">&quot;&quot;&quot;\nResearch CLI - Multi-document synthesis and analysis tool.\n\nPerforms focused research across a collection of documents by:\n1. Extracting content relevant to a specific research question from each document\n2. Synthesizing findings into a unified strategic analysis\n\nPerfect for competitive intelligence, market research, or strategic analysis\nwhere you have multiple sources (transcripts, articles, reports, etc.) and\nneed to understand what they collectively say about a specific topic.\n\nUsage:\n   research_cli.py &quot;Company's AI strategy and competitive positioning&quot;\n   research_cli.py &quot;Product roadmap priorities&quot; --dir ./q3_meetings/\n   research_cli.py &quot;Market expansion plans&quot; --pattern &quot;*.md,*.pdf,*.txt&quot;\n\nThe tool uses async processing for speed and provides structured output\nsuitable for strategic decision-making. Requires documents to be in\ntext-readable format (.md, .txt, or other plain text files).\n\nTBD: have this use Siphon to arbitrarily convert multiple document formats; for the moment it's only .txt and .md files, so you need to manually create the set of resources.\nNOTE: we exclude Python and Jinja2 files by default, as they are not relevant to the research synthesis. The jinja2 file is where we might define a more complex prompt template for the contextual focus.\n&quot;&quot;&quot;\n\nfrom pathlib import Path\nfrom conduit.sync import Conduit, Model, Prompt, Verbosity, ConduitCache\nfrom conduit.batch import AsyncConduit, ModelAsync\nimport argparse\n\nModel._conduit_cache = ConduitCache()\nModelAsync._conduit_cache = ConduitCache()\n\nprompt1 = &quot;&quot;&quot;\nExtract all content relevant to the following contextual focus:\n\n&lt;contextual_focus&gt;\n{{contextual_focus}}\n&lt;/contextual_focus&gt;\n\nFrom the provided document, identify and extract:\n1. Direct statements, quotes, or claims related to this topic\n2. Supporting data, metrics, or evidence  \n3. Strategic decisions, initiatives, or plans\n4. Customer use cases, examples, or case studies\n5. Competitive positioning or market analysis\n6. Future roadmap items or announced developments\n\nFor each extracted item:\n- Provide the specific text or paraphrase\n- Note the source context (e.g., &quot;CEO statement&quot;, &quot;product demo&quot;, &quot;Q2 earnings call&quot;)\n- Indicate confidence level: Direct quote, Clear inference, or Implied from context\n\nGroup extractions by category and prioritize recent information and authoritative sources.\n\nExclude general company background, tangential mentions, and purely historical information unless strategically relevant.\n\nIf no relevant content is found, state &quot;No relevant content identified.&quot;\n\nHere's the document content:\n&lt;document_content&gt;\n{{document_content}}\n&lt;/document_content&gt;\n\nReturn ONLY your summary.\n&quot;&quot;&quot;\n\n\nprompt2 = &quot;&quot;&quot;\nBased on the following extracted summaries from multiple sources, create a unified analysis of:\n&lt;contextual_focus&gt;\n{{contextual_focus}}\n&lt;/contextual_focus&gt;\n\nEXTRACTED SUMMARIES:\n&lt;summaries&gt;\n{{summaries}}\n&lt;/summaries&gt;\n\nSynthesize this information into a coherent summary that addresses:\n- Key themes and strategic direction\n- Supporting evidence and data points\n- Notable contradictions or gaps between sources\n- Actionable insights and implications\n\nStructure your response with clear sections. Prioritize information from authoritative sources and recent developments. Where sources conflict or information is uncertain, note this explicitly.\n\nFocus on providing a comprehensive yet concise understanding suitable for strategic decision-making.\n&quot;&quot;&quot;\n\n\ndef research_directory(contextual_focus: str, paths: list[Path | str]) -&gt; str:\n    &quot;&quot;&quot;\n    Research synthesis across multiple documents in a directory.\n\n    Args:\n        contextual_focus (str): The research focus or question to guide the synthesis.\n        paths (list[Path | str]): List of document paths to analyze. Can be strings or Path objects.\n\n    Returns:\n        str: A synthesized summary of the research findings across the documents.\n    &quot;&quot;&quot;\n    # Coerce paths to Path objects if they are strings\n    paths = [Path(p) if isinstance(p, str) else p for p in paths]\n\n    # First conduit\n    prompt = Prompt(prompt1)\n\n    # generate async input variables\n    input_variables_list = []\n    for p in paths:\n        document_content = Path(p).read_text()\n        input_variables_list.append(\n            {\n                &quot;contextual_focus&quot;: contextual_focus,\n                &quot;document_content&quot;: document_content,\n            }\n        )\n\n    # Second conduit\n    model = ModelAsync(&quot;gemini&quot;)\n    conduit = AsyncConduit(\n        model=model,\n        prompt=prompt,\n    )\n    responses = conduit.run(\n        input_variables_list=input_variables_list,\n        verbose=Verbosity.SUMMARY,\n    )\n\n    # Combine responses into a single summary\n    model = Model(&quot;gemini2.5&quot;)\n    prompt = Prompt(prompt2)\n    conduit = Conduit(model=model, prompt=prompt)\n    response = conduit.run(\n        input_variables={\n            &quot;contextual_focus&quot;: contextual_focus,\n            &quot;summaries&quot;: &quot;\\n\\n&quot;.join([str(r.content) for r in responses]),\n        },\n        verbose=Verbosity.SUMMARY,\n    )\n\n    return str(response.content)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=&quot;Research synthesis across documents&quot;)\n    parser.add_argument(&quot;focus&quot;, help=&quot;Research focus/question&quot;)\n    parser.add_argument(&quot;--dir&quot;, default=&quot;.&quot;, help=&quot;Directory containing documents&quot;)\n    parser.add_argument(\n        &quot;--pattern&quot;, default=&quot;*.md,*.txt&quot;, help=&quot;File patterns to include&quot;\n    )\n    parser.add_argument(\n        &quot;--exclude&quot;, default=&quot;*.py,*.jinja2&quot;, help=&quot;File patterns to exclude&quot;\n    )\n\n    args = parser.parse_args()\n\n    # Your existing logic but with args.focus instead of hardcoded contextual_focus\n    contextual_focus = args.focus\n    patterns = args.pattern.split(&quot;,&quot;)\n\n    paths = []\n\n    # Handle include patterns\n    for pattern in patterns:\n        paths.extend(Path(args.dir).glob(pattern.strip()))\n\n    # Handle exclude patterns\n    exclude_patterns = args.exclude.split(&quot;,&quot;) if args.exclude else []\n    for exclude_pattern in exclude_patterns:\n        paths = [p for p in paths if not p.match(exclude_pattern.strip())]\n\n    if not paths:\n        print(\n            &quot;No documents found matching the specified inclusion / exclusion patterns.&quot;\n        )\n        return\n\n    print(f&quot;Researching focus: {contextual_focus}&quot;)\n    print(f&quot;Found {len(paths)} documents to analyze.&quot;)\n    summary = research_directory(contextual_focus, paths)\n    print(&quot;\\nSynthesis Summary:&quot;)\n    print(summary)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</file>\n    <file path=\"src/siphon/scripts/record_cli.py\">from datetime import datetime\nfrom pydub import AudioSegment\nimport pyaudio, wave, threading, os, sys\n\n\nclass AudioRecorder:\n    def __init__(self):\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 44100\n        self.recording = False\n        self.frames = []\n        self.audio = pyaudio.PyAudio()\n        self.stream = None\n\n    def start_recording(self):\n        &quot;&quot;&quot;Start recording audio&quot;&quot;&quot;\n        self.frames = []\n        self.recording = True\n\n        try:\n            self.stream = self.audio.open(\n                format=self.format,\n                channels=self.channels,\n                rate=self.rate,\n                input=True,\n                frames_per_buffer=self.chunk,\n            )\n\n            print(&quot;\ud83d\udd34 Recording started... Press Enter to stop recording.&quot;)\n\n            # Record in a separate thread\n            def record():\n                while self.recording:\n                    try:\n                        data = self.stream.read(self.chunk, exception_on_overflow=False)\n                        self.frames.append(data)\n                    except Exception as e:\n                        print(f&quot;Error during recording: {e}&quot;)\n                        break\n\n            record_thread = threading.Thread(target=record)\n            record_thread.daemon = True\n            record_thread.start()\n\n            return True\n\n        except Exception as e:\n            print(f&quot;Error starting recording: {e}&quot;)\n            print(&quot;Make sure your microphone is accessible and try again.&quot;)\n            return False\n\n    def stop_recording(self):\n        &quot;&quot;&quot;Stop recording audio&quot;&quot;&quot;\n        if self.recording:\n            self.recording = False\n            if self.stream:\n                self.stream.stop_stream()\n                self.stream.close()\n            print(&quot;\u23f9\ufe0f  Recording stopped.&quot;)\n            return True\n        return False\n\n    def save_recording(self, filename=None):\n        &quot;&quot;&quot;Save the recorded audio as WAV first, then convert to MP3&quot;&quot;&quot;\n        if not self.frames:\n            print(&quot;No audio data to save.&quot;)\n            return None\n\n        # Generate filename if not provided\n        if not filename:\n            timestamp = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)\n            filename = f&quot;recording_{timestamp}&quot;\n\n        # Ensure we have the full path\n        if not filename.endswith(&quot;.mp3&quot;):\n            filename += &quot;.mp3&quot;\n\n        # First save as WAV (temporary)\n        temp_wav = filename.replace(&quot;.mp3&quot;, &quot;.wav&quot;)\n\n        try:\n            # Save WAV file\n            wf = wave.open(temp_wav, &quot;wb&quot;)\n            wf.setnchannels(self.channels)\n            wf.setsampwidth(self.audio.get_sample_size(self.format))\n            wf.setframerate(self.rate)\n            wf.writeframes(b&quot;&quot;.join(self.frames))\n            wf.close()\n\n            # Convert WAV to MP3\n            audio_segment = AudioSegment.from_wav(temp_wav)\n            audio_segment.export(filename, format=&quot;mp3&quot;, bitrate=&quot;192k&quot;)\n\n            # Remove temporary WAV file\n            os.remove(temp_wav)\n\n            print(f&quot;\ud83d\udcbe Recording saved as: {filename}&quot;)\n            return filename\n\n        except Exception as e:\n            print(f&quot;Error saving recording: {e}&quot;)\n            return None\n\n    def cleanup(self):\n        &quot;&quot;&quot;Clean up audio resources&quot;&quot;&quot;\n        if self.stream:\n            self.stream.close()\n        self.audio.terminate()\n\n\ndef main():\n    print(&quot;\ud83c\udf99\ufe0f  AudioMemos Clone - Command Line Audio Recorder&quot;)\n    print(&quot;=&quot; * 50)\n\n    recorder = AudioRecorder()\n\n    try:\n        while True:\n            print(&quot;\\nOptions:&quot;)\n            print(&quot;1. Start new recording&quot;)\n            print(&quot;2. Exit&quot;)\n\n            choice = input(&quot;\\nSelect option (1-2): &quot;).strip()\n\n            if choice == &quot;1&quot;:\n                # Confirm recording start\n                confirm = input(&quot;Ready to start recording? (y/n): &quot;).strip().lower()\n                if confirm in [&quot;y&quot;, &quot;yes&quot;]:\n                    if recorder.start_recording():\n                        # Wait for user to press Enter to stop\n                        input()  # This blocks until user presses Enter\n\n                        if recorder.stop_recording():\n                            # Ask for filename\n                            custom_name = input(\n                                &quot;Enter filename (or press Enter for auto-generated): &quot;\n                            ).strip()\n                            filename = custom_name if custom_name else None\n\n                            saved_file = recorder.save_recording(filename)\n                            if saved_file:\n                                print(f&quot;\u2705 Successfully saved: {saved_file}&quot;)\n                    else:\n                        print(&quot;\u274c Failed to start recording.&quot;)\n                else:\n                    print(&quot;Recording cancelled.&quot;)\n\n            elif choice == &quot;2&quot;:\n                print(&quot;\ud83d\udc4b Goodbye!&quot;)\n                break\n            else:\n                print(&quot;Invalid option. Please select 1 or 2.&quot;)\n\n    except KeyboardInterrupt:\n        print(&quot;\\n\\n\u26a0\ufe0f  Recording interrupted by user.&quot;)\n        recorder.stop_recording()\n\n    except Exception as e:\n        print(f&quot;\\n\u274c An error occurred: {e}&quot;)\n\n    finally:\n        recorder.cleanup()\n\n\nif __name__ == &quot;__main__&quot;:\n    # Check if required packages are installed\n    try:\n        import pyaudio\n        import pydub\n    except ImportError as e:\n        print(&quot;\u274c Missing required packages!&quot;)\n        print(&quot;Please install required packages:&quot;)\n        print(&quot;pip install pyaudio pydub&quot;)\n        print(&quot;\\nFor macOS, you might also need:&quot;)\n        print(&quot;brew install portaudio&quot;)\n        print(&quot;brew install ffmpeg&quot;)\n        sys.exit(1)\n\n    main()\n</file>\n    <file path=\"src/siphon/scripts/test_sourcetype.py\">&quot;&quot;&quot;\nDev script for debugging individual sourcetype e2e processing as siphons.\n\nUsage:\n    python test_sourcetype.py &lt;sourcetype&gt;\n\nWhere sourcetype is the SourceType value, i.e. &quot;Text&quot;, &quot;Video&quot;, &quot;YouTube&quot;, etc.\n&quot;&quot;&quot;\n</file>\n    <file path=\"src/siphon/scripts/flatten_cli.py\">#!/usr/bin/env python3\n&quot;&quot;&quot;\nFlatten - CLI tool for converting GitHub repositories and local directories\ninto LLM-friendly XML format.\n\nUsage:\n    python Flatten.py .                                    # Flatten current directory\n    python Flatten.py /path/to/directory                   # Flatten specific directory\n    python Flatten.py https://github.com/owner/repo        # Flatten GitHub repository\n\nOptions:\n    -l, --last        Retrieve and display the last stored response\n    -d, --docs        Generate project README documentation\n    -p, --pretty      Pretty-print the output XML\n    -v, --verbose     Enable verbose readme generation (default is terse)\n&quot;&quot;&quot;\n\nimport argparse\nimport sys\nimport json\nfrom pathlib import Path\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\n\nfrom siphon.ingestion.github.flatten_directory import flatten_directory\nfrom siphon.ingestion.github.flatten_url import flatten_github_repo\nfrom conduit.sync import Response\n\n\nPREVIOUS_RESPONSE_FILE = Path(__file__).parent / &quot;flatten&quot; / &quot;previous_response.json&quot;\n\n\ndef store_response(response: Response) -&gt; None:\n    cache_dict = response.to_cache_dict()\n    with PREVIOUS_RESPONSE_FILE.open(&quot;w&quot;, encoding=&quot;utf-8&quot;) as f:\n        json.dump(cache_dict, f, indent=2)\n\n\ndef retrieve_response() -&gt; Response | None:\n    if not PREVIOUS_RESPONSE_FILE.exists():\n        return None\n    with PREVIOUS_RESPONSE_FILE.open(&quot;r&quot;, encoding=&quot;utf-8&quot;) as f:\n        cache_dict = json.load(f)\n    return Response.from_cache_dict(cache_dict)\n\n\ndef main():\n    &quot;&quot;&quot;Main CLI entry point for the Flatten tool.&quot;&quot;&quot;\n    parser = argparse.ArgumentParser(\n        description=&quot;Flatten a GitHub repo or local directory into LLM-friendly XML format&quot;,\n    )\n    # Basic use case: generate XML as LLM context\n    parser.add_argument(\n        &quot;target&quot;,\n        type=str,\n        help=&quot;GitHub URL, directory path, or '.' for current directory&quot;,\n    )\n    # Docs generation args\n    parser.add_argument(\n        &quot;-l&quot;,\n        &quot;--last&quot;,\n        action=&quot;store_true&quot;,\n        help=&quot;Retrieve and display the last stored response&quot;,\n    )\n    parser.add_argument(\n        &quot;-d&quot;,\n        &quot;--docs&quot;,\n        action=&quot;store_true&quot;,\n        help=&quot;Generate project README.&quot;,\n    )\n    parser.add_argument(\n        &quot;-v&quot;,\n        &quot;--verbose&quot;,\n        action=&quot;store_true&quot;,\n        help=&quot;Enable verbose readme. Default is terse.&quot;,\n    )\n    parser.add_argument(\n        &quot;-p&quot;,\n        &quot;--pretty&quot;,\n        action=&quot;store_true&quot;,\n        help=&quot;Pretty-print the output XML&quot;,\n    )\n\n    args = parser.parse_args()\n    target = args.target\n\n    # Detect no input, if no args provided, show help\n    # If no args provided, show help and exit\n    if len(sys.argv) == 1:\n        parser.print_help(sys.stderr)\n        sys.exit(1)\n\n    try:\n        if target.startswith(&quot;https://github.com/&quot;):\n            # Process GitHub repository\n            output = flatten_github_repo(target)\n        else:\n            # Process local directory (including &quot;.&quot;)\n            output = flatten_directory(target)\n\n    except Exception as e:\n        print(f&quot;Error processing target '{target}': {e}&quot;, file=sys.stderr)\n        sys.exit(1)\n\n    assert output is not None, &quot;No output generated from the target.&quot;\n\n    # Handle --last flag to retrieve previous response\n    if args.last:\n        previous_response = retrieve_response()\n        if previous_response is not None:\n            if args.pretty:\n                console = Console()\n                md = Markdown(previous_response.content)\n                console.print(md)\n                exit(0)\n            else:\n                print(previous_response.content)\n                exit(0)\n        else:\n            print(&quot;No previous response found.&quot;, file=sys.stderr)\n        return\n    # Handle --docs flag to generate README\n    if args.docs:\n        from siphon.scripts.flatten.generate_docs import generate_docs\n\n        prompt_type = &quot;verbose&quot; if args.verbose else &quot;terse&quot;\n        response = generate_docs(xml_string=output, prompt_type=prompt_type)\n        assert isinstance(response, Response), (\n            f&quot;Expected Response object from generate_docs, got {type(response)}&quot;\n        )\n        store_response(response)\n        if args.pretty:\n            console = Console()\n            md = Markdown(response.content)\n            console.print(md)\n        else:\n            print(response.content)\n\n    # Finally, print output.\n    else:\n        print(output)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</file>\n    <file path=\"src/siphon/scripts/siphonserver_cli.py\"># siphonserver = &quot;siphon.server.run:main&quot;\nraise NotImplementedError()\n\n</file>\n    <file path=\"src/siphon/scripts/summarize_cli.py\">&quot;&quot;&quot;\nIf you want quick summarization without persistence as a Siphon, use this script. This can use the same prompts as SyntheticData.\n&quot;&quot;&quot;\n</file>\n    <file path=\"src/siphon/scripts/flatten/generate_docs.py\">from conduit.sync import Conduit, Prompt, Model, Response, Verbosity, ConduitCache\nfrom pathlib import Path\nfrom typing import Literal\nfrom rich.console import Console\n\n# Constants\nTERSE_PROMPT_FILE = Path(__file__).parent / &quot;terse_docs_prompt.jinja2&quot;\nVERBOSE_PROMPT_FILE = Path(__file__).parent / &quot;verbose_docs_prompt.jinja2&quot;\nPREFERRED_MODEL = &quot;gemini2.5&quot;\nVERBOSITY = Verbosity.COMPLETE\nCONSOLE = Console()\n# Our singleton\nModel.console = CONSOLE\n\n\ndef generate_docs(\n    xml_string: str, prompt_type: Literal[&quot;terse&quot;, &quot;verbose&quot;]\n) -&gt; Response:\n    &quot;&quot;&quot;\n    Generate documentation from an XML string using a predefined prompt and model.\n    &quot;&quot;&quot;\n    # Switch between terse and verbose prompts\n    prompt_file = TERSE_PROMPT_FILE if prompt_type == &quot;terse&quot; else VERBOSE_PROMPT_FILE\n    # Build the conduit\n    prompt = Prompt(prompt_file.read_text())\n    model = Model(PREFERRED_MODEL)\n    conduit = Conduit(prompt=prompt, model=model)\n    response = conduit.run(input_variables={&quot;code&quot;: xml_string}, verbose=VERBOSITY)\n    # Validate response type\n    if not isinstance(response, Response):\n        raise ValueError(\n            f&quot;Expected response to be of type Response, got {type(response)}&quot;\n        )\n    return response\n</file>\n    <file path=\"src/siphon/cli/cli_params.py\">&quot;&quot;&quot;\nCLI parameters for the siphon tool.\n\nThese parameters can be provided via command-line arguments or programmatically.\n\nNote: there are two major sources of context: addressable content (source) and ephemeral content (content + content_type).\nAddressable content is content that can be retrieved from a file path or URL. This is specified via the `source` parameter.\nEphemeral content is content that is provided directly, such as pasted text or images from the clipboard. This is specified via the `content` and `content_type` parameters.\n\nEphemeral: existence precedes essence.\nAddressable: essence precedes existence.\n&quot;&quot;&quot;\n\nfrom siphon.data.uri import URI\nfrom typing import Optional, Literal\n\nfrom pydantic import BaseModel, Field, field_validator\n\n\nclass CLIParams(BaseModel):\n    # Addressable content, like files or URLs\n    source: Optional[str] = Field(\n        ..., description=&quot;Path to the file or URL to retrieve context from&quot;\n    )\n    # Ephemeral content, like pasted text or images from clipboard\n    content: Optional[str] = Field(\n        default=None, description=&quot;Direct content to use as context&quot;\n    )\n    content_type: Optional[str] = Field(\n        default=None,\n        description=&quot;Ephemeral sources, like stdin, pasted text, images from clipboard. This informs URI.&quot;,\n    )\n    # Request parameters\n    cache_options: Optional[Literal[&quot;c&quot;, &quot;u&quot;, &quot;r&quot;]] = Field(\n        default=&quot;c&quot;,\n        description=&quot;Special cache flags: 'u' (uncached, do not save), or 'r' (recache, save again). 'c' is default (cache the content).&quot;,\n    )\n    cloud: bool = Field(\n        default=False, description=&quot;Use cloud LLMs for synthetic data if applicable.&quot;\n    )\n    # User metadata\n    tags: list[str] = Field(\n        default_factory=list, description=&quot;Tags to apply to the context.&quot;\n    )\n\n    @field_validator(&quot;source&quot;)\n    def validate_source(cls, v):\n        &quot;&quot;&quot;\n        If we have a source, validate that it is a valid file path or URL.\n        &quot;&quot;&quot;\n        if v is None:\n            return v\n        uri = URI.from_source(v)\n        if not uri:\n            raise ValueError(f&quot;Invalid source: {v}. Must be a valid file path or URL.&quot;)\n        return v\n\n    @field_validator(&quot;content&quot;, &quot;content_type&quot;, mode=&quot;after&quot;)\n    def validate_content_and_source(cls, v, info):\n        &quot;&quot;&quot;\n        Ensure that either source is provided, or both content and content_type are provided.\n        &quot;&quot;&quot;\n        source = info.data.get(&quot;source&quot;)\n        content = info.data.get(&quot;content&quot;)\n        content_type = info.data.get(&quot;content_type&quot;)\n\n        if source and (content or content_type):\n            raise ValueError(&quot;Cannot provide both source and content/content_type.&quot;)\n        if not source and (not content or not content_type):\n            raise ValueError(\n                &quot;Must provide either source or both content and content_type.&quot;\n            )\n        return v\n</file>\n    <file path=\"src/siphon/cli/siphon_cli.py\">&quot;&quot;&quot;\nThis is the CLI entry point for siphon.\n\nThink of this as `cat` for LLMs. A simple command-line interface to convert files or URLs into context for LLMs.\n\nUsers can either provide a file path or a URL to retrieve the context.\nUsage:\n    python siphon_cli.py &lt;file_or_url&gt;\n\nThis script will determine if the input is a file path or a URL,\nand then retrieve the context + store it from the specified source.\n&quot;&quot;&quot;\n\nfrom siphon.main.siphon import siphon\nfrom siphon.cli.cli_params import CLIParams\nfrom siphon.cli.implicit_input import ImplicitInput\nfrom siphon.logs.logging_config import configure_logging\nimport argparse, logging, sys\nfrom typing import TYPE_CHECKING\n\nlogger = configure_logging(\n    level=logging.ERROR,\n    console=True,\n)\n\n\nif TYPE_CHECKING:\n    from conduit.message.imagemessage import ImageMessage\n\n\ndef grab_image_from_clipboard() -&gt; tuple | None:\n    &quot;&quot;&quot;\n    Attempt to grab image from clipboard; return tuple of mime_type and base64.\n    &quot;&quot;&quot;\n    import os\n\n    if &quot;SSH_CLIENT&quot; in os.environ or &quot;SSH_TTY&quot; in os.environ:\n        print(&quot;Image paste not available over SSH.&quot;)\n        return\n\n    import warnings\n    from PIL import ImageGrab\n    import base64, io, sys\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(&quot;ignore&quot;)  # Suppress PIL warnings\n        image = ImageGrab.grabclipboard()\n\n    if image:\n        buffer = io.BytesIO()\n        image.save(buffer, format=&quot;PNG&quot;)  # type: ignore[reportCallIssue]\n        img_base64 = base64.b64encode(buffer.getvalue()).decode()\n        # Save for next query\n        print(&quot;Image captured!&quot;)\n        # Build our ImageMessage\n        image_content = img_base64\n        mime_type = &quot;image/png&quot;\n        return mime_type, image_content\n    else:\n        print(&quot;No image detected.&quot;)\n        sys.exit()\n\n\ndef create_image_message(\n    combined_query: str, mime_type: str, image_content: str\n) -&gt; &quot;ImageMessage | None&quot;:\n    if not image_content or not mime_type:\n        return\n    role = &quot;user&quot;\n    text_content = combined_query\n\n    from conduit.message.imagemessage import ImageMessage\n\n    imagemessage = ImageMessage(\n        role=role,\n        text_content=text_content,\n        image_content=image_content,\n        mime_type=mime_type,\n    )\n    return imagemessage\n\n\ndef parse_tags(tags: str) -&gt; list[str]:\n    &quot;&quot;&quot;\n    Parse a comma-delimited string of tags into a list.\n    &quot;&quot;&quot;\n    if not tags:\n        return []\n    return [tag.strip() for tag in tags.split(&quot;,&quot;) if tag.strip()]\n\n\ndef main():\n    # If script was run with no arguments, attempt to grab implicit input.\n    if len(sys.argv) == 1:\n        logger.info(\n            &quot;No arguments provided. Attempting to grab from stdin or clipboard.&quot;\n        )\n        implicit_input = ImplicitInput.from_environment()\n        if implicit_input:\n            implicit_input.print()\n            sys.exit()\n\n    # Parse command-line arguments\n    parser = argparse.ArgumentParser(description=&quot;siphon file to LLM context&quot;)\n    parser.add_argument(\n        &quot;source&quot;, type=str, nargs=&quot;?&quot;, help=&quot;Path to the file to convert&quot;\n    )\n    parser.add_argument(\n        &quot;-C&quot;,\n        &quot;--cloud&quot;,\n        action=&quot;store_true&quot;,\n        help=&quot;Use cloud LLMs for synthetic data if applicable&quot;,\n    )\n    parser.add_argument(\n        &quot;-r&quot;,\n        &quot;--return_type&quot;,\n        type=str,\n        choices=[&quot;c&quot;, &quot;s&quot;, &quot;u&quot;],\n        default=&quot;s&quot;,\n        help=&quot;Type of data to return: 'c' (context), 's' (synthetic data), or 'u' (URI). Defaults to 'synthetic_data', i.e. a summary.&quot;,\n    )\n    parser.add_argument(\n        &quot;-c&quot;,\n        &quot;--cache-options&quot;,\n        type=str,\n        choices=[&quot;u&quot;, &quot;r&quot;, &quot;c&quot;],\n        default=&quot;c&quot;,\n        help=&quot;Special cache flags: 'u' (uncached, do not save), 'r' (recache, save again), or 'c' (cached, use existing cache if available). Defaults to 'c'.&quot;,\n    )\n    parser.add_argument(\n        &quot;-i&quot;,\n        &quot;--image&quot;,\n        action=&quot;store_true&quot;,\n        help=&quot;Grab an image from the clipboard and use it as context.&quot;,\n    )\n    parser.add_argument(\n        &quot;-t&quot;,\n        &quot;--tags&quot;,\n        type=str,\n        help=&quot;Comma-delimited list of tags. Useful for organizing content.&quot;,\n    )\n    parser.add_argument(\n        &quot;-p&quot;,\n        &quot;--pretty&quot;,\n        action=&quot;store_true&quot;,\n        help=&quot;Pretty print the output.&quot;,\n    )\n    parser.add_argument(\n        &quot;--raw&quot;,\n        action=&quot;store_true&quot;,\n        help=&quot;Output raw markdown without formatting.&quot;,\n    )\n    parser.add_argument(\n        &quot;--last&quot;,\n        &quot;-l&quot;,\n        action=&quot;store_true&quot;,\n        help=&quot;Load the last processed content from the cache.&quot;,\n    )\n    args = parser.parse_args()\n    args_dict = vars(args)\n    query = CLIParams(\n        source=args_dict[&quot;source&quot;],\n        cache_options=args_dict[&quot;cache_options&quot;],\n        cloud=args_dict[&quot;cloud&quot;],\n        tags=parse_tags(args_dict[&quot;tags&quot;]),\n    )\n    # Detect if no input; just print help and exit.\n    if len(sys.argv) == 1:\n        parser.print_help(sys.stderr)\n        sys.exit(1)\n\n    # Get ProcessedContent, by various means.\n    ## If we just want the last siphon:\n    if args.last:\n        from siphon.database.postgres.PGRES_processed_content import get_last_siphon\n\n        processed_content = get_last_siphon()\n        if not processed_content:\n            logger.error(&quot;No last processed content found in cache.&quot;)\n            sys.exit(1)\n\n        if args.pretty:\n            output = f&quot;# {processed_content.title}: {processed_content.id}\\n\\n{processed_content.summary}&quot;\n            from rich.markdown import Markdown\n            from rich.console import Console\n\n            console = Console()\n            markdown = Markdown(output)\n            console.print(markdown)\n            sys.exit()\n        else:\n            print(processed_content.summary)\n            sys.exit()\n\n    ## If we want to grab an image from the clipboard and process it:\n    if args.image:\n        raise NotImplementedError(\n            &quot;Image context is not yet implemented. Please use a file or URL.&quot;\n        )\n\n    ## If we want to process a Source:\n    if query:\n        processed_content = siphon(query)\n        output = f&quot;# {processed_content.title}: {processed_content.id}\\n\\n{processed_content.summary}&quot;\n        if args.pretty:\n            output = processed_content.summary\n            from rich.markdown import Markdown\n            from rich.console import Console\n\n            console = Console()\n            markdown = Markdown(output)\n            console.print(markdown)\n            sys.exit()\n        if args.return_type:\n            match args.return_type:\n                case &quot;c&quot;:\n                    output = processed_content.context\n                    print(output)\n                    sys.exit()\n                case &quot;s&quot;:\n                    output = processed_content.summary\n                    print(output)\n                    sys.exit()\n                case &quot;u&quot;:\n                    output = processed_content.uri\n                    print(output)\n                    sys.exit()\n                case _:\n                    logger.error(&quot;Invalid return type specified.&quot;)\n                    sys.exit(1)\n        if args.raw:\n            print(output)\n            sys.exit()\n        else:\n            from rich.markdown import Markdown\n            from rich.console import Console\n\n            console = Console()\n            markdown = Markdown(output)\n            console.print(markdown)\n            sys.exit()\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</file>\n    <file path=\"src/siphon/cli/implicit_input.py\">from pydantic import BaseModel, field_validator\nfrom typing import Literal\nfrom PIL.Image import Image\n\n\nclass ImplicitInput(BaseModel):\n    &quot;&quot;&quot;\n    Represents an implicit input for our cli.\n    This can be content from stdin, clipboard text, or clipboard image.\n    Use the `from_environment` class method to create an instance based on the current environment, if you detect there are no args passed to the cli.\n\n    Example:\n    ```python\n    if len(sys.argv) == 1:\n        implicit_input = ImplicitInput.from_environment()\n        if implicit_input:\n            implicit_input.print() # Prints the content of the implicit input for the user as confirmation.\n    ```\n    &quot;&quot;&quot;\n\n    type: Literal[&quot;stdin&quot;, &quot;clipboard_text&quot;, &quot;clipboard_image&quot;]\n    content: str | Image\n\n    class Config:\n        &quot;&quot;&quot;\n        Pydantic configuration to allow for arbitrary types.\n        This is necessary because the content can be either a string or a PIL Image.\n        &quot;&quot;&quot;\n\n        arbitrary_types_allowed = True\n\n    @field_validator(&quot;content&quot;)\n    def check_image(cls, v):\n        &quot;&quot;&quot;\n        Pydantic doesn't knkow how to validate PIL Image types, so we need to do it manually.\n        This ensures that the content is either a string or a PIL Image.\n        &quot;&quot;&quot;\n        if isinstance(v, str):\n            return v\n        if not isinstance(v, Image):\n            raise TypeError(&quot;content must be a str or PIL.Image.Image&quot;)\n        return v\n\n    # Convenience methods\n    def print(self) -&gt; None:\n        &quot;&quot;&quot;\n        Print the content of the implicit input.\n        If it's an image, it will print a placeholder message.\n        &quot;&quot;&quot;\n        match self.type:\n            case &quot;stdin&quot;:\n                snippet = str(self.content).strip().replace(&quot;\\n&quot;, &quot; &quot;)[:80]\n                print(f&quot;Implicit input from {self.type}: {snippet}...&quot;)\n            case &quot;clipboard_text&quot;:\n                snippet = str(self.content).strip().replace(&quot;\\n&quot;, &quot; &quot;)[:80]\n                print(f&quot;Implicit input from {self.type}: {snippet}...&quot;)\n            case &quot;clipboard_image&quot;:\n                import subprocess, io\n\n                buf = io.BytesIO()\n                assert isinstance(self.content, Image), (\n                    &quot;Content must be a PIL Image instance&quot;\n                )\n                self.content.save(buf, format=&quot;PNG&quot;)\n                buf.seek(0)\n                # Detect if chafa is available\n                try:\n                    subprocess.run(\n                        [&quot;chafa&quot;, &quot;--version&quot;],\n                        check=True,\n                        stdout=subprocess.DEVNULL,\n                        stderr=subprocess.DEVNULL,\n                    )\n                except FileNotFoundError:\n                    print(\n                        &quot;chafa is not installed. Please install it to display images in the terminal.&quot;\n                    )\n                    return\n                # Print to terminal\n                subprocess.run([&quot;chafa&quot;, &quot;-&quot;], input=buf.read(), check=True)\n            case _:\n                raise ValueError(f&quot;Unknown implicit input type: {self.type}&quot;)\n\n    # Constructor methods\n    @classmethod\n    def from_environment(cls) -&gt; &quot;ImplicitInput | None&quot;:\n        &quot;&quot;&quot;\n        Create an ImplicitInput instance from the environment.\n        This contains logic for the naked case, i.e. siphon is called with no arguments. Detects + captures implicit input.\n        First detect:\n        (1) if stdin was piped (e.g. `cat file.txt | siphon`): capture that and return it.\n        (2) detect if clipboard has text: if so, capture that and return it.\n        (3) detect if clipboard has an image: if so, capture that and return it.\n        (4) if none of the above, return None.\n        &quot;&quot;&quot;\n\n        stdin = cls._from_stdin()\n        if stdin:\n            return stdin\n        clipboard_text = cls._from_clipboard_text()\n        if clipboard_text:\n            return clipboard_text\n        clipboard_image = cls._from_clipboard_image()\n        if clipboard_image:\n            return clipboard_image\n        return None\n\n    @classmethod\n    def _from_stdin(cls) -&gt; &quot;ImplicitInput | None&quot;:\n        &quot;&quot;&quot;\n        Create an ImplicitInput instance from stdin.\n        &quot;&quot;&quot;\n        import sys\n\n        if not sys.stdin.isatty():\n            stdin_data = sys.stdin.read()\n            if stdin_data.strip():\n                return cls(type=&quot;stdin&quot;, content=stdin_data.strip())\n\n    @classmethod\n    def _from_clipboard_text(cls) -&gt; &quot;ImplicitInput | None&quot;:\n        &quot;&quot;&quot;\n        Create an ImplicitInput instance from clipboard text.\n        &quot;&quot;&quot;\n        import pyperclip\n\n        try:\n            clip_text = pyperclip.paste()\n            if clip_text.strip():\n                return cls(type=&quot;clipboard_text&quot;, content=clip_text.strip())\n        except pyperclip.PyperclipException:\n            pass\n\n    @classmethod\n    def _from_clipboard_image(cls) -&gt; &quot;ImplicitInput | None&quot;:\n        &quot;&quot;&quot;\n        Create an ImplicitInput instance from clipboard image.\n        &quot;&quot;&quot;\n        from PIL import ImageGrab, Image\n        import io, subprocess, sys\n\n        try:\n            image = ImageGrab.grabclipboard()\n            if isinstance(image, Image.Image):\n                return cls(\n                    type=&quot;clipboard_image&quot;,\n                    content=image,\n                )\n        except Exception as e:\n            print(f&quot;Error processing clipboard image: {e}&quot;, file=sys.stderr)\n\n        return None\n</file>\n    <file path=\"src/siphon/tests/test_context.py\">import pytest\nfrom siphon.data.uri import URI\nfrom siphon.tests.fixtures.assets import sample_assets\nfrom siphon.ingestion.retrieve import retrieve_llm_context\n\nassets_path = sample_assets[&quot;assets_path&quot;]\nfiletypes = sample_assets[&quot;filetypes&quot;]\nsourcetypes = {\n    k: v for k, v in sample_assets.items() if k not in (&quot;assets_path&quot;, &quot;filetypes&quot;)\n}\n\n\n@pytest.mark.parametrize(&quot;sourcetype&quot;, list(sourcetypes.keys()))\ndef test_context_sourcetype(sourcetype):\n    source = sourcetypes[sourcetype]\n    uri = URI.from_source(source)\n    assert uri is not None, f&quot;URI could not be created for sourcetype '{sourcetype}'&quot;\n    llm_context = retrieve_llm_context(uri)\n    assert llm_context, f&quot;Failed to retrieve llm_context for '{sourcetype}'&quot;\n    assert len(llm_context) &gt; 0, f&quot;llm_context is empty for '{sourcetype}'&quot;\n\n\n@pytest.mark.parametrize(&quot;filetype&quot;, list(filetypes.keys()))\ndef test_context_filetype(filetype):\n    source = filetypes[filetype]\n    uri = URI.from_source(source)\n    assert uri is not None, f&quot;URI could not be created for filetype '{filetype}'&quot;\n    llm_context = retrieve_llm_context(uri)\n    assert llm_context, f&quot;Failed to retrieve llm_context for '{filetype}'&quot;\n    assert len(llm_context) &gt; 0, f&quot;llm_context is empty for '{filetype}'&quot;\n</file>\n    <file path=\"src/siphon/tests/test_youtube.py\">#!/usr/bin/env python3\n&quot;&quot;&quot;\nTest script for YouTube channel URL extraction.\n&quot;&quot;&quot;\n\nimport os\nimport sys\nfrom googleapiclient.discovery import build\n\n\ndef test_channel_urls():\n    &quot;&quot;&quot;Test extracting URLs from @aiDotEngineer channel.&quot;&quot;&quot;\n\n    # Check for API key\n    api_key = os.getenv(&quot;YOUTUBE_API_KEY&quot;)\n    if not api_key:\n        print(&quot;Error: YOUTUBE_API_KEY environment variable not set&quot;)\n        sys.exit(1)\n\n    # Initialize YouTube service\n    service = build(&quot;youtube&quot;, &quot;v3&quot;, developerKey=api_key)\n\n    # Get channel info for @aiDotEngineer\n    try:\n        # First get channel ID from username\n        search_response = (\n            service.search()\n            .list(part=&quot;snippet&quot;, q=&quot;aiDotEngineer&quot;, type=&quot;channel&quot;, maxResults=1)\n            .execute()\n        )\n\n        if not search_response[&quot;items&quot;]:\n            print(&quot;Channel not found&quot;)\n            return\n\n        channel_id = search_response[&quot;items&quot;][0][&quot;snippet&quot;][&quot;channelId&quot;]\n        print(f&quot;Found channel ID: {channel_id}&quot;)\n\n        # Get channel details to find uploads playlist\n        channel_response = (\n            service.channels().list(part=&quot;contentDetails&quot;, id=channel_id).execute()\n        )\n\n        uploads_playlist = channel_response[&quot;items&quot;][0][&quot;contentDetails&quot;][\n            &quot;relatedPlaylists&quot;\n        ][&quot;uploads&quot;]\n        print(f&quot;Uploads playlist: {uploads_playlist}&quot;)\n\n        # Get videos from uploads playlist\n        videos_response = (\n            service.playlistItems()\n            .list(part=&quot;snippet&quot;, playlistId=uploads_playlist, maxResults=50)\n            .execute()\n        )\n\n        print(f&quot;\\nFound {len(videos_response['items'])} videos:&quot;)\n        for item in videos_response[&quot;items&quot;]:\n            video_id = item[&quot;snippet&quot;][&quot;resourceId&quot;][&quot;videoId&quot;]\n            title = item[&quot;snippet&quot;][&quot;title&quot;]\n            url = f&quot;https://youtube.com/watch?v={video_id}&quot;\n            print(f&quot;{url}&quot;)\n\n    except Exception as e:\n        print(f&quot;Error: {e}&quot;)\n\n\nif __name__ == &quot;__main__&quot;:\n    test_channel_urls()\n</file>\n    <file path=\"src/siphon/tests/test_display.py\">from siphon.data.processed_content import ProcessedContent\nfrom siphon.tests.fixtures.example_ProcessedContent import dummy_content\n\ndummy_content.pretty_print()\n\n</file>\n    <file path=\"src/siphon/tests/test_minimal_siphon.py\">&quot;&quot;&quot;\nSiphon pipeline through iterative development;\nFirst just did URIs, then context generation, now synthetic data + completed ProcessedContent.\n&quot;&quot;&quot;\n\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.processed_content import ProcessedContent\nfrom siphon.cli.cli_params import CLIParams\nfrom siphon.main.siphon import siphon\nfrom siphon.tests.fixtures.assets import sample_assets\nimport pytest\n\n# Import Conduit so we can set a cache.\nfrom conduit.sync import Model, ConduitCache\n\nModel._conduit_cache = ConduitCache(db_path=&quot;.test_minimal_cache.db&quot;)\n\n# Create our list of source types\nimmediately_usable_types = &quot;&quot;&quot;\nText\nDoc\nAudio\nImage\nGitHub\nYouTube\nArticle\n&quot;&quot;&quot;.strip().split(&quot;\\n&quot;)\n\nimmediately_usable_types = [SourceType(x) for x in immediately_usable_types]\nsourcetype_assets = sample_assets[&quot;sourcetypes&quot;]\n\n# Just text for now\nimmediate_usable_types = [immediately_usable_types[0]]  # Text\n\n\n@pytest.mark.parametrize(&quot;source_type&quot;, immediately_usable_types)\ndef test_minimal_siphon(source_type):\n    &quot;&quot;&quot;\n    Test the minimal siphon pipeline for a subset of source types.\n    &quot;&quot;&quot;\n    # Get the sample source for the given source type\n    source = sourcetype_assets[source_type]\n\n    # Create CLI parameters\n    cli_params = CLIParams(source=str(source))\n\n    # Run siphon and get the context\n    processed_content = siphon(cli_params)\n\n    # Display\n    processed_content.pretty_print()\n\n    assert isinstance(processed_content, ProcessedContent)\n    assert processed_content.synthetic_data is not None\n</file>\n    <file path=\"src/siphon/tests/test_uri.py\">import pytest\nfrom siphon.tests.fixtures.assets import sample_assets\nfrom siphon.data.type_definitions.uriSchemes import URISchemes\nfrom siphon.data.uri import URI\n\nsourcetypes = sample_assets[&quot;sourcetypes&quot;]\n\n\n# URI tests\n@pytest.mark.parametrize(&quot;source_and_example_uri&quot;, sourcetypes.items())\ndef test_URI(source_and_example_uri):\n    sourcetype, example_uri = source_and_example_uri\n    uri = URI.from_source(sourcetypes[sourcetype])\n    if uri is None:\n        raise ValueError(f&quot;URI for {sourcetype.value} is None, check sample_assets.&quot;)\n    assert uri.sourcetype == sourcetype\n    assert str(uri.source) == str(sourcetypes[sourcetype])\n    assert len(uri.uri) &gt; 0\n    assert URISchemes[sourcetype.value] in uri.uri, (\n        f&quot;URI scheme mismatch for {sourcetype.value}&quot;\n    )\n    print(uri)\n</file>\n    <file path=\"src/siphon/tests/test_postgres.py\">#!/usr/bin/env python3\nimport socket\nimport psycopg2\nimport os\n\ndef test_postgres():\n   # Test connection detection\n   endpoints = [&quot;localhost&quot;, &quot;10.0.0.82&quot;, &quot;68.47.92.102&quot;]\n   \n   for host in endpoints:\n       try:\n           socket.create_connection((host, 5432), timeout=1).close()\n           print(f&quot;\u2705 Found PostgreSQL at {host}&quot;)\n           \n           # Test actual database connection\n           conn = psycopg2.connect(\n               dbname=&quot;siphon&quot;, host=host, port=&quot;5432&quot;, \n               user=&quot;bianders&quot;, password=os.getenv(&quot;POSTGRES_PASSWORD&quot;)\n           )\n           cursor = conn.cursor()\n           cursor.execute(&quot;SELECT version();&quot;)\n           print(f&quot;\u2705 Database connected: {cursor.fetchone()[0][:50]}...&quot;)\n           conn.close()\n           return\n       except Exception as e:\n           print(f&quot;\u274c {host}: {e}&quot;)\n   \n   print(&quot;\u274c No PostgreSQL server found&quot;)\n\nif __name__ == &quot;__main__&quot;:\n   test_postgres()\n</file>\n    <file path=\"src/siphon/tests/test_local.py\">from siphon.cli.cli_params import CLIParams\nfrom siphon.tests.fixtures.assets import sample_assets\nfrom siphon.main.siphon import siphon\n\n\n# def test_synthetic_data_local():\n# &quot;&quot;&quot;Test that SyntheticData can be generated in local mode.&quot;&quot;&quot;\nsample_asset = sample_assets[&quot;filetypes&quot;][&quot;pdf&quot;]\nparams = CLIParams(\n    source=str(sample_asset),\n    cache_options=&quot;r&quot;,\n    local=True,  # Local mode\n)\n\nprocessed_content = siphon(params)\n\nprint(processed_content)\n</file>\n    <file path=\"src/siphon/tests/test_integration.py\">&quot;&quot;&quot;\nGoes through each SourceType and extension to creating a ProcessedContent doc.\n&quot;&quot;&quot;\n\nimport pytest\nfrom siphon.cli.cli_params import CLIParams\nfrom siphon.data.processed_content import ProcessedContent\nfrom siphon.main.siphon import siphon\nfrom siphon.tests.fixtures.assets import sample_assets\n\nassets_path = sample_assets.pop(&quot;assets_path&quot;)\nfiletypes = sample_assets.pop(&quot;filetypes&quot;)\nsourcetypes = sample_assets\n\n\n@pytest.mark.parametrize(&quot;sourcetype&quot;, list(sourcetypes.keys()))\ndef test_integration_sourcetypes():\n    &quot;&quot;&quot;\n    Test the integration of all source types and file extensions.\n    &quot;&quot;&quot;\n    cli_params = CLIParams(source=str(sample_assets[sourcetype]))\n    if not cli_params:\n        raise ValueError(\n            f&quot;CLIParams could not be created for {sourcetype} with source {sample_assets[sourcetype]}&quot;\n        )\n    assert isinstance(cli_params, CLIParams), (\n        f&quot;CLIParams is not an instance of CLIParams for {sourcetype} with source {sample_assets[sourcetype]}&quot;\n    )\n    processed_content = siphon(cli_params)\n    assert isinstance(processed_content, ProcessedContent)\n\n\n@pytest.mark.parametrize(&quot;filetype&quot;, list(filetypes.keys()))\ndef test_integration_filetypes():\n    &quot;&quot;&quot;\n    Test the integration of all file types.\n    &quot;&quot;&quot;\n    cli_params = CLIParams(\n        source=str(filetypes[filetype]),\n    )\n    if not cli_params:\n        raise ValueError(\n            f&quot;CLIParams could not be created for filetype {filetype} with source {sample_assets['assets_path']}&quot;\n        )\n    assert isinstance(cli_params, CLIParams), (\n        f&quot;CLIParams is not an instance of CLIParams for filetype {filetype} with source {sample_assets['assets_path']}&quot;\n    )\n    processed_content = siphon(cli_params)\n    assert isinstance(processed_content, ProcessedContent)\n</file>\n    <file path=\"src/siphon/tests/audio/test_audio.py\">from siphon.tests.fixtures.example_files import example_audio_file\nfrom siphon.ingestion.audio.local_transcript import get_local_transcript\nfrom siphon.ingestion.audio.openai_transcript import get_openai_transcript\nfrom pathlib import Path\n\ndir_path = Path(__file__).parent\nassets_dir = dir_path.parent.parent / &quot;assets&quot;\n\n# audio_file = assets_dir / &quot;learningmktplace6-23-2025.mp3&quot;\naudio_file = assets_dir / &quot;demos.mp3&quot;\n\n# transcript = get_openai_transcript(example_audio_file)\n# print(transcript)\n\ntranscript = get_local_transcript(audio_file)\nprint(transcript)\n</file>\n    <file path=\"src/siphon/tests/audio/test_transcribe.py\">from siphon.tests.fixtures.example_files import example_audio_file\nfrom siphon.ingestion.audio.transcribe import transcribe\n\ntranscription = transcribe(example_audio_file)\nprint(transcription)\n</file>\n    <file path=\"src/siphon/tests/audio/test_diarize.py\">from siphon.ingestion.audio.diarize import diarize\nfrom siphon.tests.fixtures.example_files import example_audio_file\n\ndiarized = diarize(example_audio_file)\nprint(diarized)\n</file>\n    <file path=\"src/siphon/tests/fixtures/assets.py\">&quot;&quot;&quot;\nImport sample_assets from the assets directory for testing and development purposes.\n&quot;&quot;&quot;\n\n&quot;&quot;&quot;\nsource_types = [SourceType.ARTICLE, SourceType.YOUTUBE, SourceType.FILE, SourceType.EMAIL, SourceType.GITHUB, SourceType.OBSIDIAN, SourceType.DRIVE]\n&quot;&quot;&quot;\n\nfrom pathlib import Path\nfrom siphon.data.type_definitions.source_type import SourceType\n\ndir_path = Path(__file__).parent\nassets_path = dir_path.parent.parent / &quot;assets&quot;\n\nfiles = &quot;&quot;&quot;\n/home/fishhouses/Brian_Code/Siphon/assets/New-Year-New-Skills-Own-Your-Career-Goals.pptx\n/home/fishhouses/Brian_Code/Siphon/assets/Talent Management Sources.docx\n/home/fishhouses/Brian_Code/Siphon/assets/Zscaler Professional Certificate Pitch 5-20-2025.pdf\n/home/fishhouses/Brian_Code/Siphon/assets/courselist_en_US (6).xlsx\n/home/fishhouses/Brian_Code/Siphon/assets/download.html\n/home/fishhouses/Brian_Code/Siphon/assets/dymphna.png\n/home/fishhouses/Brian_Code/Siphon/assets/ezra-transcripts.zip\n/home/fishhouses/Brian_Code/Siphon/assets/monthly-cert-insights (14).csv\n/home/fishhouses/Brian_Code/Siphon/assets/morris1.jpg\n/home/fishhouses/Brian_Code/Siphon/assets/output.mp3\n&quot;&quot;&quot;.strip().split(&quot;\\n&quot;)\n\nsample_pptx = assets_path / &quot;New-Year-New-Skills-Own-Your-Career-Goals.pptx&quot;\nsample_docx = assets_path / &quot;Talent Management Sources.docx&quot;\nsample_pdf = assets_path / &quot;Zscaler Professional Certificate Pitch 5-20-2025.pdf&quot;\nsample_xlsx = assets_path / &quot;courselist_en_US (6).xlsx&quot;\nsample_html = assets_path / &quot;download.html&quot;\nsample_md = assets_path / &quot;python_dp.md&quot;\nsample_image = assets_path / &quot;dymphna.png&quot;\nsample_zip = assets_path / &quot;ezra-transcripts.zip&quot;\nsample_csv = assets_path / &quot;monthly-cert-insights (14).csv&quot;\nsample_jpg = assets_path / &quot;morris1.jpg&quot;\nsample_mp3 = assets_path / &quot;output.mp3&quot;\n\nsample_assets = {\n    # If you ever need the dir\n    &quot;assets_path&quot;: assets_path,\n    # Specific file types\n    &quot;filetypes&quot;: {\n        &quot;pptx&quot;: sample_pptx,\n        &quot;docx&quot;: sample_docx,\n        &quot;pdf&quot;: sample_pdf,\n        &quot;xlsx&quot;: sample_xlsx,\n        &quot;html&quot;: sample_html,\n        &quot;png&quot;: sample_image,\n        &quot;zip&quot;: sample_zip,\n        &quot;csv&quot;: sample_csv,\n        &quot;jpg&quot;: sample_jpg,\n        &quot;mp3&quot;: sample_mp3,\n    },\n    # Sourcetypes\n    &quot;sourcetypes&quot;: {\n        SourceType(&quot;Text&quot;): sample_md,\n        SourceType(&quot;Audio&quot;): sample_mp3,\n        SourceType(&quot;Image&quot;): sample_image,\n        SourceType(&quot;Doc&quot;): sample_docx,\n        SourceType(&quot;YouTube&quot;): &quot;https://www.youtube.com/watch?v=6kfxxzCmLmk&quot;,\n        SourceType(&quot;GitHub&quot;): &quot;https://github.com/acesanderson/Siphon&quot;,\n        SourceType(\n            &quot;Article&quot;\n        ): &quot;https://mwichary.medium.com/one-hundred-and-thirty-seven-seconds-2a0a3dfbc59e&quot;,\n        # SourceType(&quot;Drive&quot;): &quot;https://drive.google.com/file/d/1a2b3c4d5e6f7g8h9i0j/view?usp=sharing&quot;,\n        # SourceType(&quot;Email&quot;): &quot;NA&quot;,\n        # SourceType(&quot;Obsidian&quot;): &quot;NA&quot;,\n    },\n}\n</file>\n    <file path=\"src/siphon/tests/fixtures/example_files.py\">from pathlib import Path\n\ndir_path = Path(__file__).parent\nassets_path = dir_path.parent.parent / &quot;assets&quot;\n\nexample_audio_file = assets_path / &quot;output.mp3&quot;\nexample_image_file = assets_path / &quot;dymphna.png&quot;\nexample_docx_file = assets_path / &quot;Talent Management Sources.docx&quot;\nexample_ocr_file = assets_path / &quot;ocr.png&quot;\n\nassert example_audio_file.exists(), &quot;Example audio file does not exist.&quot;\nassert example_image_file.exists(), &quot;Example image file does not exist.&quot;\nassert example_docx_file.exists(), &quot;Example docx file does not exist.&quot;\n</file>\n    <file path=\"src/siphon/tests/fixtures/example_ProcessedContent.py\">&quot;&quot;&quot;\nSimple dummy ProcessedContent for testing display.\n\nUsage:\n    from siphon.tests.fixtures.dummy_processed_content import dummy_content\n    dummy_content.pretty_print()  # assuming you've added the mixin to ProcessedContent\n&quot;&quot;&quot;\n\nfrom siphon.data.processed_content import ProcessedContent\nfrom siphon.data.uri import URI\nfrom siphon.data.context import Context\nfrom siphon.data.synthetic_data import SyntheticData\nfrom siphon.data.type_definitions.source_type import SourceType\n\n\n# Simple YouTube context with metadata\nclass DummyYouTubeContext(Context):\n    sourcetype: SourceType = SourceType.YOUTUBE\n    context: str = &quot;This is a great tutorial on machine learning basics. We cover linear regression, decision trees, and neural networks with practical Python examples. Perfect for beginners!&quot;\n\n    # YouTube metadata\n    url: str = &quot;https://www.youtube.com/watch?v=dQw4w9WgXcQ&quot;\n    domain: str = &quot;youtube.com&quot;\n    video_id: str = &quot;dQw4w9WgXcQ&quot;\n    channel: str = &quot;AI Learning Hub&quot;\n    duration: int = 1547  # 25 min 47 sec\n    view_count: int = 45782\n\n\n# Create dummy ProcessedContent\ndummy_content = ProcessedContent(\n    uri=URI(\n        source=&quot;https://www.youtube.com/watch?v=dQw4w9WgXcQ&quot;,\n        sourcetype=SourceType.YOUTUBE,\n        uri=&quot;youtube://dQw4w9WgXcQ&quot;,\n    ),\n    llm_context=DummyYouTubeContext(),\n    synthetic_data=SyntheticData(\n        sourcetype=SourceType.YOUTUBE,\n        title=&quot;Machine Learning Basics Tutorial&quot;,\n        description=&quot;Comprehensive guide covering ML fundamentals with Python examples&quot;,\n        summary=&quot;A 25-minute tutorial covering linear regression, decision trees, and neural networks. Great for beginners looking to get started with machine learning.&quot;,\n        topics=[&quot;machine learning&quot;, &quot;python&quot;, &quot;tutorial&quot;, &quot;AI&quot;],\n        entities=[&quot;Python&quot;, &quot;scikit-learn&quot;, &quot;TensorFlow&quot;],\n    ),\n)\n</file>\n    <file path=\"src/siphon/tests/data/test_URI.py\">test_sources = [\n    &quot;https://docs.google.com/spreadsheets/d/1xSokeHrhsqEt7XL2J3NtkU1oRGvd5vgNM5HdY-8W2WQ/edit?gid=0#gid=0&quot;,\n    &quot;https://docs.google.com/presentation/d/1vcBhiBht-WDYNc9ONZEnXUBrdHXfOXVB5IRdowQbSxI/edit?slide=id.p2#slide=id.p2&quot;, \n    &quot;https://docs.google.com/document/d/1yr44NsdKTU-pD73foyA0MXpgJFYyitTDLJ9GXe3eIPc/edit?tab=t.0&quot;,\n    &quot;https://www.youtube.com/watch?v=tv_Kry7STio&quot;,\n    &quot;https://youtu.be/dQw4w9WgXcQ&quot;,\n    &quot;https://github.com/acesanderson/Conduit&quot;,\n    &quot;https://github.com/acesanderson/Conduit/blob/main/README.md&quot;,\n    &quot;/home/fishhouses/Brian_Code/Siphon/README.md&quot;,\n    &quot;https://www.404media.co/the-ai-slop-fight-between-iran-and-israel/&quot;,\n    &quot;    https://docs.google.com/spreadsheets/d/123/edit  &quot;,  # Test whitespace\n]\n\nprint(&quot;Testing fixed URI parsing:&quot;)\nprint(&quot;=&quot; * 80)\n\nfor source in test_sources:\n    try:\n        uri_obj = URI.from_source(source)\n        print(f&quot;\u2705 {source}&quot;)\n        print(f&quot;   Result: {uri_obj}&quot;)\n        print()\n    except Exception as e:\n        print(f&quot;\u274c {source}&quot;)\n        print(f&quot;   Error: {e}&quot;)\n        print()\n\nprint(&quot;\\nExpected Results:&quot;)\nprint(&quot;-&quot; * 40)\nprint(&quot;Google Sheets: drive://sheet/1xSokeHrhsqEt7XL2J3NtkU1oRGvd5vgNM5HdY-8W2WQ&quot;)\nprint(&quot;Google Slides: drive://slide/1vcBhiBht-WDYNc9ONZEnXUBrdHXfOXVB5IRdowQbSxI&quot;) \nprint(&quot;Google Docs:   drive://doc/1yr44NsdKTU-pD73foyA0MXpgJFYyitTDLJ9GXe3eIPc&quot;)\nprint(&quot;YouTube:       youtube://tv_Kry7STio&quot;)\nprint(&quot;GitHub repo:   github://acesanderson/Conduit&quot;)\nprint(&quot;GitHub file:   github://acesanderson/Conduit/README.md&quot;) t\n</file>\n    <file path=\"src/siphon/collections/test_collections.py\"># test_siphon_query_progression.py\nimport pytest\nfrom siphon.collections.corpus.siphon_corpus import CorpusFactory\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom dbclients import get_postgres_client\n\n# =============================================================================\n# LEVEL 1: Basic Infrastructure (from previous tests)\n# =============================================================================\n\n\ndef test_library_snapshot():\n    &quot;&quot;&quot;Basic snapshot from entire library&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    snapshot = corpus.snapshot()\n\n\ndef test_corpus_len():\n    &quot;&quot;&quot;Count items in corpus&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    count = len(corpus)\n\n    assert isinstance(count, int)\n    assert count &gt;= 0\n\n\ndef test_query_creation():\n    &quot;&quot;&quot;Create query from corpus&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    query = corpus.query()\n\n    assert query is not None\n    assert hasattr(query, &quot;corpus&quot;)\n\n\ndef test_query_to_list():\n    &quot;&quot;&quot;Convert query to list&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    items = corpus.query().to_list()\n\n    assert isinstance(items, list)\n    assert len(items) == len(corpus)\n\n\n# =============================================================================\n# LEVEL 2: Basic Filtering Infrastructure\n# =============================================================================\n\n\ndef test_filter_by_source_type_single():\n    &quot;&quot;&quot;Filter corpus to specific source type&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    youtube_corpus = corpus.filter_by_source_type(SourceType.YOUTUBE)\n\n    assert len(youtube_corpus) &lt;= len(corpus)\n    # All items should be YouTube\n    for item in youtube_corpus:\n        assert item.uri.sourcetype == SourceType.YOUTUBE\n\n\ndef test_filter_by_source_type_maintains_interface():\n    &quot;&quot;&quot;Filtered corpus should still be a siphonCorpus&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    filtered = corpus.filter_by_source_type(SourceType.DOC)\n\n    # Should still be queryable\n    query = filtered.query()\n    assert query is not None\n    assert len(filtered) &lt;= len(corpus)\n\n\ndef test_filter_returns_new_corpus():\n    &quot;&quot;&quot;Filtering should return a new corpus instance&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    filtered = corpus.filter_by_source_type(SourceType.ARTICLE)\n\n    assert filtered is not corpus\n    assert type(filtered).__name__ in [&quot;DatabaseCorpus&quot;, &quot;InMemoryCorpus&quot;]\n\n\ndef test_source_type_counts_basic():\n    &quot;&quot;&quot;Get breakdown by source type&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    counts = corpus.get_source_type_counts()\n\n    assert isinstance(counts, dict)\n    assert all(isinstance(k, SourceType) for k in counts.keys())\n    assert all(isinstance(v, int) for v in counts.values())\n\n\n# =============================================================================\n# LEVEL 3: Query Interface Foundation\n# =============================================================================\n\n\ndef test_query_filter_delegation():\n    &quot;&quot;&quot;Query should delegate filtering to corpus&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    query = corpus.query()\n\n    # This should work and return a new query\n    filtered_query = query.filter_by_source_type(SourceType.YOUTUBE)\n    assert filtered_query is not query\n    assert type(filtered_query).__name__ == &quot;siphonQuery&quot;\n\n\ndef test_query_terminal_operations():\n    &quot;&quot;&quot;Terminal operations should return results, not queries&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    query = corpus.query()\n\n    # These should NOT return siphonQuery objects\n    results = query.to_list()\n    assert isinstance(results, list)\n\n    count = query.count()\n    assert isinstance(count, int)\n\n    exists = query.exists()\n    assert isinstance(exists, bool)\n\n\ndef test_query_first_and_last():\n    &quot;&quot;&quot;First and last operations&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    query = corpus.query()\n\n    first_item = query.first()\n    last_item = query.last()\n\n    if len(corpus) &gt; 0:\n        assert first_item is not None\n        assert last_item is not None\n        # They might be the same if corpus has only one item\n        assert hasattr(first_item, &quot;uri&quot;)\n        assert hasattr(last_item, &quot;uri&quot;)\n    else:\n        assert first_item is None\n        assert last_item is None\n\n\ndef test_empty_corpus_query_handling():\n    &quot;&quot;&quot;Handle empty corpus gracefully&quot;&quot;&quot;\n    empty_corpus = CorpusFactory.from_processed_content_list([])\n    query = empty_corpus.query()\n\n    assert query.count() == 0\n    assert query.first() is None\n    assert query.last() is None\n    assert query.exists() is False\n    assert query.to_list() == []\n\n\n# =============================================================================\n# LEVEL 4: Basic Monadic Conduiting\n# =============================================================================\n\n\ndef test_simple_filter_conduit():\n    &quot;&quot;&quot;Conduit one filter operation&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    results = corpus.query().filter_by_source_type(SourceType.ARTICLE).to_list()\n\n    for item in results:\n        assert item.uri.sourcetype == SourceType.ARTICLE\n\n\ndef test_filter_plus_limit():\n    &quot;&quot;&quot;Conduit filter and limit operations&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    results = corpus.query().filter_by_source_type(SourceType.DOC).limit(3).to_list()\n\n    assert len(results) &lt;= 3\n    for item in results:\n        assert item.uri.sourcetype == SourceType.DOC\n\n\ndef test_monadic_returns_new_instances():\n    &quot;&quot;&quot;Each monadic operation should return a new siphonQuery&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    original_query = corpus.query()\n\n    filtered_query = original_query.filter_by_source_type(SourceType.YOUTUBE)\n    limited_query = filtered_query.limit(5)\n\n    # Each should be a different query object\n    assert original_query is not filtered_query\n    assert filtered_query is not limited_query\n\n    # But they should all be siphonQuery instances\n    assert type(original_query).__name__ == &quot;siphonQuery&quot;\n    assert type(filtered_query).__name__ == &quot;siphonQuery&quot;\n    assert type(limited_query).__name__ == &quot;siphonQuery&quot;\n\n\ndef test_conduiting_preserves_state():\n    &quot;&quot;&quot;Conduited operations should accumulate correctly&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    # This conduit should apply both filter and limit\n    results = (\n        corpus.query().filter_by_source_type(SourceType.ARTICLE).limit(2).to_list()\n    )\n\n    assert len(results) &lt;= 2\n    for item in results:\n        assert item.uri.sourcetype == SourceType.ARTICLE\n\n\n# =============================================================================\n# LEVEL 5: Content-Based Operations\n# =============================================================================\n\n\ndef test_filter_by_content_basic():\n    &quot;&quot;&quot;Basic content text search&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    ai_content = corpus.query().filter_by_content(&quot;AI&quot;).to_list()\n\n    # Should find content mentioning AI\n    for item in ai_content:\n        content_text = item.context.lower()\n        assert &quot;ai&quot; in content_text or &quot;artificial intelligence&quot; in content_text\n\n\ndef test_content_search_case_insensitive():\n    &quot;&quot;&quot;Content search should be case insensitive&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    results_lower = corpus.query().filter_by_content(&quot;strategy&quot;).count()\n    results_upper = corpus.query().filter_by_content(&quot;STRATEGY&quot;).count()\n    results_mixed = corpus.query().filter_by_content(&quot;Strategy&quot;).count()\n\n    # Should return same count regardless of case\n    assert results_lower == results_upper == results_mixed\n\n\ndef test_content_search_with_source_filter():\n    &quot;&quot;&quot;Combine content search with source type filtering&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    strategy_docs = (\n        corpus.query()\n        .filter_by_source_type(SourceType.DOC)\n        .filter_by_content(&quot;strategy&quot;)\n        .to_list()\n    )\n\n    for item in strategy_docs:\n        assert item.uri.sourcetype == SourceType.DOC\n        assert &quot;strategy&quot; in item.context.lower()\n\n\ndef test_content_search_empty_results():\n    &quot;&quot;&quot;Content search with no matches should return empty&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    # Search for something very unlikely to exist\n    results = corpus.query().filter_by_content(&quot;xyzabc123nonexistent&quot;).to_list()\n\n    assert len(results) == 0\n\n\n# =============================================================================\n# LEVEL 6: Ordering and Pagination\n# =============================================================================\n\n\ndef test_order_by_date_basic():\n    &quot;&quot;&quot;Basic date ordering&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    results = corpus.query().order_by_date(ascending=False).limit(5).to_list()\n\n    assert len(results) &lt;= 5\n    # Should be ordered by date (newest first)\n    if len(results) &gt; 1:\n        for i in range(len(results) - 1):\n            # Note: This assumes your ProcessedContent has a created_at or similar field\n            # You might need to adjust based on your actual date field\n            assert hasattr(results[i], &quot;uri&quot;)  # Basic check for now\n\n\ndef test_limit_and_offset():\n    &quot;&quot;&quot;Test pagination with limit and offset&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    first_page = corpus.query().limit(3).to_list()\n\n    second_page = corpus.query().offset(3).limit(3).to_list()\n\n    assert len(first_page) &lt;= 3\n    assert len(second_page) &lt;= 3\n\n    # Pages should be different (assuming corpus has &gt;3 items)\n    if len(corpus) &gt; 3:\n        first_uris = {item.uri.uri for item in first_page}\n        second_uris = {item.uri.uri for item in second_page}\n        assert first_uris != second_uris\n\n\ndef test_order_with_content_filter():\n    &quot;&quot;&quot;Combine ordering with content filtering&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    results = (\n        corpus.query()\n        .filter_by_content(&quot;AI&quot;)\n        .order_by_date(ascending=False)\n        .limit(3)\n        .to_list()\n    )\n\n    assert len(results) &lt;= 3\n    for item in results:\n        assert (\n            &quot;ai&quot; in item.context.lower()\n            or &quot;artificial intelligence&quot; in item.context.lower()\n        )\n\n\ndef test_paginate_helper():\n    &quot;&quot;&quot;Test pagination helper method&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    page_1 = corpus.query().paginate(page_size=2, page_number=1).to_list()\n\n    page_2 = corpus.query().paginate(page_size=2, page_number=2).to_list()\n\n    assert len(page_1) &lt;= 2\n    assert len(page_2) &lt;= 2\n\n    if len(corpus) &gt; 2:\n        page_1_uris = {item.uri.uri for item in page_1}\n        page_2_uris = {item.uri.uri for item in page_2}\n        assert page_1_uris != page_2_uris\n\n\n# =============================================================================\n# LEVEL 7: Advanced Content Operations\n# =============================================================================\n\n\ndef test_filter_by_title():\n    &quot;&quot;&quot;Filter by title field from synthetic data&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    results = corpus.query().filter_by_title(&quot;LinkedIn&quot;).to_list()\n\n    for item in results:\n        if item.title:  # Some items might not have titles\n            assert &quot;linkedin&quot; in item.title.lower()\n\n\ndef test_multiple_content_filters():\n    &quot;&quot;&quot;Apply multiple content filters&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    results = (\n        corpus.query().filter_by_content(&quot;AI&quot;).filter_by_content(&quot;strategy&quot;).to_list()\n    )\n\n    for item in results:\n        content_lower = item.context.lower()\n        assert &quot;ai&quot; in content_lower and &quot;strategy&quot; in content_lower\n\n\ndef test_source_type_multiple():\n    &quot;&quot;&quot;Filter by multiple source types&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    results = (\n        corpus.query()\n        .filter_by_source_type([SourceType.DOC, SourceType.YOUTUBE])\n        .to_list()\n    )\n\n    allowed_types = {SourceType.DOC, SourceType.YOUTUBE}\n    for item in results:\n        assert item.uri.sourcetype in allowed_types\n\n\ndef test_filter_by_tags():\n    &quot;&quot;&quot;Filter by tags (if any content has tags)&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    # This might return empty if no content has tags yet\n    results = corpus.query().filter_by_tags([&quot;strategy&quot;]).to_list()\n\n    # Just verify it doesn't crash\n    assert isinstance(results, list)\n\n\n# =============================================================================\n# LEVEL 8: Semantic Search Foundation\n# =============================================================================\n\n\ndef test_semantic_search_basic():\n    &quot;&quot;&quot;Basic semantic search functionality&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    results = corpus.query().semantic_search(&quot;artificial intelligence&quot;, k=3).to_list()\n\n    assert len(results) &lt;= 3\n    # Results should be relevant to AI (basic check)\n    assert all(hasattr(item, &quot;context&quot;) for item in results)\n\n\ndef test_semantic_search_with_filters():\n    &quot;&quot;&quot;Combine semantic search with filtering&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    results = (\n        corpus.query()\n        .filter_by_source_type(SourceType.YOUTUBE)\n        .semantic_search(&quot;machine learning&quot;, k=5)\n        .to_list()\n    )\n\n    assert len(results) &lt;= 5\n    for item in results:\n        assert item.uri.sourcetype == SourceType.YOUTUBE\n\n\ndef test_similar_to_content():\n    &quot;&quot;&quot;Find content similar to a specific item&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n    all_items = corpus.query().to_list()\n\n    if len(all_items) &gt; 0:\n        reference_item = all_items[0]\n        similar_items = corpus.query().similar_to(reference_item, k=3).to_list()\n\n        assert len(similar_items) &lt;= 3\n        # Reference item might be in results\n        assert all(hasattr(item, &quot;uri&quot;) for item in similar_items)\n\n\ndef test_semantic_search_empty_query():\n    &quot;&quot;&quot;Semantic search with empty query should handle gracefully&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    results = corpus.query().semantic_search(&quot;&quot;, k=5).to_list()\n\n    # Should either return empty or handle gracefully\n    assert isinstance(results, list)\n\n\n# =============================================================================\n# LEVEL 9: Cross-Source Intelligence\n# =============================================================================\n\n\ndef test_cross_source_content_analysis():\n    &quot;&quot;&quot;Find related content across different source types&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    # Get strategy content from different sources\n    doc_strategy = (\n        corpus.query()\n        .filter_by_source_type(SourceType.DOC)\n        .filter_by_content(&quot;strategy&quot;)\n        .to_list()\n    )\n\n    youtube_strategy = (\n        corpus.query()\n        .filter_by_source_type(SourceType.YOUTUBE)\n        .filter_by_content(&quot;strategy&quot;)\n        .to_list()\n    )\n\n    # Both should exist in a realistic corpus\n    # This tests that the same concepts appear across sources\n    assert isinstance(doc_strategy, list)\n    assert isinstance(youtube_strategy, list)\n\n\ndef test_competitive_intelligence_pattern():\n    &quot;&quot;&quot;Test pattern for competitive analysis&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    competitive_content = (\n        corpus.query()\n        .filter_by_content(&quot;competitive&quot;)\n        .semantic_search(&quot;market analysis positioning&quot;, k=5)\n        .to_list()\n    )\n\n    assert len(competitive_content) &lt;= 5\n    for item in competitive_content:\n        content_lower = item.context.lower()\n        assert &quot;competitive&quot; in content_lower or &quot;market&quot; in content_lower\n\n\ndef test_ai_trend_analysis():\n    &quot;&quot;&quot;Pattern for analyzing AI trends across sources&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    ai_trends = (\n        corpus.query()\n        .filter_by_content(&quot;AI&quot;)\n        .order_by_date(ascending=False)\n        .limit(10)\n        .to_list()\n    )\n\n    assert len(ai_trends) &lt;= 10\n    for item in ai_trends:\n        content_lower = item.context.lower()\n        assert &quot;ai&quot; in content_lower or &quot;artificial intelligence&quot; in content_lower\n\n\ndef test_source_type_distribution_analysis():\n    &quot;&quot;&quot;Analyze content distribution across source types&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    # Get AI content from each major source type\n    source_types = [SourceType.DOC, SourceType.YOUTUBE, SourceType.ARTICLE]\n    distribution = {}\n\n    for source_type in source_types:\n        count = (\n            corpus.query()\n            .filter_by_source_type(source_type)\n            .filter_by_content(&quot;AI&quot;)\n            .count()\n        )\n        distribution[source_type] = count\n\n    # Verify we got some distribution data\n    assert isinstance(distribution, dict)\n    assert len(distribution) == 3\n\n\n# =============================================================================\n# LEVEL 10: Collection Transformations\n# =============================================================================\n\n\ndef test_query_to_corpus_conversion():\n    &quot;&quot;&quot;Convert query results to new corpus&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    new_corpus = corpus.query().filter_by_content(&quot;strategy&quot;).to_corpus()\n\n    # Should be a new corpus with filtered content\n    assert new_corpus is not corpus\n    assert hasattr(new_corpus, &quot;query&quot;)\n    assert len(new_corpus) &lt;= len(corpus)\n\n\ndef test_corpus_to_sourdough_basic():\n    &quot;&quot;&quot;Convert filtered corpus to Sourdough collection&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    strategy_corpus = (\n        corpus.query()\n        .filter_by_content(&quot;strategy&quot;)\n        .filter_by_source_type(SourceType.DOC)\n        .to_corpus()\n    )\n\n    sourdough = strategy_corpus.query().to_sourdough(\n        max_tokens=1000, focus_areas=[&quot;strategy&quot;]\n    )\n\n    assert hasattr(sourdough, &quot;get_current_snapshot&quot;)\n    assert len(sourdough) &gt;= 0  # Might be empty if no strategy docs\n\n\ndef test_sourdough_snapshot_generation():\n    &quot;&quot;&quot;Test Sourdough snapshot generation&quot;&quot;&quot;\n    corpus = CorpusFactory.from_library()\n\n    # Create sourdough with any available content\n    limited_corpus = corpus.query().limit(5).to_corpus()\n    sourdough = limited_corpus.query().to_sourdough(max_tokens=500)\n\n    snapshot = sourdough.get_current_snapshot()\n\n    assert isinstance(snapshot, str)\n    if len(sourdough) &gt; 0:\n        assert &quot;Strategic Snapshot&quot; in snapshot\n\n\ndef test_database_to_memory_conversion():\n    &quot;&quot;&quot;Convert database corpus to in-memory corpus&quot;&quot;&quot;\n    db_corpus = CorpusFactory.from_library()\n\n    memory_corpus = (\n        db_corpus.query()\n        .filter_by_source_type(SourceType.ARTICLE)\n        .limit(10)\n        .to_corpus()\n    )\n\n    # Should be convertible and queryable\n    assert hasattr(memory_corpus, &quot;query&quot;)\n    query_from_memory = memory_corpus.query()\n    assert query_from_memory is not None\n</file>\n    <file path=\"src/siphon/collections/corpus/__init__.py\">\n\n</file>\n    <file path=\"src/siphon/collections/corpus/siphon_corpus.py\">&quot;&quot;&quot;\nSiphonCorpus - Lightweight wrapper around collections of ProcessedContent objects\n\nProvides convenience functions for managing and constructing ProcessedContent. To query a SiphonCorpus, you should attach it to a SiphonQuery object.\n\nA SiphonCorpus can be constructed from a CorpusFactory, which can take various sources:\n- Entire library of ProcessedContent from the database (DatabaseCorpus)\n- Files in a directory (e.g., markdown, PDFs)\n- List of URLs (e.g., YouTube, GitHub, articles)\n- Content tagged with specific tags (not yet implemented)\n- User-defined queries (not yet implemented) -- i.e. a SiphonQuery object\n- Existing list of ProcessedContent objects\n- TBD\n\nA SiphonCorpus, on initialization, will also have access to the following (either pre-existing for persistent, or created on-the-fly for in-memory):\n- A lightweight in-memory representation of the corpus\n- a vector store for similarity search (TBD)\n- a graph representation of the corpus (TBD)\n- NER, topic modeling, and other NLP features (TBD)\n- Automatic summarization of content (TBD)\n- Automatic tagging and categorization (TBD)\n\n## Architecture\n\nWe define an abstract base class `SiphonCorpus` that provides a rich interface.\n\nWe then implement:\n- DatabaseCorpus (which uses pgres for specific queries, and leverages persistent Chroma and Neo4j databases, as well as library-wide AI tagging etc.)\n- InMemoryCorpus (which is an in-memory set of ProcessedContent objects, with ephemeral chroma and networkx instead of neo4j)\n\nThese would use the same interface, but have different implementations for the underlying storage and retrieval mechanisms.\n\nWe then have a `CorpusFactory` that provides methods to create the appropriate corpus type based on the source:\n&quot;&quot;&quot;\n\nfrom siphon.data.processed_content import ProcessedContent\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom psycopg2.extras import RealDictCursor\nfrom typing import override\nfrom collections.abc import Iterator\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\n\n\nclass SiphonCorpus(ABC):\n    &quot;&quot;&quot;\n    Abstract interface - all corpus types look the same to SiphonQuery.\n    &quot;&quot;&quot;\n\n    # Collection Management\n    @abstractmethod\n    def add(self, content: ProcessedContent) -&gt; None: ...\n\n    @abstractmethod\n    def remove(self, content: ProcessedContent) -&gt; None: ...\n\n    @abstractmethod\n    def remove_by_uri(self, uri: str) -&gt; bool: ...\n\n    # Iteration &amp; Access\n    @abstractmethod\n    def __iter__(self) -&gt; Iterator[ProcessedContent]: ...\n\n    @abstractmethod\n    def __len__(self) -&gt; int: ...\n\n    @abstractmethod\n    def __contains__(self, content: ProcessedContent) -&gt; bool: ...\n\n    # Query Interface (returns new corpus for conduiting)\n    @abstractmethod\n    def filter_by_source_type(self, source_type: SourceType) -&gt; &quot;SiphonCorpus&quot;: ...\n\n    @abstractmethod\n    def filter_by_date_range(self, start_date, end_date) -&gt; &quot;SiphonCorpus&quot;: ...\n\n    @abstractmethod\n    def filter_by_tags(self, tags: list[str]) -&gt; &quot;SiphonCorpus&quot;: ...\n\n    # Metadata &amp; Views\n    @abstractmethod\n    def snapshot(self) -&gt; str: ...\n\n    @abstractmethod\n    def get_source_type_counts(self) -&gt; dict[SourceType, int]: ...\n\n    @abstractmethod\n    def is_empty(self) -&gt; bool: ...\n\n    # Query Entry Point\n    @abstractmethod\n    def query(self) -&gt; &quot;SiphonQuery&quot;: ...\n\n\nclass InMemoryCorpus(SiphonCorpus):\n    &quot;&quot;&quot;\n    In-memory corpus for fast operations on materialized data\n    &quot;&quot;&quot;\n\n    def __init__(self, source: str, corpus: list[ProcessedContent] = None):\n        self.source = source\n        self.corpus = corpus if corpus is not None else []  # Fix: Use list, not dict\n        # Vector store stuff\n        self.description_collection = None\n        self.chunked_context_collection = None\n        self.vector_store_created = False\n\n    # Create vectors and graphs on the fly as needed\n    def _create_vector_store(self):\n        import chromadb\n        from chromadb.utils import embedding_functions\n        import torch\n\n        # Check GPU availability\n        device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n        print(f&quot;Using device: {device}&quot;)\n\n        client = chromadb.EphemeralClient()\n\n        # Check if corpus is empty\n        if not self.corpus:\n            print(&quot;Warning: Cannot create vector store - corpus is empty&quot;)\n            return\n\n        # Create GPU-enabled embedding function\n        # Option 1: Use sentence-transformers with GPU support\n        try:\n            embedding_function = (\n                embedding_functions.SentenceTransformerEmbeddingFunction(\n                    model_name=&quot;all-MiniLM-L6-v2&quot;,  # Fast, good quality model\n                    device=device,\n                )\n            )\n        except Exception as e:\n            embedding_function = embedding_functions.DefaultEmbeddingFunction()\n\n        # Alternative Option 2: Use Hugging Face transformers directly\n        # embedding_function = embedding_functions.HuggingFaceEmbeddingFunction(\n        #     api_key=&quot;your_hf_token&quot;,  # Only needed for gated models\n        #     model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;\n        # )\n\n        # We want ephemeral collections for: uri:description, uri:chunked_context\n        description_collection = client.create_collection(\n            name=&quot;siphon_in_memory_descriptions&quot;, embedding_function=embedding_function\n        )\n\n        # Collect valid descriptions\n        valid_descriptions = []\n        valid_desc_ids = []\n        for content in self.corpus:\n            if (\n                content.description and content.description.strip()\n            ):  # Check for non-empty description\n                valid_descriptions.append(content.description)\n                valid_desc_ids.append(content.uri.uri)\n\n        # Add descriptions in batches to avoid size limits\n        if valid_descriptions:\n            batch_size = 500  # Smaller batches for GPU to avoid memory issues\n            print(\n                f&quot;Adding {len(valid_descriptions)} descriptions in batches of {batch_size}&quot;\n            )\n            for i in range(0, len(valid_descriptions), batch_size):\n                batch_docs = valid_descriptions[i : i + batch_size]\n                batch_ids = valid_desc_ids[i : i + batch_size]\n                print(\n                    f&quot;Adding description batch {i // batch_size + 1}: {len(batch_docs)} items&quot;\n                )\n                description_collection.add(\n                    documents=batch_docs,\n                    ids=batch_ids,\n                )\n        self.description_collection = description_collection\n\n        # Handle chunked context - use same embedding function for consistency\n        chunked_context_collection = client.create_collection(\n            name=&quot;siphon_in_memory_chunked_context&quot;,\n            embedding_function=embedding_function,\n        )\n\n        all_chunks = []\n        all_chunk_ids = []\n\n        for content in self.corpus:\n            if (\n                content.context and content.context.strip()\n            ):  # Check for non-empty context\n                # Chunk the context into smaller pieces for better search\n                chunks = [\n                    content.context[i : i + 500]\n                    for i in range(0, len(content.context), 500)\n                    if content.context[\n                        i : i + 500\n                    ].strip()  # Only include non-empty chunks\n                ]\n                chunk_ids = [f&quot;{content.uri.uri}_chunk_{j}&quot; for j in range(len(chunks))]\n\n                all_chunks.extend(chunks)\n                all_chunk_ids.extend(chunk_ids)\n\n        # Add chunks in batches to avoid ChromaDB batch size limits\n        if all_chunks:\n            batch_size = 500  # Smaller batches for GPU processing\n            print(\n                f&quot;Adding {len(all_chunks)} chunks in batches of {batch_size} using {device}&quot;\n            )\n            for i in range(0, len(all_chunks), batch_size):\n                batch_docs = all_chunks[i : i + batch_size]\n                batch_ids = all_chunk_ids[i : i + batch_size]\n                print(\n                    f&quot;Adding chunk batch {i // batch_size + 1}/{(len(all_chunks) - 1) // batch_size + 1}: {len(batch_docs)} chunks&quot;\n                )\n                chunked_context_collection.add(documents=batch_docs, ids=batch_ids)\n\n        self.chunked_context_collection = chunked_context_collection\n        self.vector_store_created = True\n        print(&quot;Vector store creation completed!&quot;)\n\n    def _query_vector_store(self, vector_collection, query: str, k: int):\n        if not self.vector_store_created:\n            self._create_vector_store()\n\n        if vector_collection is None:\n            return {&quot;documents&quot;: [[]], &quot;ids&quot;: [[]], &quot;distances&quot;: [[]]}\n\n        results = vector_collection.query(query_texts=[query], n_results=k)\n        return results\n\n    # Collection Management\n    @override\n    def add(self, content: ProcessedContent) -&gt; None:\n        if content not in self.corpus:\n            self.corpus.append(content)\n            # Reset vector store so it gets recreated with new content\n            self.vector_store_created = False\n\n    @override\n    def remove(self, content: ProcessedContent) -&gt; None:\n        self.corpus.remove(content)\n        # Reset vector store so it gets recreated without removed content\n        self.vector_store_created = False\n\n    @override\n    def remove_by_uri(self, uri: str) -&gt; bool:\n        to_remove = next((c for c in self.corpus if c.uri == uri), None)\n        if to_remove:\n            self.corpus.remove(to_remove)\n            # Reset vector store\n            self.vector_store_created = False\n            return True\n        return False\n\n    # Iteration &amp; Access\n    @override\n    def __iter__(self) -&gt; Iterator[ProcessedContent]:\n        return iter(self.corpus)\n\n    @override\n    def __len__(self) -&gt; int:\n        return len(self.corpus)\n\n    @override\n    def __contains__(self, content: ProcessedContent) -&gt; bool:\n        return content in self.corpus\n\n    # Query Interface (returns new InMemoryCorpus with filtered data)\n    @override\n    def filter_by_source_type(self, source_type: SourceType) -&gt; &quot;InMemoryCorpus&quot;:\n        filtered = [\n            c for c in self.corpus if c.source_type == source_type\n        ]  # Fix: Use list comprehension\n        return InMemoryCorpus(\n            source=f&quot;{self.source}|source_type={source_type}&quot;, corpus=filtered\n        )\n\n    @override\n    def filter_by_date_range(self, start_date, end_date) -&gt; &quot;InMemoryCorpus&quot;:\n        filtered = [\n            c\n            for c in self.corpus\n            if c.date_added and start_date &lt;= c.date_added &lt;= end_date\n        ]  # Fix: Use list comprehension\n        return InMemoryCorpus(\n            source=f&quot;{self.source}|date_range={start_date}_{end_date}&quot;, corpus=filtered\n        )\n\n    @override\n    def filter_by_tags(self, tags: list[str]) -&gt; &quot;InMemoryCorpus&quot;:\n        filtered = [\n            c for c in self.corpus if any(tag in c.tags for tag in tags)\n        ]  # Fix: Use list comprehension\n        return InMemoryCorpus(\n            source=f&quot;{self.source}|tags={','.join(tags)}&quot;, corpus=filtered\n        )\n\n    # Rest of your methods remain the same...\n    def text(self) -&gt; str: ...\n\n    def to_dataframe(self): ...\n\n    @override\n    def snapshot(self):\n        from siphon.collections.query.snapshot import generate_snapshot\n\n        generate_snapshot(self.corpus)\n\n    @override\n    def get_source_type_counts(self) -&gt; dict[SourceType, int]: ...\n\n    @override\n    def is_empty(self) -&gt; bool:\n        return len(self.corpus) == 0\n\n    @override\n    def query(self) -&gt; &quot;SiphonQuery&quot;:\n        from siphon.collections.query.siphon_query import SiphonQuery\n\n        return SiphonQuery(self)\n\n\nclass DatabaseCorpus(SiphonCorpus):\n    &quot;&quot;&quot;\n    Database-backed corpus with lazy SQL query building.\n    Note: This is a read-only corpus for now; adding/removing content should be done via dedicated ingestion/removal functions.\n\n    &quot;&quot;&quot;\n\n    raise NotImplementedError(&quot;DatabaseCorpus is not yet implemented.&quot;)\n\n    # def __init__(self, db_connection=get_db_connection):\n    #     &quot;&quot;&quot;\n    #     Initialize a database-backed corpus.\n    #\n    #     Args:\n    #         db_connection_func: contextlib.contextmanager to get a database connection\n    #     &quot;&quot;&quot;\n    #     self.db_connection = db_connection\n    #\n    # @override\n    # def __len__(self) -&gt; int:\n    #     &quot;&quot;&quot;Return the number of items in the corpus.&quot;&quot;&quot;\n    #     with self.db_connection() as conn, conn.cursor() as cursor:\n    #         cursor.execute(&quot;SELECT COUNT(*) FROM processed_content&quot;)\n    #         count = cursor.fetchone()[0]\n    #     return count\n    #\n    # # Collection Management\n    # @override\n    # def add(self, content: ProcessedContent) -&gt; None:\n    #     raise NotImplementedError()\n    #\n    # @override\n    # def remove(self, content: ProcessedContent) -&gt; None:\n    #     raise NotImplementedError()\n    #\n    # @override\n    # def remove_by_uri(self, uri: str) -&gt; bool:\n    #     raise NotImplementedError()\n    #\n    # # Iteration &amp; Access\n    # @override\n    # def __iter__(self):\n    #     with (\n    #         self.db_connection() as conn,\n    #         conn.cursor(cursor_factory=RealDictCursor) as cursor,\n    #     ):\n    #         cursor.execute(&quot;SELECT * FROM processed_content&quot;)\n    #         while True:\n    #             rows = cursor.fetchmany(1000)  # Batch of 1000\n    #             if not rows:\n    #                 break\n    #             for row in rows:\n    #                 yield ProcessedContent.model_validate_from_cache(row[&quot;data&quot;])\n    #\n    # @override\n    # def __contains__(self, content: ProcessedContent) -&gt; bool:\n    #     raise NotImplementedError()\n    #\n    # # Query Interface (returns new DatabaseCorpus with modified SQL)\n    # @override\n    # def filter_by_source_type(self, source_type: SourceType) -&gt; &quot;DatabaseCorpus&quot;:\n    #     raise NotImplementedError()\n    #\n    # @override\n    # def filter_by_date_range(self, start_date, end_date) -&gt; &quot;DatabaseCorpus&quot;:\n    #     raise NotImplementedError()\n    #\n    # @override\n    # def filter_by_tags(self, tags: list[str]) -&gt; &quot;DatabaseCorpus&quot;:\n    #     raise NotImplementedError()\n    #\n    # # Database-specific methods\n    # def _get_all_processed_content(self) -&gt; &quot;list[ProcessedContent]&quot;:\n    #     from siphon.database.postgres.PGRES_processed_content import get_all_siphon\n    #\n    #     return get_all_siphon()\n    #\n    # # Metadata &amp; Views\n    # @override\n    # def snapshot(self):\n    #     from siphon.collections.query.snapshot import generate_snapshot\n    #\n    #     generate_snapshot()\n    #\n    # @override\n    # def get_source_type_counts(self) -&gt; dict[SourceType, int]: ...\n    #\n    # @override\n    # def is_empty(self) -&gt; bool: ...\n    #\n    # @override\n    # def query(self) -&gt; &quot;SiphonQuery&quot;:\n    #     &quot;&quot;&quot;\n    #     Create a SiphonQuery instance for this corpus.\n    #     For now, this immediately goes to InMemoryCorpus.\n    #     As database grows and performance degrades, we can implement more queries within DatabaseCorpus and move logic to database layer, at which point this would return self.\n    #\n    #     &quot;&quot;&quot;\n    #     from siphon.collections.query.siphon_query import SiphonQuery\n    #\n    #     processed_content_list = self._get_all_processed_content()\n    #     corpus = InMemoryCorpus(source=&quot;DatabaseCorpus&quot;, corpus=processed_content_list)\n    #\n    #     return SiphonQuery(corpus)\n    #\n\n\nclass CorpusFactory:\n    &quot;&quot;&quot;\n    Factory for creating appropriate corpus implementations\n    &quot;&quot;&quot;\n\n    # Database-backed creation\n    @staticmethod\n    def from_library() -&gt; DatabaseCorpus:\n        &quot;&quot;&quot;Create a DatabaseCorpus from the entire library of ProcessedContent.&quot;&quot;&quot;\n        return DatabaseCorpus()\n\n    @staticmethod\n    def from_tag(tag: str) -&gt; DatabaseCorpus: ...\n\n    @staticmethod\n    def from_date_range(start_date, end_date) -&gt; DatabaseCorpus: ...\n\n    # In-memory creation\n    @staticmethod\n    def from_directory(\n        directory_path: str | Path, pattern: str = &quot;*&quot;\n    ) -&gt; InMemoryCorpus: ...\n\n    @staticmethod\n    def from_url_list(urls: list[str]) -&gt; InMemoryCorpus: ...\n\n    @staticmethod\n    def from_processed_content_list(\n        content_list: list[ProcessedContent],\n    ) -&gt; InMemoryCorpus: ...\n\n    @staticmethod\n    def from_files(file_paths: list[str]) -&gt; InMemoryCorpus: ...\n\n\nif __name__ == &quot;__main__&quot;:\n    corpus = CorpusFactory.from_library()\n    print(f&quot;Iterated {sum(1 for _ in corpus)} items vs len() = {len(corpus)}&quot;)\n</file>\n    <file path=\"src/siphon/collections/corpus/sourdough.py\">&quot;&quot;&quot;\nSourdough - Auto-updating, auto-curated strategic knowledge base\n\nSpecialized ProcessedCorpus that maintains itself within token limits,\nauto-prunes outdated content, and provides strategic intelligence capabilities.\nUses composition rather than inheritance for flexibility.\n&quot;&quot;&quot;\n\nimport time\nfrom typing import Optional, Any\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom pydantic import BaseModel, Field\n\nfrom siphon.data.processed_content import ProcessedContent\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom .processed_corpus import ProcessedCorpus\n\n\nclass SourdoughSnapshot(BaseModel):\n    &quot;&quot;&quot;Represents a versioned snapshot of the Sourdough state.&quot;&quot;&quot;\n\n    timestamp: int = Field(..., description=&quot;Unix timestamp of snapshot creation&quot;)\n    version: str = Field(..., description=&quot;Semantic version of this snapshot&quot;)\n    content_count: int = Field(\n        ..., description=&quot;Number of content items in this snapshot&quot;\n    )\n    token_count: int = Field(..., description=&quot;Estimated token count of snapshot&quot;)\n    summary: str = Field(..., description=&quot;Executive summary of strategic content&quot;)\n\n\nclass Sourdough:\n    &quot;&quot;&quot;\n    Auto-updating strategic knowledge base that maintains itself within constraints.\n\n    Like sourdough starter, this requires regular feeding and maintenance to stay healthy.\n    Automatically prunes outdated content and maintains strategic focus.\n    &quot;&quot;&quot;\n\n    def __init__(\n        self,\n        max_tokens: int = 4000,\n        focus_areas: Optional[list[str]] = None,\n        auto_prune: bool = True,\n    ):\n        &quot;&quot;&quot;\n        Initialize Sourdough strategic knowledge base.\n\n        Args:\n            max_tokens: Maximum tokens to maintain in current snapshot\n            focus_areas: Strategic focus areas to prioritize (e.g., [&quot;AI&quot;, &quot;strategy&quot;, &quot;product&quot;])\n            auto_prune: Whether to automatically prune content on updates\n        &quot;&quot;&quot;\n        # Core corpus management (composition, not inheritance)\n        self._corpus = ProcessedCorpus.from_processed_content_list([])\n\n        # Sourdough-specific configuration\n        self.max_tokens = max_tokens\n        self.focus_areas = focus_areas or []\n        self.auto_prune = auto_prune\n\n        # State management\n        self._snapshot_history: list[SourdoughSnapshot] = []\n        self._last_update: int = int(time.time())\n        self._drift_threshold = 0.3  # 30% content change triggers drift alert\n\n        # Persistence (matches your design spec)\n        self._snapshot_file = Path(&quot;current_strategy.md&quot;)\n\n    # ============================================================================\n    # Core Sourdough Operations (Auto-curation)\n    # ============================================================================\n\n    def feed(self, content: ProcessedContent | list[ProcessedContent]) -&gt; None:\n        &quot;&quot;&quot;\n        Add new content to the Sourdough (feeding the starter).\n\n        Args:\n            content: Single ProcessedContent or list of content to add\n        &quot;&quot;&quot;\n        if isinstance(content, ProcessedContent):\n            content = [content]\n\n        for item in content:\n            self._corpus.add(item)\n\n        if self.auto_prune:\n            self._auto_prune()\n\n        self._last_update = int(time.time())\n        print(f&quot;Fed Sourdough with {len(content)} new items&quot;)\n\n    def _auto_prune(self) -&gt; None:\n        &quot;&quot;&quot;\n        Automatically prune content to stay within token limits.\n        Prioritizes recent, high-value strategic content.\n        &quot;&quot;&quot;\n        current_tokens = self._estimate_token_count()\n\n        if current_tokens &lt;= self.max_tokens:\n            return\n\n        print(f&quot;Auto-pruning: {current_tokens} tokens &gt; {self.max_tokens} limit&quot;)\n\n        # Score content by strategic value\n        scored_content = []\n        for content in self._corpus:\n            score = self._calculate_strategic_score(content)\n            scored_content.append((score, content))\n\n        # Sort by score (highest first) and keep top content\n        scored_content.sort(reverse=True, key=lambda x: x[0])\n\n        # Keep content until we hit token limit\n        pruned_corpus = []\n        running_tokens = 0\n\n        for score, content in scored_content:\n            _ = score  # Unused, but kept for clarity\n            content_tokens = len(content.context.split()) * 1.3  # Rough token estimate\n\n            if running_tokens + content_tokens &lt;= self.max_tokens:\n                pruned_corpus.append(content)\n                running_tokens += content_tokens\n            else:\n                # Store pruned content in &quot;hooch&quot; (temporary storage)\n                self._add_to_hooch(content)\n\n        # Update corpus with pruned content\n        self._corpus = ProcessedCorpus.from_processed_content_list(pruned_corpus)\n\n        final_tokens = self._estimate_token_count()\n        print(f&quot;Pruning complete: {final_tokens} tokens remaining&quot;)\n\n    def _calculate_strategic_score(self, content: ProcessedContent) -&gt; float:\n        &quot;&quot;&quot;\n        Calculate strategic value score for content prioritization.\n\n        Returns:\n            Score from 0.0 to 1.0 (higher = more strategic value)\n        &quot;&quot;&quot;\n        score = 0.0\n\n        # Recency bonus (content from last 30 days gets higher score)\n        # TODO: Add timestamp to ProcessedContent for proper recency calculation\n        score += 0.3  # Default recency score\n\n        # Focus area relevance\n        content_text = (\n            content.title + &quot; &quot; + content.description + &quot; &quot; + content.context[:500]\n        ).lower()\n\n        focus_matches = sum(\n            1 for area in self.focus_areas if area.lower() in content_text\n        )\n        if self.focus_areas:\n            score += (focus_matches / len(self.focus_areas)) * 0.4\n\n        # Source type weighting (prioritize certain types)\n        source_weights = {\n            SourceType.YOUTUBE: 0.1,  # Often tactical\n            SourceType.GITHUB: 0.05,  # Technical, less strategic\n            SourceType.ARTICLE: 0.15,\n            SourceType.DOC: 0.2,  # Often strategic documents\n            SourceType.AUDIO: 0.2,  # Meeting recordings, high value\n        }\n        score += source_weights.get(content.uri.sourcetype, 0.1)\n\n        # Authority indicators (CEO, executive content)\n        authority_keywords = [&quot;ceo&quot;, &quot;executive&quot;, &quot;strategy&quot;, &quot;roadmap&quot;, &quot;okr&quot;]\n        authority_matches = sum(\n            1 for keyword in authority_keywords if keyword in content_text\n        )\n        score += min(authority_matches * 0.05, 0.1)\n\n        return min(score, 1.0)  # Cap at 1.0\n\n    def _add_to_hooch(self, content: ProcessedContent) -&gt; None:\n        &quot;&quot;&quot;\n        Add pruned content to temporary storage ('hooch').\n        Content is retained for 30 days before permanent deletion.\n        &quot;&quot;&quot;\n        # TODO: Implement hooch storage (temporary retention of pruned content)\n        # This could be a separate database table or file system storage\n        print(f&quot;Added to hooch: {content.title or content.uri.uri}&quot;)\n\n    # ============================================================================\n    # Snapshot Management (Core Sourdough Feature)\n    # ============================================================================\n\n    def get_current_snapshot(self) -&gt; str:\n        &quot;&quot;&quot;\n        Get the current strategic snapshot in markdown format.\n        This is the main product of Sourdough - always fresh strategic intel.\n        &quot;&quot;&quot;\n        if self._corpus.is_empty():\n            return &quot;# Strategic Snapshot\\n\\n*No strategic content available.*&quot;\n\n        snapshot_lines = [\n            f&quot;# Strategic Snapshot \u2014 {datetime.now().strftime('%B %Y')}&quot;,\n            &quot;&quot;,\n            f&quot;*Last updated: {datetime.fromtimestamp(self._last_update).strftime('%Y-%m-%d %H:%M')}*&quot;,\n            f&quot;*Content items: {len(self._corpus)} | Estimated tokens: {self._estimate_token_count()}*&quot;,\n            &quot;&quot;,\n        ]\n\n        # Group content by strategic themes\n        themes = self._extract_strategic_themes()\n\n        for theme, contents in themes.items():\n            snapshot_lines.extend([f&quot;## {theme}&quot;, &quot;&quot;])\n\n            for content in contents:\n                # Extract key insights from each piece of content\n                insights = self._extract_key_insights(content)\n                snapshot_lines.extend(\n                    [\n                        f&quot;### {content.title or 'Strategic Update'}&quot;,\n                        f&quot;*Source: {content.uri.sourcetype.value}*&quot;,\n                        &quot;&quot;,\n                        insights,\n                        &quot;&quot;,\n                    ]\n                )\n\n        # Add strategic action items\n        action_items = self._generate_action_items()\n        if action_items:\n            snapshot_lines.extend([&quot;## Strategic Actions&quot;, &quot;&quot;, action_items, &quot;&quot;])\n\n        return &quot;\\n&quot;.join(snapshot_lines)\n\n    def save_snapshot(self) -&gt; str:\n        &quot;&quot;&quot;\n        Save current snapshot to file and create version history.\n\n        Returns:\n            Version identifier of saved snapshot\n        &quot;&quot;&quot;\n        snapshot_content = self.get_current_snapshot()\n\n        # Save to current snapshot file\n        self._snapshot_file.write_text(snapshot_content)\n\n        # Create versioned snapshot for history\n        version = f&quot;v{len(self._snapshot_history) + 1}.{int(time.time())}&quot;\n\n        snapshot_record = SourdoughSnapshot(\n            timestamp=int(time.time()),\n            version=version,\n            content_count=len(self._corpus),\n            token_count=self._estimate_token_count(),\n            summary=self._generate_executive_summary(),\n        )\n\n        self._snapshot_history.append(snapshot_record)\n\n        # Save versioned snapshot file\n        versioned_file = Path(f&quot;strategy_snapshot_{version}.md&quot;)\n        versioned_file.write_text(snapshot_content)\n\n        print(f&quot;Saved snapshot {version} with {len(self._corpus)} items&quot;)\n        return version\n\n    def load_snapshot(self, version: str) -&gt; bool:\n        &quot;&quot;&quot;\n        Load a specific snapshot version.\n\n        Args:\n            version: Version identifier to load\n\n        Returns:\n            True if snapshot was loaded successfully\n        &quot;&quot;&quot;\n        # TODO: Implement snapshot loading from versioned files\n        # This would restore the corpus state from a specific point in time\n        raise NotImplementedError(&quot;Snapshot loading not yet implemented&quot;)\n\n    # ============================================================================\n    # Strategic Intelligence Operations\n    # ============================================================================\n\n    def _extract_strategic_themes(self) -&gt; dict[str, list[ProcessedContent]]:\n        &quot;&quot;&quot;\n        Group content by strategic themes for better organization.\n\n        Returns:\n            Dictionary mapping theme names to content lists\n        &quot;&quot;&quot;\n        themes = {\n            &quot;Priorities &amp; Objectives&quot;: [],\n            &quot;Active Projects&quot;: [],\n            &quot;Market Intelligence&quot;: [],\n            &quot;Organizational Updates&quot;: [],\n            &quot;Technical Strategy&quot;: [],\n            &quot;Other Strategic Content&quot;: [],\n        }\n\n        for content in self._corpus:\n            content_text = (\n                content.title + &quot; &quot; + content.description + &quot; &quot; + content.context[:500]\n            ).lower()\n\n            # Simple keyword-based theme classification\n            if any(\n                keyword in content_text\n                for keyword in [&quot;okr&quot;, &quot;priority&quot;, &quot;goal&quot;, &quot;objective&quot;, &quot;target&quot;]\n            ):\n                themes[&quot;Priorities &amp; Objectives&quot;].append(content)\n            elif any(\n                keyword in content_text\n                for keyword in [&quot;project&quot;, &quot;initiative&quot;, &quot;launch&quot;, &quot;beta&quot;]\n            ):\n                themes[&quot;Active Projects&quot;].append(content)\n            elif any(\n                keyword in content_text\n                for keyword in [&quot;market&quot;, &quot;competitor&quot;, &quot;industry&quot;, &quot;customer&quot;]\n            ):\n                themes[&quot;Market Intelligence&quot;].append(content)\n            elif any(\n                keyword in content_text\n                for keyword in [&quot;hiring&quot;, &quot;team&quot;, &quot;organization&quot;, &quot;culture&quot;]\n            ):\n                themes[&quot;Organizational Updates&quot;].append(content)\n            elif any(\n                keyword in content_text\n                for keyword in [\n                    &quot;technical&quot;,\n                    &quot;architecture&quot;,\n                    &quot;platform&quot;,\n                    &quot;infrastructure&quot;,\n                ]\n            ):\n                themes[&quot;Technical Strategy&quot;].append(content)\n            else:\n                themes[&quot;Other Strategic Content&quot;].append(content)\n\n        # Remove empty themes\n        return {theme: contents for theme, contents in themes.items() if contents}\n\n    def _extract_key_insights(self, content: ProcessedContent) -&gt; str:\n        &quot;&quot;&quot;\n        Extract key strategic insights from a piece of content.\n\n        Args:\n            content: ProcessedContent to extract insights from\n\n        Returns:\n            Formatted insights text\n        &quot;&quot;&quot;\n        # Use the summary if available, otherwise create excerpt\n        if content.summary:\n            return content.summary[:300] + (&quot;...&quot; if len(content.summary) &gt; 300 else &quot;&quot;)\n        elif content.description:\n            return content.description\n        else:\n            # Extract first meaningful paragraph from context\n            paragraphs = content.context.split(&quot;\\n\\n&quot;)\n            for para in paragraphs:\n                if len(para.strip()) &gt; 50:  # Skip short lines\n                    excerpt = para.strip()[:300]\n                    return excerpt + (&quot;...&quot; if len(para) &gt; 300 else &quot;&quot;)\n\n            return &quot;No summary available.&quot;\n\n    def _generate_action_items(self) -&gt; str:\n        &quot;&quot;&quot;\n        Generate strategic action items based on current content.\n\n        Returns:\n            Formatted action items text\n        &quot;&quot;&quot;\n        # TODO: Implement AI-powered action item generation\n        # This could use LLM to analyze content and suggest strategic actions\n        return &quot;- Review strategic priorities based on recent updates\\n- Monitor progress on active projects\\n- Assess market intelligence for opportunities&quot;\n\n    def _generate_executive_summary(self) -&gt; str:\n        &quot;&quot;&quot;\n        Generate high-level executive summary of strategic state.\n\n        Returns:\n            Executive summary text\n        &quot;&quot;&quot;\n        if self._corpus.is_empty():\n            return &quot;No strategic content available for summary.&quot;\n\n        theme_count = len(self._extract_strategic_themes())\n        content_count = len(self._corpus)\n\n        return (\n            f&quot;Strategic snapshot containing {content_count} key items across &quot;\n            f&quot;{theme_count} strategic themes. Focus areas: {', '.join(self.focus_areas) if self.focus_areas else 'general strategy'}.&quot;\n        )\n\n    # ============================================================================\n    # Monitoring &amp; Health Operations\n    # ============================================================================\n\n    def detect_drift(self) -&gt; dict[str, Any]:\n        &quot;&quot;&quot;\n        Detect if strategic content has significantly changed (drift detection).\n\n        Returns:\n            Drift analysis with recommendations\n        &quot;&quot;&quot;\n        if len(self._snapshot_history) &lt; 2:\n            return {\n                &quot;drift_detected&quot;: False,\n                &quot;reason&quot;: &quot;Insufficient history for comparison&quot;,\n            }\n\n        current_snapshot = self._snapshot_history[-1]\n        previous_snapshot = self._snapshot_history[-2]\n\n        # Calculate content change percentage\n        content_change = abs(\n            current_snapshot.content_count - previous_snapshot.content_count\n        )\n        change_percentage = content_change / max(previous_snapshot.content_count, 1)\n\n        drift_detected = change_percentage &gt; self._drift_threshold\n\n        return {\n            &quot;drift_detected&quot;: drift_detected,\n            &quot;change_percentage&quot;: change_percentage,\n            &quot;threshold&quot;: self._drift_threshold,\n            &quot;recommendation&quot;: &quot;Consider reviewing strategic focus areas&quot;\n            if drift_detected\n            else &quot;Strategic content is stable&quot;,\n            &quot;content_change&quot;: content_change,\n            &quot;timespan_hours&quot;: (current_snapshot.timestamp - previous_snapshot.timestamp)\n            / 3600,\n        }\n\n    def get_health_status(self) -&gt; dict[str, Any]:\n        &quot;&quot;&quot;\n        Get comprehensive health status of the Sourdough.\n\n        Returns:\n            Health status and metrics\n        &quot;&quot;&quot;\n        current_tokens = self._estimate_token_count()\n        token_utilization = current_tokens / self.max_tokens\n\n        # Assess content freshness (how recent is the newest content)\n        hours_since_update = (time.time() - self._last_update) / 3600\n\n        # Health scoring\n        health_score = 1.0\n        health_issues = []\n\n        if token_utilization &gt; 0.9:\n            health_score -= 0.3\n            health_issues.append(&quot;Near token limit&quot;)\n\n        if hours_since_update &gt; 168:  # 1 week\n            health_score -= 0.2\n            health_issues.append(&quot;Content may be stale&quot;)\n\n        if self._corpus.is_empty():\n            health_score = 0.0\n            health_issues.append(&quot;No strategic content&quot;)\n\n        return {\n            &quot;health_score&quot;: health_score,\n            &quot;status&quot;: &quot;healthy&quot;\n            if health_score &gt; 0.7\n            else &quot;needs_attention&quot;\n            if health_score &gt; 0.3\n            else &quot;unhealthy&quot;,\n            &quot;token_utilization&quot;: token_utilization,\n            &quot;content_count&quot;: len(self._corpus),\n            &quot;hours_since_update&quot;: hours_since_update,\n            &quot;focus_areas&quot;: self.focus_areas,\n            &quot;issues&quot;: health_issues,\n            &quot;last_snapshot&quot;: self._snapshot_history[-1].version\n            if self._snapshot_history\n            else &quot;none&quot;,\n        }\n\n    # ============================================================================\n    # Utility Methods &amp; Delegation to ProcessedCorpus\n    # ============================================================================\n\n    def _estimate_token_count(self) -&gt; int:\n        &quot;&quot;&quot;\n        Estimate total token count of current corpus.\n\n        Returns:\n            Estimated token count\n        &quot;&quot;&quot;\n        if self._corpus.is_empty():\n            return 0\n\n        # Rough estimation: 1 token \u2248 0.75 words\n        total_words = sum(len(content.context.split()) for content in self._corpus)\n        return int(total_words * 1.3)\n\n    # Delegate common operations to the internal corpus\n    def search(self, query: str) -&gt; ProcessedCorpus:\n        &quot;&quot;&quot;Search within strategic content.&quot;&quot;&quot;\n        return self._corpus.search(query)\n\n    def filter_by_source_type(self, source_type: SourceType) -&gt; ProcessedCorpus:\n        &quot;&quot;&quot;Filter strategic content by source type.&quot;&quot;&quot;\n        return self._corpus.filter_by_source_type(source_type)\n\n    def get_source_type_counts(self) -&gt; dict[SourceType, int]:\n        &quot;&quot;&quot;Get breakdown of content by source type.&quot;&quot;&quot;\n        return self._corpus.get_source_type_counts()\n\n    def __len__(self) -&gt; int:\n        &quot;&quot;&quot;Return number of items in strategic corpus.&quot;&quot;&quot;\n        return len(self._corpus)\n\n    def __repr__(self) -&gt; str:\n        &quot;&quot;&quot;String representation of Sourdough.&quot;&quot;&quot;\n        health = self.get_health_status()\n        return (\n            f&quot;Sourdough(content={len(self._corpus)}, &quot;\n            f&quot;tokens={self._estimate_token_count()}/{self.max_tokens}, &quot;\n            f&quot;health={health['status']})&quot;\n        )\n\n    # ============================================================================\n    # Configuration &amp; Management\n    # ============================================================================\n\n    def update_focus_areas(self, focus_areas: list[str]) -&gt; None:\n        &quot;&quot;&quot;Update strategic focus areas and re-score content.&quot;&quot;&quot;\n        self.focus_areas = focus_areas\n        if self.auto_prune:\n            self._auto_prune()  # Re-evaluate with new focus areas\n\n    def set_token_limit(self, max_tokens: int) -&gt; None:\n        &quot;&quot;&quot;Update token limit and prune if necessary.&quot;&quot;&quot;\n        self.max_tokens = max_tokens\n        if self.auto_prune:\n            self._auto_prune()\n\n    def export_for_llm(self) -&gt; str:\n        &quot;&quot;&quot;\n        Export current strategic content in format optimized for LLM consumption.\n\n        Returns:\n            Formatted strategic context for LLM prompts\n        &quot;&quot;&quot;\n        snapshot = self.get_current_snapshot()\n\n        # Add metadata for LLM context\n        context_header = [\n            &quot;# Strategic Context&quot;,\n            f&quot;Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}&quot;,\n            f&quot;Content items: {len(self._corpus)}&quot;,\n            f&quot;Focus areas: {', '.join(self.focus_areas) if self.focus_areas else 'General strategy'}&quot;,\n            &quot;&quot;,\n            &quot;---&quot;,\n            &quot;&quot;,\n        ]\n\n        return &quot;\\n&quot;.join(context_header) + snapshot\n</file>\n    <file path=\"src/siphon/collections/query/snapshot.py\">&quot;&quot;&quot;\nPostgreSQL cache snapshot utility with Rich formatting\n&quot;&quot;&quot;\n\nfrom siphon.database.postgres.PGRES_connection import get_db_connection\nfrom psycopg2.extras import RealDictCursor\nfrom datetime import datetime, timedelta\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\nfrom rich.text import Text\nfrom rich import box\n\n\nconsole = Console()\n\n# Source type styling\nSOURCE_STYLES = {\n    &quot;YouTube&quot;: (&quot;\ud83d\udcfa&quot;, &quot;red&quot;),\n    &quot;Article&quot;: (&quot;\ud83d\udcc4&quot;, &quot;blue&quot;),\n    &quot;Doc&quot;: (&quot;\ud83d\udccb&quot;, &quot;green&quot;),\n    &quot;Image&quot;: (&quot;\ud83d\uddbc\ufe0f&quot;, &quot;magenta&quot;),\n    &quot;Audio&quot;: (&quot;\ud83c\udfb5&quot;, &quot;yellow&quot;),\n    &quot;GitHub&quot;: (&quot;\ud83d\udc19&quot;, &quot;dark purple&quot;),\n    &quot;Drive&quot;: (&quot;\ud83d\udcbe&quot;, &quot;cyan&quot;),\n}\n\n\ndef get_total_count():\n    &quot;&quot;&quot;Total cached items&quot;&quot;&quot;\n    with get_db_connection() as conn, conn.cursor() as cur:\n        cur.execute(&quot;SELECT COUNT(*) FROM processed_content&quot;)\n        return cur.fetchone()[0]\n\n\ndef format_size(chars):\n    &quot;&quot;&quot;Format character count in human readable form&quot;&quot;&quot;\n    if chars &lt; 1000:\n        return f&quot;{chars}&quot;\n    elif chars &lt; 1000000:\n        return f&quot;{chars / 1000:.1f}K&quot;\n    else:\n        return f&quot;{chars / 1000000:.1f}M&quot;\n\n\ndef create_header():\n    &quot;&quot;&quot;Create header panel with total count&quot;&quot;&quot;\n    total = get_total_count()\n    header_text = Text.assemble(\n        (&quot;\ud83d\udcda &quot;, &quot;bright_blue&quot;),\n        (f&quot;{total:,}&quot;, &quot;bold bright_white&quot;),\n        (&quot; Total Items&quot;, &quot;bright_blue&quot;),\n    )\n    return Panel(\n        header_text,\n        title=&quot;[bold bright_cyan]SIPHON LIBRARY SNAPSHOT[/bold bright_cyan]&quot;,\n        border_style=&quot;bright_cyan&quot;,\n        box=box.DOUBLE,\n    )\n\n\ndef create_source_distribution():\n    &quot;&quot;&quot;Create source type distribution chart&quot;&quot;&quot;\n    with get_db_connection() as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:\n        cur.execute(&quot;&quot;&quot;\n           SELECT data-&gt;&gt;'sourcetype' as source_type, COUNT(*) as count \n           FROM processed_content \n           GROUP BY data-&gt;&gt;'sourcetype' \n           ORDER BY count DESC\n       &quot;&quot;&quot;)\n        results = cur.fetchall()\n\n        if not results:\n            return Panel(&quot;No data found&quot;, title=&quot;Source Distribution&quot;)\n\n        table = Table(show_header=False, box=None, padding=(0, 1))\n        table.add_column(&quot;Type&quot;, width=12)\n        table.add_column(&quot;Bar&quot;, min_width=20)\n        table.add_column(&quot;Count&quot;, justify=&quot;right&quot;, width=6)\n\n        max_count = max(r[&quot;count&quot;] for r in results)\n\n        for row in results:\n            source_type = row[&quot;source_type&quot;]\n            count = row[&quot;count&quot;]\n            icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n            # Calculate bar width (max 30 chars)\n            bar_width = int((count / max_count) * 30)\n            bar = &quot;\u2588&quot; * bar_width\n\n            type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n            bar_text = Text(bar, style=color)\n            count_text = Text(str(count), style=&quot;bold white&quot;)\n\n            table.add_row(type_text, bar_text, count_text)\n\n        return Panel(\n            table,\n            title=&quot;[bold green]\ud83d\udcca Source Distribution[/bold green]&quot;,\n            border_style=&quot;green&quot;,\n        )\n\n\ndef create_recent_additions(hours=24):\n    &quot;&quot;&quot;Create recent additions panel&quot;&quot;&quot;\n    with get_db_connection() as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:\n        cutoff = datetime.now() - timedelta(hours=hours)\n        cur.execute(\n            &quot;&quot;&quot;\n           SELECT data-&gt;&gt;'sourcetype' as source_type, \n                  COALESCE(data-&gt;'synthetic_data'-&gt;&gt;'title', 'Untitled') as title,\n                  created_at\n           FROM processed_content \n           WHERE created_at &gt; %s \n           ORDER BY created_at DESC \n           LIMIT 10\n       &quot;&quot;&quot;,\n            (cutoff,),\n        )\n\n        results = cur.fetchall()\n\n        if not results:\n            content = Text(f&quot;No additions in last {hours}h&quot;, style=&quot;dim&quot;)\n        else:\n            table = Table(show_header=False, box=None)\n            table.add_column(&quot;Time&quot;, width=11)\n            table.add_column(&quot;Type&quot;, width=12)\n            table.add_column(&quot;Title&quot;, max_width=40)\n\n            for row in results:\n                source_type = row[&quot;source_type&quot;]\n                icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n                time_text = Text(\n                    row[&quot;created_at&quot;].strftime(&quot;%m/%d %H:%M&quot;), style=&quot;cyan&quot;\n                )\n                type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n                title = (row[&quot;title&quot;] or &quot;Untitled&quot;)[:40].replace(&quot;\\n&quot;, &quot; &quot;)\n                title_text = Text(title, style=&quot;white&quot;)\n\n                table.add_row(time_text, type_text, title_text)\n\n            content = table\n\n        return Panel(\n            content,\n            title=f&quot;[bold yellow]\ud83d\udd52 Recent ({hours}h)[/bold yellow]&quot;,\n            border_style=&quot;yellow&quot;,\n        )\n\n\ndef create_latest_items():\n    &quot;&quot;&quot;Create latest items panel&quot;&quot;&quot;\n    with get_db_connection() as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:\n        cur.execute(&quot;&quot;&quot;\n           SELECT data-&gt;&gt;'sourcetype' as source_type,\n                  COALESCE(data-&gt;'synthetic_data'-&gt;&gt;'title', 'Untitled') as title,\n                  created_at\n           FROM processed_content \n           ORDER BY created_at DESC \n           LIMIT 3\n       &quot;&quot;&quot;)\n\n        results = cur.fetchall()\n        table = Table(show_header=False, box=None)\n        table.add_column(&quot;Time&quot;, width=11)\n        table.add_column(&quot;Type&quot;, width=12)\n        table.add_column(&quot;Title&quot;, max_width=70)\n\n        for row in results:\n            source_type = row[&quot;source_type&quot;]\n            icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n            time_text = Text(\n                row[&quot;created_at&quot;].strftime(&quot;%m/%d %H:%M&quot;), style=&quot;bright_green&quot;\n            )\n            type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n            title = (row[&quot;title&quot;] or &quot;Untitled&quot;)[:70].replace(&quot;\\n&quot;, &quot; &quot;)\n            title_text = Text(title, style=&quot;dim&quot;)\n\n            table.add_row(time_text, type_text, title_text)\n\n        return Panel(\n            table,\n            title=&quot;[bold bright_green]\u2b50 Latest Items[/bold bright_green]&quot;,\n            border_style=&quot;bright_green&quot;,\n        )\n\n\ndef create_size_extremes():\n    &quot;&quot;&quot;Create size extremes panel&quot;&quot;&quot;\n    with get_db_connection() as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:\n        # Get largest\n        cur.execute(&quot;&quot;&quot;\n           SELECT data-&gt;&gt;'sourcetype' as source_type,\n                  COALESCE(data-&gt;'synthetic_data'-&gt;&gt;'title', 'Untitled') as title,\n                  LENGTH(data-&gt;'context_data'-&gt;&gt;'context') as size\n           FROM processed_content \n           WHERE data-&gt;'context_data'-&gt;&gt;'context' IS NOT NULL\n           ORDER BY size DESC LIMIT 3\n       &quot;&quot;&quot;)\n        largest = cur.fetchall()\n\n        # Get smallest\n        cur.execute(&quot;&quot;&quot;\n           SELECT data-&gt;&gt;'sourcetype' as source_type,\n                  COALESCE(data-&gt;'synthetic_data'-&gt;&gt;'title', 'Untitled') as title,\n                  LENGTH(data-&gt;'context_data'-&gt;&gt;'context') as size\n           FROM processed_content \n           WHERE data-&gt;'context_data'-&gt;&gt;'context' IS NOT NULL\n           ORDER BY size ASC LIMIT 3\n       &quot;&quot;&quot;)\n        smallest = cur.fetchall()\n\n        table = Table(show_header=False, box=None)\n        table.add_column(&quot;Size&quot;, width=8, justify=&quot;right&quot;)\n        table.add_column(&quot;Type&quot;, width=12)\n        table.add_column(&quot;Title&quot;, max_width=70)\n\n        # Add largest\n        for i, row in enumerate(largest):\n            source_type = row[&quot;source_type&quot;]\n            icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n            size_text = Text(f&quot;{format_size(row['size'])} &quot;, style=&quot;red&quot;)\n            type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n            title = (row[&quot;title&quot;] or &quot;Untitled&quot;)[:70].replace(&quot;\\n&quot;, &quot; &quot;)\n            title_text = Text(title, style=&quot;dim&quot;)\n\n            table.add_row(size_text, type_text, title_text)\n\n        # Add separator\n        table.add_row(&quot;&quot;, &quot;&quot;, &quot;&quot;)\n\n        # Add smallest\n        for row in smallest:\n            source_type = row[&quot;source_type&quot;]\n            icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n            size_text = Text(f&quot;{format_size(row['size'])} &quot;, style=&quot;green&quot;)\n            type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n            title = (row[&quot;title&quot;] or &quot;Untitled&quot;)[:70].replace(&quot;\\n&quot;, &quot; &quot;)\n            title_text = Text(title, style=&quot;dim&quot;)\n\n            table.add_row(size_text, type_text, title_text)\n\n        return Panel(\n            table,\n            title=&quot;[bold magenta]\ud83d\udccf Size Extremes[/bold magenta]&quot;,\n            border_style=&quot;magenta&quot;,\n        )\n\n\ndef search_by_source_type(source_type):\n    &quot;&quot;&quot;Show items for specific source type&quot;&quot;&quot;\n    with get_db_connection() as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:\n        cur.execute(\n            &quot;&quot;&quot;\n           SELECT COALESCE(data-&gt;'synthetic_data'-&gt;&gt;'title', 'Untitled') as title,\n                  created_at\n           FROM processed_content \n           WHERE data-&gt;&gt;'sourcetype' = %s \n           ORDER BY created_at DESC \n           LIMIT 20\n       &quot;&quot;&quot;,\n            (source_type,),\n        )\n\n        results = cur.fetchall()\n\n        if not results:\n            console.print(f&quot;[red]No {source_type} items found[/red]&quot;)\n            return\n\n        icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n        table = Table(show_header=True, box=box.MINIMAL)\n        table.add_column(&quot;Date&quot;, width=11)\n        table.add_column(&quot;Title&quot;, max_width=60)\n\n        for row in results:\n            date_text = row[&quot;created_at&quot;].strftime(&quot;%m/%d %H:%M&quot;)\n            title = (row[&quot;title&quot;] or &quot;Untitled&quot;)[:60]\n            table.add_row(date_text, title)\n\n        panel = Panel(\n            table,\n            title=f&quot;[bold {color}]{icon} {source_type} Items ({len(results)})[/bold {color}]&quot;,\n            border_style=color,\n        )\n        console.print(panel)\n\n\ndef generate_snapshot_for_list(corpus_list: list, console: Console = console):\n    &quot;&quot;&quot;Generate snapshot for in-memory corpus list&quot;&quot;&quot;\n\n    def create_header():\n        &quot;&quot;&quot;Create header panel with total count&quot;&quot;&quot;\n        header_text = Text.assemble(\n            (&quot;\ud83d\udcda &quot;, &quot;bright_blue&quot;),\n            (f&quot;{len(corpus_list):,}&quot;, &quot;bold bright_white&quot;),\n            (&quot; Items in Corpus&quot;, &quot;bright_blue&quot;),\n        )\n        return Panel(\n            header_text,\n            title=&quot;[bold bright_cyan]CORPUS SNAPSHOT[/bold bright_cyan]&quot;,\n            border_style=&quot;bright_cyan&quot;,\n            box=box.DOUBLE,\n        )\n\n    def create_source_distribution():\n        &quot;&quot;&quot;Create source type distribution chart&quot;&quot;&quot;\n        # Count by source type\n        source_counts = {}\n        for item in corpus_list:\n            source_type = item.uri.sourcetype.value\n            source_counts[source_type] = source_counts.get(source_type, 0) + 1\n\n        if not source_counts:\n            return Panel(&quot;No data found&quot;, title=&quot;Source Distribution&quot;)\n\n        # Sort by count descending\n        sorted_sources = sorted(source_counts.items(), key=lambda x: x[1], reverse=True)\n\n        table = Table(show_header=False, box=None, padding=(0, 1))\n        table.add_column(&quot;Type&quot;, width=12)\n        table.add_column(&quot;Bar&quot;, min_width=20)\n        table.add_column(&quot;Count&quot;, justify=&quot;right&quot;, width=6)\n\n        max_count = max(source_counts.values())\n\n        for source_type, count in sorted_sources:\n            icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n            # Calculate bar width (max 30 chars)\n            bar_width = int((count / max_count) * 30)\n            bar = &quot;\u2588&quot; * bar_width\n\n            type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n            bar_text = Text(bar, style=color)\n            count_text = Text(str(count), style=&quot;bold white&quot;)\n\n            table.add_row(type_text, bar_text, count_text)\n\n        return Panel(\n            table,\n            title=&quot;[bold green]\ud83d\udcca Source Distribution[/bold green]&quot;,\n            border_style=&quot;green&quot;,\n        )\n\n    def create_latest_items():\n        &quot;&quot;&quot;Create latest items panel - shows first 3 items&quot;&quot;&quot;\n        # Take first 3 items (no timestamp sorting available)\n        latest_items = corpus_list[:3]\n\n        if not latest_items:\n            content = Text(&quot;No items available&quot;, style=&quot;dim&quot;)\n        else:\n            table = Table(show_header=False, box=None)\n            table.add_column(&quot;Index&quot;, width=8)\n            table.add_column(&quot;Type&quot;, width=12)\n            table.add_column(&quot;Title&quot;, max_width=70)\n\n            for i, item in enumerate(latest_items, 1):\n                source_type = item.uri.sourcetype.value\n                icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n                index_text = Text(f&quot;#{i}&quot;, style=&quot;bright_green&quot;)\n                type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n                title = (item.title or &quot;Untitled&quot;)[:70].replace(&quot;\\n&quot;, &quot; &quot;)\n                title_text = Text(title, style=&quot;dim&quot;)\n\n                table.add_row(index_text, type_text, title_text)\n\n            content = table\n\n        return Panel(\n            content,\n            title=&quot;[bold bright_green]\u2b50 Sample Items[/bold bright_green]&quot;,\n            border_style=&quot;bright_green&quot;,\n        )\n\n    def create_size_extremes():\n        &quot;&quot;&quot;Create size extremes panel&quot;&quot;&quot;\n        if not corpus_list:\n            return Panel(&quot;No items available&quot;, title=&quot;Size Extremes&quot;)\n\n        # Calculate sizes and sort\n        items_with_sizes = []\n        for item in corpus_list:\n            context_length = len(item.context) if item.context else 0\n            items_with_sizes.append((item, context_length))\n\n        # Sort by size\n        items_with_sizes.sort(key=lambda x: x[1], reverse=True)\n\n        # Get largest 3 and smallest 3\n        largest = items_with_sizes[:3]\n        smallest = items_with_sizes[-3:] if len(items_with_sizes) &gt; 3 else []\n\n        table = Table(show_header=False, box=None)\n        table.add_column(&quot;Size&quot;, width=8, justify=&quot;right&quot;)\n        table.add_column(&quot;Type&quot;, width=12)\n        table.add_column(&quot;Title&quot;, max_width=70)\n\n        # Add largest\n        for item, size in largest:\n            source_type = item.uri.sourcetype.value\n            icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n            size_text = Text(f&quot;{format_size(size)} &quot;, style=&quot;red&quot;)\n            type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n            title = (item.title or &quot;Untitled&quot;)[:70].replace(&quot;\\n&quot;, &quot; &quot;)\n            title_text = Text(title, style=&quot;dim&quot;)\n\n            table.add_row(size_text, type_text, title_text)\n\n        # Add separator if we have both largest and smallest\n        if smallest:\n            table.add_row(&quot;&quot;, &quot;&quot;, &quot;&quot;)\n\n        # Add smallest\n        for item, size in smallest:\n            source_type = item.uri.sourcetype.value\n            icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n            size_text = Text(f&quot;{format_size(size)} &quot;, style=&quot;green&quot;)\n            type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n            title = (item.title or &quot;Untitled&quot;)[:70].replace(&quot;\\n&quot;, &quot; &quot;)\n            title_text = Text(title, style=&quot;dim&quot;)\n\n            table.add_row(size_text, type_text, title_text)\n\n        return Panel(\n            table,\n            title=&quot;[bold magenta]\ud83d\udccf Size Extremes[/bold magenta]&quot;,\n            border_style=&quot;magenta&quot;,\n        )\n\n    def create_content_summary():\n        &quot;&quot;&quot;Create a summary of content types and characteristics&quot;&quot;&quot;\n        # Count items with various attributes\n        with_titles = sum(1 for item in corpus_list if item.title)\n        with_descriptions = sum(1 for item in corpus_list if item.description)\n        with_summaries = sum(1 for item in corpus_list if item.summary)\n        with_tags = sum(1 for item in corpus_list if item.tags)\n\n        total_context_chars = sum(\n            len(item.context) for item in corpus_list if item.context\n        )\n        avg_context_size = total_context_chars // len(corpus_list) if corpus_list else 0\n\n        table = Table(show_header=False, box=None, padding=(0, 1))\n        table.add_column(&quot;Metric&quot;, width=20)\n        table.add_column(&quot;Value&quot;, width=15, justify=&quot;right&quot;)\n        table.add_column(&quot;Percentage&quot;, width=10, justify=&quot;right&quot;)\n\n        metrics = [\n            (&quot;Items with titles&quot;, with_titles),\n            (&quot;Items with descriptions&quot;, with_descriptions),\n            (&quot;Items with summaries&quot;, with_summaries),\n            (&quot;Items with tags&quot;, with_tags),\n        ]\n\n        for metric_name, count in metrics:\n            percentage = (\n                f&quot;{(count / len(corpus_list) * 100):.1f}%&quot; if corpus_list else &quot;0%&quot;\n            )\n            table.add_row(\n                Text(metric_name, style=&quot;cyan&quot;),\n                Text(str(count), style=&quot;white&quot;),\n                Text(percentage, style=&quot;dim&quot;),\n            )\n\n        # Add average size row\n        table.add_row(&quot;&quot;, &quot;&quot;, &quot;&quot;)  # Separator\n        table.add_row(\n            Text(&quot;Avg context size&quot;, style=&quot;cyan&quot;),\n            Text(format_size(avg_context_size), style=&quot;white&quot;),\n            Text(&quot;&quot;, style=&quot;dim&quot;),\n        )\n\n        return Panel(\n            table,\n            title=&quot;[bold blue]\ud83d\udcc8 Content Summary[/bold blue]&quot;,\n            border_style=&quot;blue&quot;,\n        )\n\n    # Generate the full snapshot\n    if not corpus_list:\n        console.print(\n            Panel(&quot;Empty corpus - no items to display&quot;, title=&quot;Corpus Snapshot&quot;)\n        )\n        return\n\n    console.print(create_header())\n    console.print()\n    console.print(create_source_distribution())\n    console.print()\n    console.print(create_content_summary())\n    console.print()\n    console.print(create_latest_items())\n    console.print()\n    console.print(create_size_extremes())\n\n\ndef generate_snapshot_for_library(console: Console = console):\n    # Default overview\n    console.print(create_header())\n    console.print()\n    console.print(create_source_distribution())\n    console.print()\n    console.print(create_recent_additions())\n    console.print()\n    console.print(create_latest_items())\n    console.print()\n    console.print(create_size_extremes())\n\n\ndef generate_snapshot(\n    console: Console = console, corpus_list: list | None = None, library: bool = True\n):\n    if library:\n        generate_snapshot_for_library(console)\n    elif corpus_list:\n        generate_snapshot_for_list(corpus_list, console)\n    else:\n        console.print(\n            &quot;[red]Error: Must specify either library=True or submit a corpus_list[/red]&quot;\n        )\n\n\ndef main():\n    generate_snapshot_for_library()\n\n\nif __name__ == &quot;__main__&quot;:\n    from siphon.collections.corpus.siphon_corpus import CorpusFactory\n\n    # Create a corpus and convert to list\n    corpus = CorpusFactory.from_library()\n    corpus_list = list(corpus)\n\n    # Generate snapshot\n    generate_snapshot_for_list(corpus_list)\n</file>\n    <file path=\"src/siphon/collections/query/siphon_query.py\">&quot;&quot;&quot;\nA SiphonQuery class to handle siphon queries.\n\nThis provides a universal interface for querying a ProcessedCorpus object, whether it's the entire library, a subset, a created-on-the-fly corpus (from a directory, fex.), etc.\n\nThis module is part of the Siphon project, which is designed to manage and process content from various sources.\n\nThis embraces the monad pattern, meaning you can daisy conduit methods together to create complex queries, for example:\nSiphonQuery().last().filter_by_date(&quot;2023-01-01&quot;).filter_by_source(&quot;example_source&quot;). This is achieved by returning a new SiphonQuery instance with the updated parameters at each step.\n\nSiphonQueries are able to access metadata, content, and other attrs in a SQL-like manner, but also have access to vector database queries for semantic search, graph queries for GraphRAG, and synthetic/AI-generated tagging/summarization access. These are generated by the ProcessedCorpus objects, but in a lazy manner -- i.e. when initiated by a specific SiphonQuery method. Example:\nclass SiphonQuery:\n    def semantic_search(self, query: str, k: int = 10) -&gt; 'SiphonQuery':\n        # This triggers vector DB creation if it doesn't exist\n        if not self.corpus.has_vector_index():\n            self.corpus._create_vector_index()  # Happens here, not at init\n        # Then do the search\n        return self._add_operation('semantic_search', query, k)\n\nNOTE: for now, SiphonQuery has all the query logic, and performs on an InMemoryCorpus. In the future, we would want to abstract these methods into InMemoryCorpus, as well as DatabaseCorpus (i.e. SQL and our persistent databases).\n&quot;&quot;&quot;\n\nfrom siphon.data.processed_content import ProcessedContent\nfrom siphon.logs.logging_config import get_logger\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.collections.corpus.siphon_corpus import (\n    SiphonCorpus,\n    InMemoryCorpus,\n)\nfrom typing import Literal\n\nlogger = get_logger(__name__)\n\n\nclass SiphonQuery:\n    &quot;&quot;&quot;\n    Monadic query interface for fluent corpus operations\n    &quot;&quot;&quot;\n\n    def __init__(self, corpus: SiphonCorpus):\n        self.corpus: SiphonCorpus = corpus\n\n    # ========================================================================\n    # Monadic operations return new SiphonQuery instances\n    # ========================================================================\n\n    # Filtering Operations (return new SiphonQuery)\n\n    def filter_by_source_type(\n        self, source_type: SourceType | list[SourceType]\n    ) -&gt; &quot;SiphonQuery&quot;:\n        # Coerce sourcetype to list if not already\n        if not isinstance(source_type, list):\n            source_type = [source_type]\n        filtered = self.corpus.filter(lambda x: x.source_type in source_type)\n        return SiphonQuery(filtered)\n\n    def filter_by_date_range(self, start_date: str, end_date: str) -&gt; &quot;SiphonQuery&quot;:\n        &quot;&quot;&quot;\n        Dates are ISO format strings: &quot;YYYY-MM-DD&quot;\n        &quot;&quot;&quot;\n        filtered = self.corpus.filter(lambda x: start_date &lt;= x.date &lt;= end_date)\n        return SiphonQuery(filtered)\n\n    def filter_by_tags(self, tags: list[str]) -&gt; &quot;SiphonQuery&quot;: ...\n\n    def filter_by_content(self, search_term: str) -&gt; &quot;SiphonQuery&quot;:\n        filtered = self.corpus.filter(lambda x: search_term in x.context)\n        return SiphonQuery(filtered)\n\n    def filter_by_title(self, title_pattern: str) -&gt; &quot;SiphonQuery&quot;:\n        filtered = self.corpus.filter(lambda x: title_pattern in x.title)\n        return SiphonQuery(filtered)\n\n    # Ordering &amp; Limiting (return new SiphonQuery)\n\n    def order_by_date(self, ascending: bool = True) -&gt; &quot;SiphonQuery&quot;:\n        ordered = self.corpus.sort(key=lambda x: x.date, reverse=not ascending)\n        return SiphonQuery(ordered)\n\n    def order_by_title(self, ascending: bool = True) -&gt; &quot;SiphonQuery&quot;:\n        ordered = self.corpus.sort(key=lambda x: x.title, reverse=not ascending)\n        return SiphonQuery(ordered)\n\n    def limit(self, n: int) -&gt; &quot;SiphonQuery&quot;:\n        limited = self.corpus[:n]\n        return SiphonQuery(limited)\n\n    def offset(self, n: int) -&gt; &quot;SiphonQuery&quot;:\n        offsetted = self.corpus[n:]\n        return SiphonQuery(offsetted)\n\n    def paginate(self, page_size: int, page_number: int) -&gt; &quot;SiphonQuery&quot;:\n        start = page_size * (page_number - 1)\n        end = start + page_size\n        paginated = self.corpus[start:end]\n        return SiphonQuery(paginated)\n\n    # Advanced Queries (return new SiphonQuery)\n\n    def semantic_search(\n        self,\n        query: str,\n        k: int = 10,\n        dataset: Literal[&quot;description&quot;, &quot;chunked&quot;] = &quot;description&quot;,\n    ) -&gt; &quot;SiphonQuery&quot;:\n        # This triggers vector DB creation if it doesn't exist\n        if not self.corpus.vector_store_created:\n            self.corpus._create_vector_store()\n        match dataset:\n            case &quot;description&quot;:\n                collection = self.corpus.description_collection\n            case &quot;chunked&quot;:\n                collection = self.corpus.chunked_context_collection\n            case _:\n                raise ValueError(f&quot;Unknown dataset: {dataset}&quot;)\n        results = self.corpus._query_vector_store(collection, query, k)\n        return results\n\n    def similar_to(self, content: ProcessedContent, k: int = 10) -&gt; &quot;SiphonQuery&quot;: ...\n\n    def graph_traverse(\n        self, start_content: ProcessedContent, depth: int = 2\n    ) -&gt; &quot;SiphonQuery&quot;: ...\n\n    # Aggregation Operations (return new SiphonQuery)\n\n    def group_by_source_type(self) -&gt; &quot;SiphonQuery&quot;: ...\n\n    def group_by_date(self, granularity: str = &quot;day&quot;) -&gt; &quot;SiphonQuery&quot;: ...\n\n    # ========================================================================\n    # Terminal Operations (return non-SiphonQuery results)\n    # ========================================================================\n\n    def to_list(self) -&gt; list[ProcessedContent]: ...\n\n    def to_corpus(self) -&gt; InMemoryCorpus: ...\n\n    def to_sourdough(self, **kwargs) -&gt; &quot;Sourdough&quot;: ...\n\n    def to_dataframe(self): ...\n\n    def first(self) -&gt; ProcessedContent | None: ...\n\n    def last(self) -&gt; ProcessedContent | None: ...\n\n    def count(self) -&gt; int: ...\n\n    def exists(self) -&gt; bool: ...\n\n    # ========================================================================\n    # View &amp; Utility Methods\n    # ========================================================================\n\n    def snapshot(self) -&gt; str:\n        &quot;&quot;&quot;Get a snapshot summary of the current query state&quot;&quot;&quot;\n        return self.corpus.snapshot()\n\n\nif __name__ == &quot;__main__&quot;:\n    from siphon.collections.corpus.siphon_corpus import DatabaseCorpus\n\n    query = DatabaseCorpus().query()\n    results = query.semantic_search(\n        &quot;Best practices for Retrieval Augmented Generation with LLMs&quot;, k=5\n    )\n    print(results)\n</file>\n    <file path=\"src/siphon/logs/logging_config.py\">import logging, sys\n\n# Global logger dictionary to ensure we return the same logger instance for each module\n_loggers = {}\n\n\ndef get_logger(name=None, level=None):\n    &quot;&quot;&quot;\n    Returns the logger with the specified name, creating it if necessary.\n    Using this function ensures the same logger instance is returned for the same name.\n\n    Args:\n        name: The logger name (typically __name__ from the calling module)\n        level: Optional level to set for this specific logger\n\n    Returns:\n        A configured logger instance\n    &quot;&quot;&quot;\n    global _loggers\n\n    # Default to the root logger if no name is provided\n    if name is None:\n        name = &quot;SiphonServer&quot;\n\n    if name in _loggers:\n        logger = _loggers[name]\n        if level is not None:\n            logger.setLevel(level)\n        return logger\n\n    # Create a new logger\n    logger = logging.getLogger(name)\n\n    # Set level if provided; otherwise inherit from parent\n    if level is not None:\n        logger.setLevel(level)\n\n    # Store in cache\n    _loggers[name] = logger\n\n    return logger\n\n\ndef configure_logging(\n    level: int = logging.INFO,\n    console=True,\n):\n    &quot;&quot;&quot;\n    Configure the root logging settings for the entire application.\n    Call this once at application startup.\n\n    Args:\n        level: The logging level (default: INFO)\n        log_file: Optional file to log to (default: None)\n        trace_mode: If True, add detailed debugging info with line numbers\n        console: If True, log to console (via stderr)\n\n    Levels:\n        - logging.DEBUG = 10\n        - logging.INFO = 20\n        - logging.WARNING = 30\n        - logging.ERROR = 40\n        - logging.CRITICAL = 50\n    &quot;&quot;&quot;\n    # Create a log formatter with trace information if requested\n    log_format = (\n        &quot;%(asctime)s [%(levelname)s] %(name)s:%(lineno)d (%(funcName)s) - %(message)s&quot;\n    )\n\n    formatter = logging.Formatter(log_format)\n\n    # Set up root logger\n    root_logger = logging.getLogger()\n    root_logger.setLevel(level)\n\n    # Remove any existing handlers to avoid duplicate logs\n    for handler in root_logger.handlers[:]:\n        root_logger.removeHandler(handler)\n\n    # Add console handler if requested, using stderr instead of stdout\n    if console:\n        console_handler = logging.StreamHandler(sys.stderr)\n        console_handler.setFormatter(formatter)\n        root_logger.addHandler(console_handler)\n\n    # Create the main application logger\n    logger = get_logger(&quot;SiphonServer&quot;, level)\n    logger.info(\n        &quot;Audio pipeline logging configured with level: %s, trace_mode: True&quot;,\n        logging.getLevelName(level),\n    )\n\n    return logger\n</file>\n    <file path=\"src/siphon/database/postgres/migrate.py\">&quot;&quot;&quot;\nThis script from 2025-08-25 updates existing records in the `processed_content` table to new schema requirements:\n- Adds `created_at` and `updated_at` timestamps to the JSONB `data` field.\n- Generates and populates `description_embedding` and `summary_embedding` vector fields using the `all-MiniLM-L6-v2` model from SentenceTransformers.\n&quot;&quot;&quot;\n\nfrom siphon.database.postgres.PGRES_connection import get_db_connection\nfrom siphon.data.processed_content import ProcessedContent\nfrom psycopg2.extras import RealDictCursor, Json\n\n\ndef migrate_existing_library():\n    from sentence_transformers import SentenceTransformer\n    from tqdm import tqdm\n    import time\n\n    model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)\n\n    with get_db_connection() as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:\n        # Get all records\n        cur.execute(\n            &quot;SELECT id, data, created_at, updated_at FROM processed_content ORDER BY id&quot;\n        )\n        records = cur.fetchall()\n\n        print(f&quot;Migrating {len(records)} records...&quot;)\n\n        for record in tqdm(records):\n            try:\n                # Reconstruct ProcessedContent\n                content = ProcessedContent.model_validate_from_cache(record[&quot;data&quot;])\n\n                # Add timestamps to the object\n                content.created_at = int(record[&quot;created_at&quot;].timestamp())\n                content.updated_at = int(record[&quot;updated_at&quot;].timestamp())\n\n                # Generate embeddings\n                desc_embedding = None\n                summary_embedding = None\n\n                if content.synthetic_data:\n                    try:\n                        if content.synthetic_data.description:\n                            desc_embedding = model.encode(\n                                content.synthetic_data.description\n                            ).tolist()\n                    except:\n                        pass\n                    try:\n                        if content.synthetic_data.summary:\n                            summary_embedding = model.encode(\n                                content.synthetic_data.summary\n                            ).tolist()\n                    except:\n                        pass\n\n                # Update both JSONB data (with timestamps) and vector columns\n                updated_data = content.model_dump_for_cache()\n                cur.execute(\n                    &quot;&quot;&quot;\n                    UPDATE processed_content \n                    SET data = %s, \n                        description_embedding = %s, \n                        summary_embedding = %s\n                    WHERE id = %s\n                &quot;&quot;&quot;,\n                    (\n                        Json(updated_data),\n                        desc_embedding,\n                        summary_embedding,\n                        record[&quot;id&quot;],\n                    ),\n                )\n\n                if record[&quot;id&quot;] % 50 == 0:\n                    conn.commit()\n\n            except Exception as e:\n                print(f&quot;Failed to migrate record {record['id']}: {e}&quot;)\n                continue\n\n        conn.commit()\n        print(&quot;Migration complete! Now creating indexes...&quot;)\n\n        # Create indexes after data is populated\n        cur.execute(&quot;&quot;&quot;\n            CREATE INDEX IF NOT EXISTS idx_description_embedding \n            ON processed_content USING hnsw (description_embedding vector_cosine_ops);\n        &quot;&quot;&quot;)\n\n        cur.execute(&quot;&quot;&quot;\n            CREATE INDEX IF NOT EXISTS idx_summary_embedding \n            ON processed_content USING hnsw (summary_embedding vector_cosine_ops);\n        &quot;&quot;&quot;)\n\n        conn.commit()\n        print(&quot;Indexes created! Migration fully complete.&quot;)\n\n\nif __name__ == &quot;__main__&quot;:\n    migrate_existing_library()\n</file>\n    <file path=\"src/siphon/database/postgres/PGRES_processed_content.py\">&quot;&quot;&quot;\nCRUD functions for Siphon project. Adapted from Kramer.database.\n\nWe will use this database as a cache for siphon data, keyed to uri.uri.\nUses factory-based serialization for robust object reconstruction.\n\nNow includes fallback cache system for when PostgreSQL is unavailable.\n&quot;&quot;&quot;\n\nfrom typing import Optional, Any\nfrom psycopg2.extras import RealDictCursor, Json\nfrom siphon.data.processed_content import ProcessedContent\nfrom dbclients import get_postgres_client\nfrom siphon.logs.logging_config import get_logger\nfrom rich.console import Console\nfrom pathlib import Path\n\nconsole = Console()\nlogger = get_logger(__name__)\n\nget_db_connection = get_postgres_client(client_type=&quot;context_db&quot;, dbname=&quot;siphon&quot;)\n\n\ndef create_table():\n    &quot;&quot;&quot;Create the processed_content table with simple JSONB storage.&quot;&quot;&quot;\n    with get_db_connection() as conn:\n        with conn.cursor() as cur:\n            cur.execute(&quot;&quot;&quot;\n                CREATE TABLE IF NOT EXISTS processed_content (\n                    id SERIAL PRIMARY KEY,\n                    uri_key TEXT UNIQUE NOT NULL,\n                    data JSONB NOT NULL,\n                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n                );\n            &quot;&quot;&quot;)\n\n            # Index for cache lookups (most important) - B-tree for exact matches\n            cur.execute(&quot;&quot;&quot;\n                CREATE INDEX IF NOT EXISTS idx_processed_content_uri_key \n                ON processed_content(uri_key);\n            &quot;&quot;&quot;)\n\n            # GIN index for JSONB queries (this is correct usage)\n            cur.execute(&quot;&quot;&quot;\n                CREATE INDEX IF NOT EXISTS idx_processed_content_jsonb \n                ON processed_content USING gin(data);\n            &quot;&quot;&quot;)\n\n            # B-tree index for timestamp queries\n            cur.execute(&quot;&quot;&quot;\n                CREATE INDEX IF NOT EXISTS idx_processed_content_updated_at \n                ON processed_content(updated_at DESC);\n            &quot;&quot;&quot;)\n\n        conn.commit()\n        logger.info(&quot;ProcessedContent cache table created successfully&quot;)\n\n\ndef ensure_table_exists():\n    &quot;&quot;&quot;Ensure the processed_content table exists, create if not.&quot;&quot;&quot;\n    with get_db_connection() as conn, conn.cursor() as cur:\n        # Check if table exists\n        cur.execute(&quot;&quot;&quot;\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_schema = 'public' \n                    AND table_name = 'processed_content'\n                );\n            &quot;&quot;&quot;)\n\n        table_exists = cur.fetchone()[0]\n\n        if not table_exists:\n            logger.info(&quot;processed_content table not found, creating...&quot;)\n            create_table()\n            return True\n        return False\n\n\n# =============================================================================\n# DIRECT POSTGRESQL FUNCTIONS (used by fallback system)\n# =============================================================================\n\n\ndef get_cached_content_direct(uri_key: str) -&gt; Optional[ProcessedContent]:\n    &quot;&quot;&quot;\n    Direct PostgreSQL access - used by fallback system.\n    Retrieve ProcessedContent from cache by URI key.\n    &quot;&quot;&quot;\n    ensure_table_exists()\n\n    logger.debug(f&quot;Checking PostgreSQL cache for {uri_key}&quot;)\n\n    with get_db_connection() as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:\n        # Try primary URI lookup first\n        cur.execute(&quot;SELECT data FROM processed_content WHERE uri_key = %s&quot;, (uri_key,))\n        row = cur.fetchone()\n\n        if row:\n            try:\n                return ProcessedContent.model_validate_from_cache(row[&quot;data&quot;])\n            except Exception as e:\n                logger.error(f&quot;Failed to deserialize cached content for {uri_key}: {e}&quot;)\n                return None\n\n        # For file sources, try checksum lookup\n        if any(\n            scheme in uri_key\n            for scheme in [&quot;text://&quot;, &quot;doc://&quot;, &quot;audio://&quot;, &quot;image://&quot;, &quot;video://&quot;]\n        ):\n            logger.debug(f&quot;Trying checksum lookup for file source: {uri_key}&quot;)\n            try:\n                # Get current checksum for this file\n                source = uri_key.split(&quot;://&quot;, 1)[1]\n                if Path(source).exists():\n                    checksum = URI.get_checksum(source)\n                    cur.execute(\n                        &quot;SELECT data FROM processed_content WHERE data-&gt;'uri'-&gt;&gt;'checksum' = %s LIMIT 1&quot;,\n                        (checksum,),\n                    )\n                    row = cur.fetchone()\n                    if row:\n                        logger.info(\n                            f&quot;Found duplicate content via checksum for {uri_key}&quot;\n                        )\n                        return ProcessedContent.model_validate_from_cache(row[&quot;data&quot;])\n            except Exception:\n                pass\n\n        return None\n\n\ndef cache_processed_content_direct(processed_content: ProcessedContent) -&gt; str:\n    &quot;&quot;&quot;Cache ProcessedContent with embeddings in separate vector columns&quot;&quot;&quot;\n    from sentence_transformers import SentenceTransformer\n\n    ensure_table_exists()\n\n    # Generate embeddings\n    model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)\n    desc_embedding = None\n    summary_embedding = None\n\n    if processed_content.synthetic_data:\n        try:\n            if processed_content.synthetic_data.description:\n                desc_embedding = model.encode(\n                    processed_content.synthetic_data.description\n                ).tolist()\n        except:\n            pass\n        try:\n            if processed_content.synthetic_data.summary:\n                summary_embedding = model.encode(\n                    processed_content.synthetic_data.summary\n                ).tolist()\n        except:\n            pass\n\n    uri_key = processed_content.uri.uri\n    data_json = processed_content.model_dump_for_cache()\n\n    with get_db_connection() as conn, conn.cursor() as cur:\n        cur.execute(\n            &quot;&quot;&quot;\n            INSERT INTO processed_content (uri_key, data, description_embedding, summary_embedding, updated_at)\n            VALUES (%s, %s, %s, %s, NOW())\n            ON CONFLICT (uri_key) \n            DO UPDATE SET \n                data = EXCLUDED.data,\n                description_embedding = EXCLUDED.description_embedding,\n                summary_embedding = EXCLUDED.summary_embedding,\n                updated_at = NOW()\n            RETURNING uri_key;\n        &quot;&quot;&quot;,\n            (uri_key, Json(data_json), desc_embedding, summary_embedding),\n        )\n\n        result = cur.fetchone()\n        conn.commit()\n        return result[0] if result else uri_key\n\n\n# =============================================================================\n# INITIALIZE FALLBACK SYSTEM\n# =============================================================================\n\n# Try to initialize the fallback cache manager\ntry:\n    from siphon.database.fallback.fallback_manager import FallbackCacheManager\n\n    _fallback_manager = FallbackCacheManager()\n    _fallback_enabled = True\n    logger.info(&quot;Fallback cache manager initialized successfully&quot;)\nexcept Exception as e:\n    logger.warning(f&quot;Fallback cache manager failed to initialize: {e}&quot;)\n    logger.warning(&quot;Falling back to direct PostgreSQL access only&quot;)\n    _fallback_manager = None\n    _fallback_enabled = False\n\n\n# =============================================================================\n# PUBLIC INTERFACE FUNCTIONS (with fallback support)\n# =============================================================================\n\n\ndef get_cached_content(uri_key: str) -&gt; Optional[ProcessedContent]:\n    &quot;&quot;&quot;\n    Public interface with fallback support.\n    Retrieve ProcessedContent from cache by URI key.\n    Auto-creates table if it doesn't exist.\n    &quot;&quot;&quot;\n    if _fallback_enabled and _fallback_manager:\n        return _fallback_manager.get_cached_content(uri_key)\n    else:\n        # Fallback to direct PostgreSQL access\n        logger.debug(&quot;Using direct PostgreSQL access (fallback system unavailable)&quot;)\n        return get_cached_content_direct(uri_key)\n\n\ndef cache_processed_content(processed_content: ProcessedContent) -&gt; str:\n    &quot;&quot;&quot;\n    Public interface with fallback support.\n    Cache a ProcessedContent object.\n    Auto-creates table if it doesn't exist.\n    &quot;&quot;&quot;\n    if _fallback_enabled and _fallback_manager:\n        return _fallback_manager.cache_processed_content(processed_content)\n    else:\n        # Fallback to direct PostgreSQL access\n        logger.debug(&quot;Using direct PostgreSQL access (fallback system unavailable)&quot;)\n        return cache_processed_content_direct(processed_content)\n\n\ndef cache_exists(uri_key: str) -&gt; bool:\n    &quot;&quot;&quot;Check if content exists in cache without retrieving it.&quot;&quot;&quot;\n    if _fallback_enabled and _fallback_manager:\n        return _fallback_manager.cache_exists(uri_key)\n    else:\n        # Direct PostgreSQL check\n        try:\n            ensure_table_exists()\n            with get_db_connection() as conn, conn.cursor() as cur:\n                cur.execute(\n                    &quot;&quot;&quot;\n                        SELECT 1 FROM processed_content \n                        WHERE uri_key = %s\n                    &quot;&quot;&quot;,\n                    (uri_key,),\n                )\n                return cur.fetchone() is not None\n        except Exception as e:\n            logger.error(f&quot;Failed to check cache existence for {uri_key}: {e}&quot;)\n            return False\n\n\n# =============================================================================\n# FALLBACK MANAGEMENT FUNCTIONS\n# =============================================================================\n\n\ndef get_fallback_stats() -&gt; dict:\n    &quot;&quot;&quot;Get statistics about the fallback cache system&quot;&quot;&quot;\n    if _fallback_enabled and _fallback_manager:\n        return _fallback_manager.get_fallback_stats()\n    else:\n        return {\n            &quot;fallback_enabled&quot;: False,\n            &quot;reason&quot;: &quot;Fallback system not initialized&quot;,\n            &quot;postgres_only&quot;: True,\n        }\n\n\ndef force_sync_backlog(batch_size: int = 50) -&gt; dict:\n    &quot;&quot;&quot;Manually trigger a sync of the backlog queue&quot;&quot;&quot;\n    if _fallback_enabled and _fallback_manager:\n        return _fallback_manager.force_sync_backlog(batch_size)\n    else:\n        return {&quot;error&quot;: &quot;Fallback system not available&quot;, &quot;fallback_enabled&quot;: False}\n\n\ndef clear_all_caches() -&gt; dict:\n    &quot;&quot;&quot;Clear both fallback caches and PostgreSQL cache. Use with caution!&quot;&quot;&quot;\n    results = {}\n\n    # Clear fallback caches if available\n    if _fallback_enabled and _fallback_manager:\n        fallback_results = _fallback_manager.clear_all_caches()\n        results.update(fallback_results)\n\n    # Clear PostgreSQL cache\n    try:\n        postgres_cleared = clear_table()\n        results[&quot;postgres_cleared&quot;] = postgres_cleared\n    except Exception as e:\n        results[&quot;postgres_error&quot;] = str(e)\n\n    logger.warning(f&quot;Cleared all caches: {results}&quot;)\n    return results\n\n\n# =============================================================================\n# LEGACY FUNCTIONS (maintained for backward compatibility)\n# =============================================================================\n\n\ndef insert_siphon(siphon: ProcessedContent) -&gt; str:\n    &quot;&quot;&quot;\n    Legacy function - maintained for backward compatibility.\n    Use cache_processed_content() instead.\n    &quot;&quot;&quot;\n    logger.warning(\n        &quot;insert_siphon() is deprecated. Use cache_processed_content() instead.&quot;\n    )\n    return cache_processed_content_direct(siphon)\n\n\ndef get_siphon_by_uri(uri_key: str) -&gt; Optional[ProcessedContent]:\n    &quot;&quot;&quot;\n    Legacy function - maintained for backward compatibility.\n    Use get_cached_content() instead.\n    &quot;&quot;&quot;\n    logger.warning(\n        &quot;get_siphon_by_uri() is deprecated. Use get_cached_content() instead.&quot;\n    )\n    return get_cached_content(uri_key)\n\n\ndef get_siphon_by_id(id: str) -&gt; Optional[ProcessedContent]:\n    &quot;&quot;&quot;Retrieve ProcessedContent by database ID (PostgreSQL only).&quot;&quot;&quot;\n    try:\n        record_id = int(id)\n    except ValueError:\n        logger.error(f&quot;Invalid ID format: {id}&quot;)\n        return None\n\n    ensure_table_exists()\n\n    with get_db_connection() as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:\n        cur.execute(\n            &quot;&quot;&quot;\n                SELECT data FROM processed_content \n                WHERE id = %s\n            &quot;&quot;&quot;,\n            (record_id,),\n        )\n\n        row = cur.fetchone()\n        if not row:\n            return None\n\n        try:\n            return ProcessedContent.model_validate_from_cache(row[&quot;data&quot;])\n        except Exception as e:\n            logger.error(f&quot;Failed to deserialize content with ID {record_id}: {e}&quot;)\n            return None\n\n\ndef get_all_siphon() -&gt; list[ProcessedContent]:\n    &quot;&quot;&quot;\n    Get all cached ProcessedContent objects (PostgreSQL only).\n    Returns most recently updated first.\n    &quot;&quot;&quot;\n    ensure_table_exists()\n\n    with get_db_connection() as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:\n        cur.execute(&quot;&quot;&quot;\n                SELECT data FROM processed_content \n                ORDER BY updated_at DESC \n            &quot;&quot;&quot;)\n\n        rows = cur.fetchall()\n        results = []\n\n        for row in rows:\n            try:\n                processed_content = ProcessedContent.model_validate_from_cache(\n                    row[&quot;data&quot;]\n                )\n                results.append(processed_content)\n            except Exception as e:\n                logger.warning(f&quot;Skipping invalid cached content: {e}&quot;)\n                continue\n\n        return results\n\n\ndef get_last_siphon() -&gt; Optional[ProcessedContent]:\n    &quot;&quot;&quot;\n    Get the most recently updated ProcessedContent object (PostgreSQL only).\n    Returns None if no content is cached.\n    &quot;&quot;&quot;\n    ensure_table_exists()\n\n    with get_db_connection() as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:\n        cur.execute(&quot;&quot;&quot;\n                SELECT data FROM processed_content \n                ORDER BY updated_at DESC \n                LIMIT 1\n            &quot;&quot;&quot;)\n\n        row = cur.fetchone()\n        if not row:\n            return None\n\n        try:\n            return ProcessedContent.model_validate_from_cache(row[&quot;data&quot;])\n        except Exception as e:\n            logger.error(f&quot;Failed to deserialize latest cached content: {e}&quot;)\n            return None\n\n\ndef search_cached_content(\n    source_type: Optional[str] = None,\n    title_query: Optional[str] = None,\n    limit: int = 50,\n) -&gt; list[ProcessedContent]:\n    &quot;&quot;&quot;\n    Basic search functionality using JSONB queries (PostgreSQL only).\n\n    Args:\n        source_type: Filter by source type (e.g., 'YouTube', 'GitHub')\n        title_query: Search in titles using full-text search\n        limit: Maximum results to return\n    &quot;&quot;&quot;\n    ensure_table_exists()\n\n    with get_db_connection() as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:\n        where_conditions = []\n        params = []\n\n        if source_type:\n            where_conditions.append(&quot;data-&gt;&gt;'sourcetype' = %s&quot;)\n            params.append(source_type)\n\n        if title_query:\n            where_conditions.append(&quot;&quot;&quot;\n                    to_tsvector('english', COALESCE(data-&gt;'synthetic_data'-&gt;&gt;'title', ''))\n                    @@ plainto_tsquery('english', %s)\n                &quot;&quot;&quot;)\n            params.append(title_query)\n\n        where_clause = &quot;&quot;\n        if where_conditions:\n            where_clause = &quot;WHERE &quot; + &quot; AND &quot;.join(where_conditions)\n\n        cur.execute(\n            f&quot;&quot;&quot;\n                SELECT data FROM processed_content\n                {where_clause}\n                ORDER BY updated_at DESC\n                LIMIT %s\n            &quot;&quot;&quot;,\n            params + [limit],\n        )\n\n        rows = cur.fetchall()\n        results = []\n\n        for row in rows:\n            try:\n                processed_content = ProcessedContent.model_validate_from_cache(\n                    row[&quot;data&quot;]\n                )\n                results.append(processed_content)\n            except Exception as e:\n                logger.warning(f&quot;Skipping invalid search result: {e}&quot;)\n                continue\n\n        return results\n\n\ndef delete_cached_content(uri_key: str) -&gt; bool:\n    &quot;&quot;&quot;Delete cached content by URI key. Returns True if deleted (PostgreSQL only).&quot;&quot;&quot;\n    ensure_table_exists()\n\n    with get_db_connection() as conn, conn.cursor() as cur:\n        cur.execute(\n            &quot;&quot;&quot;\n                DELETE FROM processed_content \n                WHERE uri_key = %s\n            &quot;&quot;&quot;,\n            (uri_key,),\n        )\n\n        deleted = cur.rowcount &gt; 0\n        conn.commit()\n\n        if deleted:\n            logger.info(f&quot;Deleted cached content from PostgreSQL: {uri_key}&quot;)\n\n        return deleted\n\n\ndef get_cache_stats() -&gt; dict[str, Any]:\n    &quot;&quot;&quot;Get basic statistics about the PostgreSQL cache.&quot;&quot;&quot;\n    ensure_table_exists()\n\n    with get_db_connection() as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:\n        cur.execute(&quot;&quot;&quot;\n            SELECT \n                COUNT(*) as total_cached,\n                COUNT(DISTINCT data-&gt;&gt;'sourcetype') as unique_source_types,\n                MIN(created_at) as oldest_cached,\n                MAX(updated_at) as most_recent_update,\n                pg_size_pretty(pg_total_relation_size('processed_content')) as table_size\n        &quot;&quot;&quot;)\n        overall_stats = cur.fetchone()\n\n        # Source type breakdown\n        cur.execute(&quot;&quot;&quot;\n            SELECT \n                data-&gt;&gt;'sourcetype' as source_type,\n                COUNT(*) as count\n            FROM processed_content \n            GROUP BY data-&gt;&gt;'sourcetype'\n            ORDER BY count DESC\n        &quot;&quot;&quot;)\n        source_type_stats = cur.fetchall()\n\n        return {\n            &quot;overall&quot;: dict(overall_stats) if overall_stats else {},\n            &quot;by_source_type&quot;: [dict(row) for row in source_type_stats],\n        }\n\n\ndef clear_table():\n    &quot;&quot;&quot;Clear all cached data from PostgreSQL. Use with caution!&quot;&quot;&quot;\n    ensure_table_exists()\n\n    with get_db_connection() as conn, conn.cursor() as cur:\n        cur.execute(&quot;DELETE FROM processed_content&quot;)\n        deleted_count = cur.rowcount\n        conn.commit()\n\n        logger.warning(\n            f&quot;Cleared all PostgreSQL cached content. Deleted {deleted_count} records.&quot;\n        )\n        return deleted_count\n\n\ndef delete_table():\n    &quot;&quot;&quot;Delete the entire PostgreSQL table. Use only for testing!&quot;&quot;&quot;\n    with get_db_connection() as conn, conn.cursor() as cur:\n        cur.execute(&quot;DROP TABLE IF EXISTS processed_content CASCADE&quot;)\n        conn.commit()\n\n        logger.warning(&quot;Deleted processed_content table entirely.&quot;)\n\n\n# =============================================================================\n# DEBUG AND UTILITY FUNCTIONS\n# =============================================================================\n\n\ndef debug_cache_status() -&gt; dict:\n    &quot;&quot;&quot;Debug function to check the status of all cache systems&quot;&quot;&quot;\n    status = {\n        &quot;fallback_enabled&quot;: _fallback_enabled,\n        &quot;fallback_manager_available&quot;: _fallback_manager is not None,\n        &quot;postgres_connection&quot;: None,\n        &quot;fallback_stats&quot;: None,\n    }\n\n    # Test PostgreSQL connection\n    try:\n        with get_db_connection() as conn, conn.cursor() as cur:\n            cur.execute(&quot;SELECT 1&quot;)\n            status[&quot;postgres_connection&quot;] = &quot;available&quot;\n    except Exception as e:\n        status[&quot;postgres_connection&quot;] = f&quot;failed: {e}&quot;\n\n    # Get fallback stats if available\n    if _fallback_enabled and _fallback_manager:\n        try:\n            status[&quot;fallback_stats&quot;] = _fallback_manager.get_fallback_stats()\n        except Exception as e:\n            status[&quot;fallback_stats&quot;] = f&quot;error: {e}&quot;\n\n    return status\n\n\ndef is_fallback_enabled() -&gt; bool:\n    &quot;&quot;&quot;Check if the fallback system is enabled and available&quot;&quot;&quot;\n    return _fallback_enabled and _fallback_manager is not None\n</file>\n    <file path=\"src/siphon/database/sqlite/manual_sync.py\">#!/usr/bin/env python3\n&quot;&quot;&quot;\nManual sync script to force synchronization of SQLite items to PostgreSQL.\nThis handles the case where items exist in SQLite but not in PostgreSQL\nand were never added to the sync queue.\n&quot;&quot;&quot;\n\nimport sqlite3\nfrom pathlib import Path\nfrom rich.console import Console\nfrom rich.progress import Progress, BarColumn, TextColumn\nfrom siphon.database.postgres.PGRES_processed_content import (\n    cache_processed_content_direct,\n)\nfrom siphon.data.processed_content import ProcessedContent\n\nconsole = Console()\n\n\ndef get_sqlite_connection():\n    &quot;&quot;&quot;Get SQLite database connection.&quot;&quot;&quot;\n    cache_path = Path(&quot;~/.siphon/fallback_cache.db&quot;).expanduser()\n    if not cache_path.exists():\n        console.print(f&quot;[red]SQLite database not found at {cache_path}[/red]&quot;)\n        return None\n    return sqlite3.connect(cache_path)\n\n\ndef get_unsynced_items():\n    &quot;&quot;&quot;Find items in SQLite that don't exist in PostgreSQL.&quot;&quot;&quot;\n    console.print(&quot;[blue]\ud83d\udd0d Finding unsynced items...[/blue]&quot;)\n\n    # Get all SQLite URIs\n    sqlite_conn = get_sqlite_connection()\n    if not sqlite_conn:\n        return []\n\n    sqlite_conn.row_factory = sqlite3.Row\n    cursor = sqlite_conn.cursor()\n    cursor.execute(&quot;SELECT uri_key, data FROM processed_content&quot;)\n    sqlite_items = {row[&quot;uri_key&quot;]: row[&quot;data&quot;] for row in cursor.fetchall()}\n    sqlite_conn.close()\n\n    if not sqlite_items:\n        console.print(&quot;[yellow]No items found in SQLite[/yellow]&quot;)\n        return []\n\n    # Check which ones exist in PostgreSQL\n    try:\n        from siphon.database.postgres.PGRES_connection import get_db_connection\n\n        with get_db_connection() as pg_conn:\n            with pg_conn.cursor() as pg_cursor:\n                # Check existence in batches\n                pg_cursor.execute(\n                    &quot;&quot;&quot;\n                    SELECT uri_key FROM processed_content \n                    WHERE uri_key = ANY(%s)\n                    &quot;&quot;&quot;,\n                    (list(sqlite_items.keys()),),\n                )\n                postgres_uris = {row[0] for row in pg_cursor.fetchall()}\n    except Exception as e:\n        console.print(f&quot;[red]Failed to connect to PostgreSQL: {e}[/red]&quot;)\n        return []\n\n    # Find unsynced items\n    unsynced_uris = set(sqlite_items.keys()) - postgres_uris\n    unsynced_items = [(uri, sqlite_items[uri]) for uri in unsynced_uris]\n\n    console.print(f&quot;[green]Found {len(unsynced_items)} unsynced items[/green]&quot;)\n    return unsynced_items\n\n\ndef sync_item(uri_key: str, data_json: str) -&gt; bool:\n    &quot;&quot;&quot;Sync a single item from SQLite to PostgreSQL.&quot;&quot;&quot;\n    try:\n        # Parse the JSON data and reconstruct ProcessedContent\n        import json\n\n        data = json.loads(data_json)\n\n        # Reconstruct ProcessedContent using the cache validation method\n        processed_content = ProcessedContent.model_validate_from_cache(data)\n\n        # Store in PostgreSQL using direct method (bypasses fallback logic)\n        result = cache_processed_content_direct(processed_content)\n\n        return True\n    except Exception as e:\n        console.print(f&quot;[red]Failed to sync {uri_key}: {e}[/red]&quot;)\n        return False\n\n\ndef main():\n    console.print(&quot;[bold blue]\ud83d\udd04 Manual SQLite \u2192 PostgreSQL Sync[/bold blue]&quot;)\n    console.print(&quot;=&quot; * 50)\n\n    # Find unsynced items\n    unsynced_items = get_unsynced_items()\n\n    if not unsynced_items:\n        console.print(&quot;[green]\u2705 All items are already synced![/green]&quot;)\n        return\n\n    # Confirm before proceeding\n    console.print(f&quot;\\n[yellow]Found {len(unsynced_items)} items to sync.[/yellow]&quot;)\n\n    # Option to show items\n    show_items = console.input(&quot;Show item details? (y/N): &quot;).lower().strip()\n    if show_items == &quot;y&quot;:\n        console.print(&quot;\\n[dim]Unsynced items:[/dim]&quot;)\n        for i, (uri, data_json) in enumerate(unsynced_items[:5], 1):\n            try:\n                import json\n\n                data = json.loads(data_json)\n                title = data.get(&quot;synthetic_data&quot;, {}).get(&quot;title&quot;, &quot;Untitled&quot;)\n                source_type = data.get(&quot;sourcetype&quot;, &quot;Unknown&quot;)\n                console.print(f&quot;  {i}. [{source_type}] {title}&quot;)\n            except:\n                console.print(f&quot;  {i}. {uri}&quot;)\n\n        if len(unsynced_items) &gt; 5:\n            console.print(f&quot;  ... and {len(unsynced_items) - 5} more&quot;)\n\n    # Confirm sync\n    proceed = (\n        console.input(f&quot;\\nSync {len(unsynced_items)} items to PostgreSQL? (y/N): &quot;)\n        .lower()\n        .strip()\n    )\n    if proceed != &quot;y&quot;:\n        console.print(&quot;[yellow]Sync cancelled.[/yellow]&quot;)\n        return\n\n    # Perform sync with progress bar\n    console.print(f&quot;\\n[blue]\ud83d\ude80 Syncing {len(unsynced_items)} items...[/blue]&quot;)\n\n    synced = 0\n    failed = 0\n\n    with Progress(\n        TextColumn(&quot;[progress.description]{task.description}&quot;),\n        BarColumn(),\n        TextColumn(&quot;[progress.percentage]{task.percentage:&gt;3.0f}%&quot;),\n        console=console,\n    ) as progress:\n        task = progress.add_task(&quot;Syncing...&quot;, total=len(unsynced_items))\n\n        for uri_key, data_json in unsynced_items:\n            if sync_item(uri_key, data_json):\n                synced += 1\n            else:\n                failed += 1\n\n            progress.advance(task)\n\n    # Results\n    console.print(f&quot;\\n[bold]Sync Complete![/bold]&quot;)\n    console.print(f&quot;[green]\u2705 Synced: {synced}[/green]&quot;)\n    if failed &gt; 0:\n        console.print(f&quot;[red]\u274c Failed: {failed}[/red]&quot;)\n\n    # Verify results\n    console.print(f&quot;\\n[blue]\ud83d\udd0d Verifying sync...[/blue]&quot;)\n    try:\n        from siphon.database.sqlite.sync_check import check_uri_overlap\n\n        overlap_stats, error = check_uri_overlap()\n\n        if error:\n            console.print(f&quot;[red]Verification failed: {error}[/red]&quot;)\n        else:\n            console.print(f&quot;[green]\u2705 Synced: {overlap_stats['synced']}[/green]&quot;)\n            console.print(\n                f&quot;[yellow]\u23f3 Still unsynced: {overlap_stats['unsynced']}[/yellow]&quot;\n            )\n\n            if overlap_stats[&quot;unsynced&quot;] == 0:\n                console.print(f&quot;[bold green]\ud83c\udf89 All items now synced![/bold green]&quot;)\n    except Exception as e:\n        console.print(f&quot;[red]Verification error: {e}[/red]&quot;)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</file>\n    <file path=\"src/siphon/database/sqlite/snapshot_sqlite3.py\">#!/usr/bin/env python3\n&quot;&quot;&quot;\nSQLite cache snapshot utility with Rich formatting\n&quot;&quot;&quot;\n\nimport argparse\nimport sqlite3\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\nfrom rich.progress import BarColumn, Progress, TextColumn\nfrom rich.text import Text\nfrom rich.layout import Layout\nfrom rich import box\n\nconsole = Console()\n\n# Create path for SQLite database\ncache_path = Path(&quot;~/.siphon/fallback_cache.db&quot;).expanduser()\n\n# Source type styling (same as PostgreSQL version)\nSOURCE_STYLES = {\n    &quot;YouTube&quot;: (&quot;\ud83d\udcfa&quot;, &quot;red&quot;),\n    &quot;Article&quot;: (&quot;\ud83d\udcc4&quot;, &quot;blue&quot;),\n    &quot;Doc&quot;: (&quot;\ud83d\udccb&quot;, &quot;green&quot;),\n    &quot;Image&quot;: (&quot;\ud83d\uddbc\ufe0f&quot;, &quot;magenta&quot;),\n    &quot;Audio&quot;: (&quot;\ud83c\udfb5&quot;, &quot;yellow&quot;),\n    &quot;GitHub&quot;: (&quot;\ud83d\udc19&quot;, &quot;dark purple&quot;),\n    &quot;Drive&quot;: (&quot;\ud83d\udcbe&quot;, &quot;cyan&quot;),\n}\n\n\ndef get_sqlite_connection():\n    &quot;&quot;&quot;Get SQLite database connection.&quot;&quot;&quot;\n    # You'll need to adjust this path based on your SQLite database location\n    # Looking at your codebase, it seems SQLite cache would be in the fallback system\n    try:\n        conn = sqlite3.connect(cache_path)\n        conn.row_factory = sqlite3.Row  # Enable dict-like access\n        return conn\n    except Exception as e:\n        console.print(f&quot;[red]Error connecting to SQLite: {e}[/red]&quot;)\n        raise\n\n\ndef get_total_count():\n    &quot;&quot;&quot;Total cached items&quot;&quot;&quot;\n    with get_sqlite_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(&quot;SELECT COUNT(*) FROM processed_content&quot;)\n        return cursor.fetchone()[0]\n\n\ndef format_size(chars):\n    &quot;&quot;&quot;Format character count in human readable form&quot;&quot;&quot;\n    if chars &lt; 1000:\n        return f&quot;{chars}&quot;\n    elif chars &lt; 1000000:\n        return f&quot;{chars / 1000:.1f}K&quot;\n    else:\n        return f&quot;{chars / 1000000:.1f}M&quot;\n\n\ndef create_header():\n    &quot;&quot;&quot;Create header panel with total count&quot;&quot;&quot;\n    total = get_total_count()\n    header_text = Text.assemble(\n        (&quot;\ud83d\udcda &quot;, &quot;bright_blue&quot;),\n        (f&quot;{total:,}&quot;, &quot;bold bright_white&quot;),\n        (&quot; Total Items&quot;, &quot;bright_blue&quot;),\n    )\n    return Panel(\n        header_text,\n        title=&quot;[bold bright_cyan]SIPHON LIBRARY SNAPSHOT[/bold bright_cyan]&quot;,\n        border_style=&quot;bright_cyan&quot;,\n        box=box.DOUBLE,\n    )\n\n\ndef create_source_distribution():\n    &quot;&quot;&quot;Create source type distribution chart&quot;&quot;&quot;\n    with get_sqlite_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(&quot;&quot;&quot;\n            SELECT json_extract(data, '$.sourcetype') as source_type, COUNT(*) as count \n            FROM processed_content \n            GROUP BY json_extract(data, '$.sourcetype') \n            ORDER BY count DESC\n        &quot;&quot;&quot;)\n        results = cursor.fetchall()\n\n        if not results:\n            return Panel(&quot;No data found&quot;, title=&quot;Source Distribution&quot;)\n\n        table = Table(show_header=False, box=None, padding=(0, 1))\n        table.add_column(&quot;Type&quot;, width=12)\n        table.add_column(&quot;Bar&quot;, min_width=20)\n        table.add_column(&quot;Count&quot;, justify=&quot;right&quot;, width=6)\n\n        max_count = max(r[&quot;count&quot;] for r in results)\n\n        for row in results:\n            source_type = row[&quot;source_type&quot;]\n            count = row[&quot;count&quot;]\n            icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n            # Calculate bar width (max 30 chars)\n            bar_width = int((count / max_count) * 30)\n            bar = &quot;\u2588&quot; * bar_width\n\n            type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n            bar_text = Text(bar, style=color)\n            count_text = Text(str(count), style=&quot;bold white&quot;)\n\n            table.add_row(type_text, bar_text, count_text)\n\n        return Panel(\n            table,\n            title=&quot;[bold green]\ud83d\udcca Source Distribution[/bold green]&quot;,\n            border_style=&quot;green&quot;,\n        )\n\n\ndef create_recent_additions(hours=24):\n    &quot;&quot;&quot;Create recent additions panel&quot;&quot;&quot;\n    with get_sqlite_connection() as conn:\n        cursor = conn.cursor()\n        cutoff = datetime.now() - timedelta(hours=hours)\n        cursor.execute(\n            &quot;&quot;&quot;\n            SELECT json_extract(data, '$.sourcetype') as source_type, \n                   COALESCE(json_extract(data, '$.synthetic_data.title'), 'Untitled') as title,\n                   created_at\n            FROM processed_content \n            WHERE created_at &gt; ? \n            ORDER BY created_at DESC \n            LIMIT 10\n        &quot;&quot;&quot;,\n            (cutoff.isoformat(),),\n        )\n\n        results = cursor.fetchall()\n\n        if not results:\n            content = Text(f&quot;No additions in last {hours}h&quot;, style=&quot;dim&quot;)\n        else:\n            table = Table(show_header=False, box=None)\n            table.add_column(&quot;Time&quot;, width=11)\n            table.add_column(&quot;Type&quot;, width=12)\n            table.add_column(&quot;Title&quot;, max_width=40)\n\n            for row in results:\n                source_type = row[&quot;source_type&quot;]\n                icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n                # Parse SQLite datetime string\n                created_at = datetime.fromisoformat(\n                    row[&quot;created_at&quot;].replace(&quot;Z&quot;, &quot;+00:00&quot;)\n                )\n                time_text = Text(created_at.strftime(&quot;%m/%d %H:%M&quot;), style=&quot;cyan&quot;)\n                type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n                title = (row[&quot;title&quot;] or &quot;Untitled&quot;)[:40].replace(&quot;\\n&quot;, &quot; &quot;)\n                title_text = Text(title, style=&quot;white&quot;)\n\n                table.add_row(time_text, type_text, title_text)\n\n            content = table\n\n        return Panel(\n            content,\n            title=f&quot;[bold yellow]\ud83d\udd52 Recent ({hours}h)[/bold yellow]&quot;,\n            border_style=&quot;yellow&quot;,\n        )\n\n\ndef create_latest_items():\n    &quot;&quot;&quot;Create latest items panel&quot;&quot;&quot;\n    with get_sqlite_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(&quot;&quot;&quot;\n            SELECT json_extract(data, '$.sourcetype') as source_type,\n                   COALESCE(json_extract(data, '$.synthetic_data.title'), 'Untitled') as title,\n                   created_at\n            FROM processed_content \n            ORDER BY created_at DESC \n            LIMIT 3\n        &quot;&quot;&quot;)\n\n        results = cursor.fetchall()\n        table = Table(show_header=False, box=None)\n        table.add_column(&quot;Time&quot;, width=11)\n        table.add_column(&quot;Type&quot;, width=12)\n        table.add_column(&quot;Title&quot;, max_width=70)\n\n        for row in results:\n            source_type = row[&quot;source_type&quot;]\n            icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n            # Parse SQLite datetime string\n            created_at = datetime.fromisoformat(\n                row[&quot;created_at&quot;].replace(&quot;Z&quot;, &quot;+00:00&quot;)\n            )\n            time_text = Text(created_at.strftime(&quot;%m/%d %H:%M&quot;), style=&quot;bright_green&quot;)\n            type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n            title = (row[&quot;title&quot;] or &quot;Untitled&quot;)[:70].replace(&quot;\\n&quot;, &quot; &quot;)\n            title_text = Text(title, style=&quot;dim&quot;)\n\n            table.add_row(time_text, type_text, title_text)\n\n        return Panel(\n            table,\n            title=&quot;[bold bright_green]\u2b50 Latest Items[/bold bright_green]&quot;,\n            border_style=&quot;bright_green&quot;,\n        )\n\n\ndef create_size_extremes():\n    &quot;&quot;&quot;Create size extremes panel&quot;&quot;&quot;\n    with get_sqlite_connection() as conn:\n        cursor = conn.cursor()\n        # Get largest\n        cursor.execute(&quot;&quot;&quot;\n            SELECT json_extract(data, '$.sourcetype') as source_type,\n                   COALESCE(json_extract(data, '$.synthetic_data.title'), 'Untitled') as title,\n                   LENGTH(json_extract(data, '$.context_data.context')) as size\n            FROM processed_content \n            WHERE json_extract(data, '$.context_data.context') IS NOT NULL\n            ORDER BY size DESC LIMIT 3\n        &quot;&quot;&quot;)\n        largest = cursor.fetchall()\n\n        # Get smallest\n        cursor.execute(&quot;&quot;&quot;\n            SELECT json_extract(data, '$.sourcetype') as source_type,\n                   COALESCE(json_extract(data, '$.synthetic_data.title'), 'Untitled') as title,\n                   LENGTH(json_extract(data, '$.context_data.context')) as size\n            FROM processed_content \n            WHERE json_extract(data, '$.context_data.context') IS NOT NULL\n            ORDER BY size ASC LIMIT 3\n        &quot;&quot;&quot;)\n        smallest = cursor.fetchall()\n\n        table = Table(show_header=False, box=None)\n        table.add_column(&quot;Size&quot;, width=8, justify=&quot;right&quot;)\n        table.add_column(&quot;Type&quot;, width=12)\n        table.add_column(&quot;Title&quot;, max_width=70)\n\n        # Add largest\n        for i, row in enumerate(largest):\n            source_type = row[&quot;source_type&quot;]\n            icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n            size_text = Text(f&quot;{format_size(row['size'])} &quot;, style=&quot;red&quot;)\n            type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n            title = (row[&quot;title&quot;] or &quot;Untitled&quot;)[:70].replace(&quot;\\n&quot;, &quot; &quot;)\n            title_text = Text(title, style=&quot;dim&quot;)\n\n            table.add_row(size_text, type_text, title_text)\n\n        # Add separator\n        table.add_row(&quot;&quot;, &quot;&quot;, &quot;&quot;)\n\n        # Add smallest\n        for row in smallest:\n            source_type = row[&quot;source_type&quot;]\n            icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n            size_text = Text(f&quot;{format_size(row['size'])} &quot;, style=&quot;green&quot;)\n            type_text = Text.assemble((icon + &quot; &quot;, color), (source_type, color))\n            title = (row[&quot;title&quot;] or &quot;Untitled&quot;)[:70].replace(&quot;\\n&quot;, &quot; &quot;)\n            title_text = Text(title, style=&quot;dim&quot;)\n\n            table.add_row(size_text, type_text, title_text)\n\n        return Panel(\n            table,\n            title=&quot;[bold magenta]\ud83d\udccf Size Extremes[/bold magenta]&quot;,\n            border_style=&quot;magenta&quot;,\n        )\n\n\ndef search_by_source_type(source_type):\n    &quot;&quot;&quot;Show items for specific source type&quot;&quot;&quot;\n    with get_sqlite_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\n            &quot;&quot;&quot;\n            SELECT COALESCE(json_extract(data, '$.synthetic_data.title'), 'Untitled') as title,\n                   created_at\n            FROM processed_content \n            WHERE json_extract(data, '$.sourcetype') = ? \n            ORDER BY created_at DESC \n            LIMIT 20\n        &quot;&quot;&quot;,\n            (source_type,),\n        )\n\n        results = cursor.fetchall()\n\n        if not results:\n            console.print(f&quot;[red]No {source_type} items found[/red]&quot;)\n            return\n\n        icon, color = SOURCE_STYLES.get(source_type, (&quot;\ud83d\udcc1&quot;, &quot;white&quot;))\n\n        table = Table(show_header=True, box=box.MINIMAL)\n        table.add_column(&quot;Date&quot;, width=11)\n        table.add_column(&quot;Title&quot;, max_width=60)\n\n        for row in results:\n            # Parse SQLite datetime string\n            created_at = datetime.fromisoformat(\n                row[&quot;created_at&quot;].replace(&quot;Z&quot;, &quot;+00:00&quot;)\n            )\n            date_text = created_at.strftime(&quot;%m/%d %H:%M&quot;)\n            title = (row[&quot;title&quot;] or &quot;Untitled&quot;)[:60]\n            table.add_row(date_text, title)\n\n        panel = Panel(\n            table,\n            title=f&quot;[bold {color}]{icon} {source_type} Items ({len(results)})[/bold {color}]&quot;,\n            border_style=color,\n        )\n        console.print(panel)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=&quot;SQLite cache snapshot&quot;)\n    parser.add_argument(&quot;--source-type&quot;, help=&quot;Show items for specific source type&quot;)\n    parser.add_argument(\n        &quot;--recent&quot;, type=int, default=24, help=&quot;Recent hours (default: 24)&quot;\n    )\n\n    args = parser.parse_args()\n\n    if args.source_type:\n        search_by_source_type(args.source_type)\n        return\n\n    # Default overview\n    console.print(create_header())\n    console.print()\n    console.print(create_source_distribution())\n    console.print()\n    console.print(create_recent_additions(args.recent))\n    console.print()\n    console.print(create_latest_items())\n    console.print()\n    console.print(create_size_extremes())\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</file>\n    <file path=\"src/siphon/database/chroma/chroma_curate.py\">&quot;&quot;&quot;\nCourse titles from cosmo export are loaded into chroma collection.\nGetter functions are provided to get course titles from chroma collection.\nThis is primarily for fuzzy matching course titles for Get.py.\n\nNOTE: Chroma needs to be running on port 8001. This code defaults to localhost.\nOn Caruana:\n```bash\nchroma run --port 8001 --path /home/bianders/Databases/chroma\n```\n&quot;&quot;&quot;\n\n# Imports are heavy and noisy. We want a spinner and to suppress all noisy torch/tensorflow output.\n# This is entirely due to SentenceTransformers and the rerankers library.\n# -----------------------------------------------------------------\nfrom rich.console import Console\nfrom contextlib import contextmanager\nimport os\nimport logging\nimport warnings\nfrom rich.progress import Progress\nimport asyncio\n\n\n# The nuclear option: shut the entire computer up\n@contextmanager\ndef silence_all_output():\n    &quot;&quot;&quot;\n    Context manager that completely silences both stdout and stderr by redirecting to /dev/null at the OS level.\n    &quot;&quot;&quot;\n    # Save original file descriptors\n    old_stdout = os.dup(1)\n    old_stderr = os.dup(2)\n\n    # Close and redirect stdout and stderr\n    os.close(1)\n    os.close(2)\n    os.open(os.devnull, os.O_WRONLY | os.O_CREAT)\n    os.dup2(1, 2)  # Redirect stderr to the same place as stdout\n\n    try:\n        yield\n    finally:\n        # Restore original stdout and stderr\n        os.dup2(old_stdout, 1)\n        os.dup2(old_stderr, 2)\n        os.close(old_stdout)\n        os.close(old_stderr)\n\n\n# Silence warnings moving forward\nwarnings.filterwarnings(&quot;ignore&quot;)\n# Suppress various logging messages\nlogging.basicConfig(level=logging.ERROR)\n\n# our imports\n# -----------------------------------------------------------------\nconsole = Console(width=100)  # for spinner\n\nwith silence_all_output():\n    with console.status(&quot;[green]Loading...&quot;, spinner=&quot;dots&quot;):\n        import sys, re, html, asyncio, chromadb\n        from pathlib import Path\n        import pandas as pd\n        from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction  # type: ignore\n        from chromadb.api.models.AsyncCollection import AsyncCollection\n        from chromadb.api import AsyncClientAPI\n\n        # Why load this here? Because it's needed in the rerank function, and we want to silence it.\n        from rerankers import Reranker  # type: ignore\n        from Kramer import get_course_title\n        import subprocess\n        import platform\n        from typing import Literal\n\n# Now that we've imported these libraries, we can set the logging levels.\nlogging.getLogger(&quot;sentence_transformers&quot;).setLevel(logging.ERROR)\nlogging.getLogger(&quot;transformers&quot;).setLevel(logging.ERROR)\nlogging.getLogger(&quot;pytorch_transformers&quot;).setLevel(logging.ERROR)\n\n\n# Two import config functions\ndef detect_device() -&gt; Literal[&quot;cuda&quot;, &quot;mps&quot;, &quot;cpu&quot;]:\n    &quot;&quot;&quot;\n    Detect available hardware acceleration for tensor operations without importing torch.\n    Returns 'cuda', 'mps', or 'cpu' based on what's available.\n    &quot;&quot;&quot;\n    # Check for CUDA GPUs\n    try:\n        # Try to use nvidia-smi command to detect NVIDIA GPUs\n        result = subprocess.run(\n            [&quot;nvidia-smi&quot;], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=1\n        )\n        if result.returncode == 0:\n            return &quot;cuda&quot;\n    except (subprocess.SubprocessError, FileNotFoundError):\n        pass\n\n    # Check for Apple MPS (Metal Performance Shaders)\n    if platform.system() == &quot;Darwin&quot;:  # macOS\n        # Check for Apple Silicon\n        try:\n            result = subprocess.run(\n                [&quot;sysctl&quot;, &quot;-n&quot;, &quot;machdep.cpu.brand_string&quot;],\n                stdout=subprocess.PIPE,\n                text=True,\n            )\n            if &quot;Apple&quot; in result.stdout:\n                return &quot;mps&quot;\n        except (subprocess.SubprocessError, FileNotFoundError):\n            pass\n\n    # Default to CPU\n    print(&quot;Neither CUDA nor MPS detected; defaulting to CPU for embeddings.&quot;)\n    return &quot;cpu&quot;\n\n\ndef detect_host() -&gt; Literal[&quot;local&quot;, &quot;remote&quot;]:\n    &quot;&quot;&quot;\n    Determines host name and whether server should be accessed remote or as localhost.\n    &quot;&quot;&quot;\n    hostnames = {\n        &quot;remote&quot;: [&quot;Botvinnik&quot;, &quot;bianders-mn7180.linkedin.biz&quot;],\n        &quot;local&quot;: [&quot;Caruana&quot;],\n    }\n    hostname = subprocess.check_output([&quot;hostname&quot;]).decode(&quot;utf-8&quot;).strip()\n    if hostname in hostnames[&quot;local&quot;]:\n        return &quot;local&quot;\n    else:\n        return &quot;remote&quot;\n\n\nasync def get_client() -&gt; AsyncClientAPI:\n    &quot;&quot;&quot;\n    Get the Chroma client. This is a wrapper around the chromadb.AsyncHttpClient.\n    &quot;&quot;&quot;\n    if detect_host() == &quot;local&quot;:\n        # Localhost\n        return await chromadb.AsyncHttpClient(port=8001)\n    else:\n        # &quot;remote&quot; here means local network (vs. localhost)\n        # Can update this later for local vs. remote networks\n        return await chromadb.AsyncHttpClient(&quot;10.0.0.82&quot;, port=8001)\n\n\n# -----------------------------------------------------------------\n# Constants\n## Our input data\ndir_path = Path(__file__).parent\nxlsx_file = dir_path.parent / &quot;data&quot; / &quot;courselist_all.xlsx&quot;\n## Configure embedding function\nembedding_model: Literal[&quot;gtr-t5-large&quot;, &quot;all-MiniLM-L6-v2&quot;] = &quot;all-MiniLM-L6-v2&quot;\nembedding_function = SentenceTransformerEmbeddingFunction(\n    model_name=embedding_model, device=detect_device()\n)\n## Our chroma client\nclient = asyncio.run(get_client())  # Preload the client for faster access later\n## And this little thing\ncurrent = 0  # Global variable for progress bar\n\n\ndef clean_text(text):\n    &quot;&quot;&quot;\n    This is useful for all Cosmo data you're bringing in through pandas.\n    &quot;&quot;&quot;\n    # Decode HTML entities\n    text = html.unescape(text)\n    # Handle common encoding issues\n    text = text.encode(&quot;ascii&quot;, &quot;ignore&quot;).decode(&quot;ascii&quot;)\n    # Remove any remaining HTML tags\n    text = re.sub(&quot;&lt;[^&lt;]+?&gt;&quot;, &quot;&quot;, text)\n    return text.strip()\n\n\nasync def heartbeat(client):\n    &quot;&quot;&quot;Generate chroma function; same with the course titles collection.&quot;&quot;&quot;\n    try:\n        _ = await client.heartbeat()\n        print(&quot;Chroma server found.&quot;)\n    except ValueError as e:\n        # Handle tenant/database connection issues\n        print(f&quot;Database connection error: {e}&quot;)\n        raise Exception(f&quot;ChromaDB connection failed - {str(e)}&quot;)\n\n\nasync def load_course_titles_from_mongodb():\n    &quot;&quot;&quot;\n    Load course titles from MongoDB course_mapping collection into ChromaDB.\n    This ensures ChromaDB only contains courses that exist in MongoDB.\n    &quot;&quot;&quot;\n    from Kramer.database.MongoDB_course_mapping import get_course_ids, get_course_title\n\n    client = await get_client()\n    try:\n        await client.delete_collection(name=&quot;course_titles&quot;)\n    except:\n        pass\n\n    collection = await client.create_collection(\n        name=&quot;course_titles&quot;, embedding_function=embedding_function\n    )\n\n    # Get all course IDs from MongoDB\n    course_ids = get_course_ids()\n    items = [(course_id, get_course_title(course_id)) for course_id in course_ids]\n\n    # Batch into chunks and load\n    chunks = list(enumerate([items[i : i + 500] for i in range(0, len(items), 500)]))\n    coroutines = [load_batch(collection, chunk, len(chunks)) for chunk in chunks]\n    await asyncio.gather(*coroutines)\n\n\nasync def load_descriptions_from_mongodb():\n    &quot;&quot;&quot;\n    Load course descriptions from MongoDB courses collection into ChromaDB.\n    Only includes courses that exist in both course_mapping and main courses collection.\n    &quot;&quot;&quot;\n    from Kramer.database.MongoDB_CRUD import get_all_courses_sync\n\n    client = await get_client()\n    try:\n        await client.delete_collection(name=&quot;course_descriptions&quot;)\n    except:\n        pass\n\n    collection = await client.create_collection(\n        name=&quot;course_descriptions&quot;, embedding_function=embedding_function\n    )\n\n    # Get courses from MongoDB (these are guaranteed to exist)\n    courses = get_all_courses_sync()\n    items = []\n    for course in courses:\n        if course.metadata.get(&quot;Course Description&quot;):\n            items.append(\n                (course.course_admin_id, course.metadata[&quot;Course Description&quot;])\n            )\n\n    # Batch and load\n    chunks = list(enumerate([items[i : i + 100] for i in range(0, len(items), 100)]))\n\n    # Progress content manager\n    with Progress() as progress:\n        task = progress.add_task(&quot;[green]Loading ChromaDB...&quot;, total=len(chunks))\n\n        async def load_batch_with_progress(collection, chunk) -&gt; None:\n            result = await load_batch(collection, chunk, len(chunks))\n            progress.advance(task)\n            return result\n\n        coroutines = [load_batch_with_progress(collection, chunk) for chunk in chunks]\n        await asyncio.gather(*coroutines)\n\n\n# Batch items into chunks of 100; enumerate them so we can pass the index to the coroutine\nasync def load_batch(collection: AsyncCollection, chunk: tuple, total: int):\n    _, items = chunk\n    for item in items:\n        await collection.add(\n            documents=[item[1]],\n            ids=[str(item[0])],\n        )\n\n\n# Query functions\nasync def query_course_descriptions(query: str, num_results: int = 5) -&gt; list[dict]:\n    global client\n    collection = await client.get_collection(name=&quot;course_descriptions&quot;)\n    results = await collection.query(query_texts=[query])\n    results = list(  # type: ignore\n        zip(results[&quot;ids&quot;][0][:num_results], results[&quot;documents&quot;][0][:num_results])  # type: ignore\n    )\n    output_list = []\n    for result in results:\n        course_title = get_course_title(int(result[0]))\n        course_description = clean_text(result[1])\n        output_list.append(\n            {\n                &quot;course_title&quot;: course_title,\n                &quot;course_description&quot;: course_description,\n            }\n        )\n    return output_list\n\n\ndef query_course_descriptions_sync(query: str, num_results: int = 5):\n    return asyncio.run(query_course_descriptions(query, num_results))\n\n\nasync def main():\n    # client = await chromadb.AsyncHttpClient(port=8001)\n    # await heartbeat(client)\n    # await load_descriptions_into_chroma()\n    query = &quot;python essential training algorithms&quot;\n    results = await query_course_descriptions(query)\n    print(results)\n\n\nif __name__ == &quot;__main__&quot;:\n    asyncio.run(main())\n</file>\n    <file path=\"src/siphon/database/pgvector/vector_db.py\">from siphon.database.postgres.PGRES_connection import PGRESConnection\n</file>\n    <file path=\"src/siphon/database/fallback/fallback_manager.py\">&quot;&quot;&quot;\nMain fallback cache manager - coordinates between PostgreSQL and SQLite\n&quot;&quot;&quot;\n\nimport time\nfrom typing import Optional\nfrom siphon.data.processed_content import ProcessedContent\nfrom siphon.database.interfaces.cache_interface import PostgreSQLCache\nfrom siphon.database.sqlite.sqlite_cache import SQLiteCache\nfrom siphon.database.fallback.sync_queue import SyncQueue\nfrom siphon.logs.logging_config import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass FallbackCacheManager:\n    def __init__(self):\n        self.postgres_cache = PostgreSQLCache()\n        self.sqlite_cache = SQLiteCache()\n        self.sync_queue = SyncQueue()\n        self._last_postgres_attempt = 0\n        self._postgres_retry_interval = 60  # seconds between sync attempts\n        self._postgres_available = (\n            None  # None=unknown, True=available, False=unavailable\n        )\n\n    def get_cached_content(self, uri_key: str) -&gt; Optional[ProcessedContent]:\n        &quot;&quot;&quot;\n        Try PostgreSQL first, fall back to SQLite.\n        This is your drop-in replacement for get_cached_content()\n        &quot;&quot;&quot;\n        # Try PostgreSQL first (if we think it might be available)\n        if self._postgres_available is not False:\n            try:\n                content = self.postgres_cache.get(uri_key)\n                if content:\n                    # PostgreSQL worked! Mark it as available and ensure SQLite backup\n                    self._postgres_available = True\n                    try:\n                        self.sqlite_cache.store(content)\n                    except Exception as e:\n                        logger.warning(f&quot;Failed to backup to SQLite: {e}&quot;)\n\n                    # Since PostgreSQL is working, try to clear sync backlog\n                    self._try_sync_backlog()\n                    return content\n            except Exception as e:\n                logger.debug(f&quot;PostgreSQL unavailable: {e}&quot;)\n                self._postgres_available = False\n\n        # Fall back to SQLite\n        content = self.sqlite_cache.get(uri_key)\n        if content:\n            logger.info(f&quot;SQLite cache hit for {uri_key}&quot;)\n\n        return content\n\n    def cache_processed_content(self, content: ProcessedContent) -&gt; str:\n        &quot;&quot;&quot;\n        Always cache to SQLite, try PostgreSQL, queue if it fails.\n        This is your drop-in replacement for cache_processed_content()\n        &quot;&quot;&quot;\n        uri_key = content.uri.uri\n\n        # Always store in SQLite first (this should never fail)\n        try:\n            self.sqlite_cache.store(content)\n            logger.debug(f&quot;Cached to SQLite: {uri_key}&quot;)\n        except Exception as e:\n            logger.error(f&quot;SQLite cache failed (this is bad!): {e}&quot;)\n            # Still try PostgreSQL even if SQLite failed\n\n        # Try PostgreSQL if we think it might be available\n        if self._postgres_available is not False:\n            try:\n                result = self.postgres_cache.store(content)\n                logger.debug(f&quot;Cached to PostgreSQL: {uri_key}&quot;)\n                self._postgres_available = True\n\n                # Since PostgreSQL worked, try to clear any backlog\n                self._try_sync_backlog()\n                return result\n\n            except Exception as e:\n                logger.info(\n                    f&quot;PostgreSQL cache failed, queuing for sync: {uri_key} - {e}&quot;\n                )\n                self._postgres_available = False\n\n                # Queue for later sync\n                self.sync_queue.add_pending_sync(content)\n        else:\n            # We know PostgreSQL is down, so just queue it\n            logger.debug(f&quot;PostgreSQL known to be down, queuing: {uri_key}&quot;)\n            self.sync_queue.add_pending_sync(content)\n\n        return uri_key\n\n    def _try_sync_backlog(self):\n        &quot;&quot;&quot;Opportunistically sync queued items when PostgreSQL is working&quot;&quot;&quot;\n        current_time = time.time()\n\n        # Don't spam sync attempts\n        if current_time - self._last_postgres_attempt &lt; self._postgres_retry_interval:\n            return\n\n        self._last_postgres_attempt = current_time\n\n        if self.sync_queue.queue:\n            result = self.sync_queue.process_sync_batch()\n            if result[&quot;synced&quot;] &gt; 0:\n                logger.info(\n                    f&quot;Synced {result['synced']} items from backlog to PostgreSQL&quot;\n                )\n\n            # If we had failures, PostgreSQL might be down again\n            if result[&quot;failed&quot;] &gt; 0:\n                self._postgres_available = False\n\n    def force_sync_backlog(self, batch_size: int = 50) -&gt; dict:\n        &quot;&quot;&quot;\n        Manually trigger a sync attempt (useful for debugging or manual intervention)\n        Returns sync statistics\n        &quot;&quot;&quot;\n        logger.info(&quot;Manual sync backlog triggered&quot;)\n        result = self.sync_queue.process_sync_batch(batch_size=batch_size)\n\n        # Update PostgreSQL availability based on results\n        if result[&quot;synced&quot;] &gt; 0:\n            self._postgres_available = True\n        elif result[&quot;failed&quot;] &gt; 0 and result[&quot;synced&quot;] == 0:\n            self._postgres_available = False\n\n        return result\n\n    def get_fallback_stats(self) -&gt; dict:\n        &quot;&quot;&quot;Get comprehensive statistics about the fallback cache system&quot;&quot;&quot;\n        try:\n            sqlite_stats = self.sqlite_cache.get_stats()\n        except Exception as e:\n            sqlite_stats = {&quot;error&quot;: str(e)}\n\n        queue_stats = self.sync_queue.get_queue_stats()\n\n        return {\n            &quot;postgres_available&quot;: self._postgres_available,\n            &quot;last_postgres_attempt&quot;: self._last_postgres_attempt,\n            &quot;sqlite_cache&quot;: sqlite_stats,\n            &quot;sync_queue&quot;: queue_stats,\n            &quot;next_sync_items&quot;: self.sync_queue.peek_next_items(3),\n        }\n\n    def cache_exists(self, uri_key: str) -&gt; bool:\n        &quot;&quot;&quot;Check if content exists in either cache&quot;&quot;&quot;\n        # Try PostgreSQL first if available\n        if self._postgres_available is not False:\n            try:\n                if self.postgres_cache.exists(uri_key):\n                    self._postgres_available = True\n                    return True\n            except Exception:\n                self._postgres_available = False\n\n        # Check SQLite\n        return self.sqlite_cache.exists(uri_key)\n\n    def clear_all_caches(self) -&gt; dict:\n        &quot;&quot;&quot;Clear both caches and sync queue. Use with caution!&quot;&quot;&quot;\n        results = {}\n\n        try:\n            results[&quot;sqlite_cleared&quot;] = self.sqlite_cache.clear_cache()\n        except Exception as e:\n            results[&quot;sqlite_error&quot;] = str(e)\n\n        try:\n            results[&quot;queue_cleared&quot;] = self.sync_queue.clear_queue()\n        except Exception as e:\n            results[&quot;queue_error&quot;] = str(e)\n\n        logger.warning(f&quot;Cleared all fallback caches: {results}&quot;)\n        return results\n</file>\n    <file path=\"src/siphon/database/fallback/sync_queue.py\">&quot;&quot;&quot;\nQueue management for syncing SQLite cache to PostgreSQL when connection is restored\n&quot;&quot;&quot;\n\nimport time\nfrom collections import deque\nfrom threading import Lock\nfrom typing import Any\nfrom siphon.data.processed_content import ProcessedContent\nfrom siphon.logs.logging_config import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass SyncQueue:\n    def __init__(self, max_queue_size: int = 1000):\n        self.queue = deque()\n        self.lock = Lock()  # Thread safety for concurrent access\n        self.max_queue_size = max_queue_size\n        self._dropped_items = 0  # Track if we had to drop items due to size\n\n    def add_pending_sync(self, content: ProcessedContent) -&gt; bool:\n        &quot;&quot;&quot;\n        Add content that needs to sync to PostgreSQL.\n        Returns True if added, False if queue is full and item was dropped.\n        &quot;&quot;&quot;\n        with self.lock:\n            # Check if we're at capacity\n            if len(self.queue) &gt;= self.max_queue_size:\n                # Drop the oldest item to make room\n                dropped = self.queue.popleft()\n                self._dropped_items += 1\n                logger.warning(\n                    f&quot;Sync queue full, dropped oldest item: {dropped['uri_key']} &quot;\n                    f&quot;(total dropped: {self._dropped_items})&quot;\n                )\n\n            # Add the new item\n            sync_item = {\n                &quot;operation&quot;: &quot;store&quot;,\n                &quot;content&quot;: content,\n                &quot;timestamp&quot;: time.time(),\n                &quot;uri_key&quot;: content.uri.uri,\n                &quot;retry_count&quot;: 0,\n            }\n\n            self.queue.append(sync_item)\n            logger.debug(\n                f&quot;Added to sync queue: {content.uri.uri} (queue size: {len(self.queue)})&quot;\n            )\n            return True\n\n    def process_sync_batch(\n        self, batch_size: int = 10, max_retries: int = 3\n    ) -&gt; dict[str, int]:\n        &quot;&quot;&quot;\n        Process a batch of pending syncs.\n        Returns dict with 'synced', 'failed', 'requeued' counts.\n        &quot;&quot;&quot;\n        if not self.queue:\n            return {&quot;synced&quot;: 0, &quot;failed&quot;: 0, &quot;requeued&quot;: 0}\n\n        synced = 0\n        failed = 0\n        requeued = 0\n\n        # Process up to batch_size items\n        for _ in range(min(batch_size, len(self.queue))):\n            if not self.queue:\n                break\n\n            with self.lock:\n                pending_item = self.queue.popleft()\n\n            try:\n                # Import here to avoid circular imports\n                from siphon.database.interfaces.cache_interface import PostgreSQLCache\n\n                postgres_cache = PostgreSQLCache()\n\n                # Try to sync to PostgreSQL\n                postgres_cache.store(pending_item[&quot;content&quot;])\n                synced += 1\n                logger.debug(f&quot;Synced to PostgreSQL: {pending_item['uri_key']}&quot;)\n\n            except Exception as e:\n                pending_item[&quot;retry_count&quot;] += 1\n\n                if pending_item[&quot;retry_count&quot;] &lt;= max_retries:\n                    # Put it back at the front for retry\n                    with self.lock:\n                        self.queue.appendleft(pending_item)\n                    requeued += 1\n                    logger.debug(\n                        f&quot;Sync failed, requeued (attempt {pending_item['retry_count']}): &quot;\n                        f&quot;{pending_item['uri_key']} - {e}&quot;\n                    )\n                else:\n                    # Give up after max retries\n                    failed += 1\n                    logger.error(\n                        f&quot;Sync failed permanently after {max_retries} attempts: &quot;\n                        f&quot;{pending_item['uri_key']} - {e}&quot;\n                    )\n\n                # If PostgreSQL is still down, don't try more items\n                if &quot;Connection&quot; in str(e) or &quot;connection&quot; in str(e).lower():\n                    logger.debug(&quot;PostgreSQL appears to be down, stopping batch sync&quot;)\n                    break\n\n        result = {&quot;synced&quot;: synced, &quot;failed&quot;: failed, &quot;requeued&quot;: requeued}\n\n        if synced &gt; 0 or failed &gt; 0:\n            logger.info(f&quot;Sync batch completed: {result}&quot;)\n\n        return result\n\n    def get_queue_stats(self) -&gt; dict[str, Any]:\n        &quot;&quot;&quot;Get statistics about the current sync queue&quot;&quot;&quot;\n        with self.lock:\n            if not self.queue:\n                return {\n                    &quot;queue_size&quot;: 0,\n                    &quot;oldest_item_age_seconds&quot;: 0,\n                    &quot;newest_item_age_seconds&quot;: 0,\n                    &quot;dropped_items_total&quot;: self._dropped_items,\n                }\n\n            current_time = time.time()\n            oldest_timestamp = min(item[&quot;timestamp&quot;] for item in self.queue)\n            newest_timestamp = max(item[&quot;timestamp&quot;] for item in self.queue)\n\n            return {\n                &quot;queue_size&quot;: len(self.queue),\n                &quot;oldest_item_age_seconds&quot;: current_time - oldest_timestamp,\n                &quot;newest_item_age_seconds&quot;: current_time - newest_timestamp,\n                &quot;dropped_items_total&quot;: self._dropped_items,\n                &quot;max_queue_size&quot;: self.max_queue_size,\n            }\n\n    def clear_queue(self) -&gt; int:\n        &quot;&quot;&quot;Clear all pending sync items. Returns number of items cleared.&quot;&quot;&quot;\n        with self.lock:\n            cleared_count = len(self.queue)\n            self.queue.clear()\n            logger.warning(\n                f&quot;Cleared sync queue. Removed {cleared_count} pending items.&quot;\n            )\n            return cleared_count\n\n    def peek_next_items(self, count: int = 5) -&gt; list[dict[str, Any]]:\n        &quot;&quot;&quot;Peek at the next few items in the queue without removing them&quot;&quot;&quot;\n        with self.lock:\n            items = []\n            for i, item in enumerate(self.queue):\n                if i &gt;= count:\n                    break\n                items.append(\n                    {\n                        &quot;uri_key&quot;: item[&quot;uri_key&quot;],\n                        &quot;timestamp&quot;: item[&quot;timestamp&quot;],\n                        &quot;retry_count&quot;: item[&quot;retry_count&quot;],\n                        &quot;age_seconds&quot;: time.time() - item[&quot;timestamp&quot;],\n                    }\n                )\n            return items\n</file>\n    <file path=\"src/siphon/data/context.py\">from pydantic import BaseModel, Field\nfrom functools import lru_cache\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.uri import URI\nfrom siphon.logs.logging_config import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass Context(BaseModel):\n    &quot;&quot;&quot;\n    LLM context generated from the URI.\n    Only one attribute, context -- but this class hides the vast majority of logic in this project.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = Field(\n        ...,\n        description=&quot;Type of source for the metadata, e.g., YouTube, File, Article, etc.&quot;,\n    )\n    context: str = Field(..., description=&quot;The context string generated from the URI.&quot;)\n\n    @classmethod\n    @lru_cache(maxsize=1000)\n    def from_uri(cls, uri: URI) -&gt; &quot;Context&quot;:  # type: ignore\n        &quot;&quot;&quot;\n        Create a Context object from a URI object.\n        Subclasses for each SourceType override this; they return a context string.\n        &quot;&quot;&quot;\n        from siphon.context.context_classes import ContextClasses\n\n        logger.info(f&quot;Looking for context class for sourcetype: {uri.sourcetype.value}&quot;)\n\n        # Calculate this from the class name minus &quot;Context&quot;\n        for context_class in ContextClasses:\n            logger.info(\n                f&quot;Checking class: {context_class.__name__} against {uri.sourcetype.value}&quot;\n            )\n            if context_class.__name__.replace(&quot;Context&quot;, &quot;&quot;) == uri.sourcetype.value:\n                logger.info(f&quot;Using URI class: {context_class.__name__}&quot;)\n                return context_class.from_uri(uri)\n\n        raise ValueError(f&quot;No Context class found for source type {uri.sourcetype}.&quot;)\n\n    def __hash__(self) -&gt; int:\n        &quot;&quot;&quot;Make Context hashable for caching purposes&quot;&quot;&quot;\n        return hash((self.context, self.sourcetype))\n\n    def __eq__(self, other) -&gt; bool:\n        &quot;&quot;&quot;Define equality for hashing&quot;&quot;&quot;\n        if isinstance(other, Context):\n            return self.context == other.context and self.sourcetype == other.sourcetype\n        return False\n</file>\n    <file path=\"src/siphon/data/processed_content_display.py\">&quot;&quot;&quot;\nRich display mixin for ProcessedContent.\n\nUsage:\n    from siphon.display.processed_content_display import ProcessedContentDisplayMixin\n\n    class ProcessedContent(ProcessedContentDisplayMixin, BaseModel):\n        # ... your existing fields ...\n        pass\n\n    # Then use:\n    processed_content.pretty_print()\n&quot;&quot;&quot;\n\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.columns import Columns\nfrom rich.table import Table\nfrom rich.text import Text\nfrom rich import box\nfrom datetime import datetime\n\n\nclass ProcessedContentDisplayMixin:\n    &quot;&quot;&quot;Mixin to add Rich display capabilities to ProcessedContent.&quot;&quot;&quot;\n\n    def pretty_print(self) -&gt; None:\n        &quot;&quot;&quot;Display ProcessedContent in a beautiful, structured format.&quot;&quot;&quot;\n        console = Console()\n\n        # Header\n        console.print(self._header_panel())\n        console.print()\n\n        # Two-column layout: URI + Metadata\n        console.print(Columns([self._uri_panel(), self._metadata_panel()], equal=True))\n        console.print()\n\n        # Context preview\n        console.print(self._context_panel())\n        console.print()\n\n        # Synthetic data (if available)\n        if self.synthetic_data:\n            console.print(self._synthetic_panel())\n            console.print()\n\n    def _header_panel(self) -&gt; Panel:\n        &quot;&quot;&quot;Main header with title and URI.&quot;&quot;&quot;\n        title = (\n            self.title if self.title else f&quot;Content from {self.uri.sourcetype.value}&quot;\n        )\n\n        content = Text()\n        content.append(title, style=&quot;bold bright_blue&quot;)\n        content.append(f&quot;\\n\ud83d\udccd {self.uri.uri}&quot;, style=&quot;dim&quot;)\n\n        return Panel(content, box=box.DOUBLE, style=&quot;bright_blue&quot;, padding=(1, 2))\n\n    def _uri_panel(self) -&gt; Panel:\n        &quot;&quot;&quot;URI details panel.&quot;&quot;&quot;\n        table = Table(show_header=False, box=None, padding=(0, 1))\n        table.add_column(&quot;Field&quot;, style=&quot;cyan bold&quot;, width=12)\n        table.add_column(&quot;Value&quot;, style=&quot;white&quot;)\n\n        table.add_row(\n            &quot;Source Type&quot;, f&quot;[bright_green]{self.uri.sourcetype.value}[/bright_green]&quot;\n        )\n        table.add_row(&quot;Original&quot;, f&quot;[yellow]{self.uri.source}[/yellow]&quot;)\n        table.add_row(&quot;URI&quot;, f&quot;[blue]{self.uri.uri}[/blue]&quot;)\n\n        return Panel(\n            table, title=&quot;[bold cyan]\ud83d\udd17 URI[/bold cyan]&quot;, box=box.ROUNDED, style=&quot;cyan&quot;\n        )\n\n    def _context_panel(self) -&gt; Panel:\n        &quot;&quot;&quot;Context preview panel.&quot;&quot;&quot;\n        context_text = self.llm_context.context\n        word_count = len(context_text.split())\n        char_count = len(context_text)\n\n        # Show preview (first 400 chars)\n        preview = (\n            context_text[:400] + &quot;...&quot; if len(context_text) &gt; 400 else context_text\n        )\n\n        content = Text(preview, style=&quot;white&quot;)\n        content.append(f&quot;\\n\\n({word_count:,} words, {char_count:,} chars)&quot;, style=&quot;dim&quot;)\n\n        return Panel(\n            content,\n            title=&quot;[bold green]\ud83d\udcc4 Content[/bold green]&quot;,\n            box=box.ROUNDED,\n            style=&quot;green&quot;,\n        )\n\n    def _metadata_panel(self) -&gt; Panel:\n        &quot;&quot;&quot;Metadata panel with context-specific info.&quot;&quot;&quot;\n        table = Table(show_header=False, box=None, padding=(0, 1))\n        table.add_column(&quot;Field&quot;, style=&quot;bright_white bold&quot;, width=12)\n        table.add_column(&quot;Value&quot;, style=&quot;white&quot;)\n\n        context = self.llm_context\n\n        # File-based metadata\n        if hasattr(context, &quot;file_path&quot;):\n            from pathlib import Path\n\n            file_path = Path(context.file_path)\n            table.add_row(&quot;File&quot;, f&quot;[yellow]{file_path.name}[/yellow]&quot;)\n\n            if hasattr(context, &quot;file_size&quot;):\n                size_mb = context.file_size / (1024 * 1024)\n                table.add_row(&quot;Size&quot;, f&quot;[white]{size_mb:.1f} MB[/white]&quot;)\n\n            if hasattr(context, &quot;content_modified_at&quot;):\n                mod_time = datetime.fromtimestamp(context.content_modified_at)\n                table.add_row(\n                    &quot;Modified&quot;, f&quot;[white]{mod_time.strftime('%Y-%m-%d')}[/white]&quot;\n                )\n\n        # Online metadata\n        elif hasattr(context, &quot;url&quot;):\n            if hasattr(context, &quot;domain&quot;):\n                table.add_row(&quot;Domain&quot;, f&quot;[yellow]{context.domain}[/yellow]&quot;)\n\n            # YouTube specifics\n            if hasattr(context, &quot;channel&quot;):\n                table.add_row(&quot;Channel&quot;, f&quot;[bright_red]{context.channel}[/bright_red]&quot;)\n            if hasattr(context, &quot;duration&quot;):\n                duration = f&quot;{context.duration // 60}:{context.duration % 60:02d}&quot;\n                table.add_row(&quot;Duration&quot;, f&quot;[white]{duration}[/white]&quot;)\n            if hasattr(context, &quot;view_count&quot;):\n                table.add_row(&quot;Views&quot;, f&quot;[white]{context.view_count:,}[/white]&quot;)\n\n            # GitHub specifics\n            if hasattr(context, &quot;stars&quot;):\n                table.add_row(&quot;Stars&quot;, f&quot;[yellow]\u2b50 {context.stars:,}[/yellow]&quot;)\n            if hasattr(context, &quot;language&quot;):\n                table.add_row(\n                    &quot;Language&quot;, f&quot;[bright_blue]{context.language}[/bright_blue]&quot;\n                )\n\n        return Panel(\n            table,\n            title=&quot;[bold bright_white]\ud83d\udcca Metadata[/bold bright_white]&quot;,\n            box=box.ROUNDED,\n            style=&quot;bright_white&quot;,\n        )\n\n    def _synthetic_panel(self) -&gt; Panel:\n        &quot;&quot;&quot;AI-generated synthetic data panel.&quot;&quot;&quot;\n        if not self.synthetic_data:\n            return Panel(\n                Text(&quot;No synthetic data&quot;, style=&quot;dim&quot;),\n                title=&quot;[bold magenta]\ud83e\udd16 AI Data[/bold magenta]&quot;,\n                style=&quot;magenta&quot;,\n            )\n\n        table = Table(show_header=False, box=None, padding=(0, 1))\n        table.add_column(&quot;Field&quot;, style=&quot;magenta bold&quot;, width=12)\n        table.add_column(&quot;Value&quot;, style=&quot;white&quot;)\n\n        sd = self.synthetic_data\n\n        if sd.title:\n            table.add_row(&quot;Title&quot;, f&quot;[bright_yellow]{sd.title}[/bright_yellow]&quot;)\n\n        if sd.description:\n            desc = (\n                sd.description[:80] + &quot;...&quot;\n                if len(sd.description) &gt; 80\n                else sd.description\n            )\n            table.add_row(&quot;Description&quot;, f&quot;[white]{desc}[/white]&quot;)\n\n        if sd.summary:\n            # words = len(sd.summary.split())\n            table.add_row(&quot;Summary&quot;, f&quot;[white]{sd.summary} words[/white]&quot;)\n\n        if sd.topics:\n            topics = &quot;, &quot;.join(sd.topics[:3])\n            if len(sd.topics) &gt; 3:\n                topics += f&quot; (+{len(sd.topics) - 3})&quot;\n            table.add_row(&quot;Topics&quot;, f&quot;[bright_cyan]{topics}[/bright_cyan]&quot;)\n\n        if sd.entities:\n            entities = &quot;, &quot;.join(sd.entities[:3])\n            if len(sd.entities) &gt; 3:\n                entities += f&quot; (+{len(sd.entities) - 3})&quot;\n            table.add_row(&quot;Entities&quot;, f&quot;[bright_red]{entities}[/bright_red]&quot;)\n\n        return Panel(\n            table,\n            title=&quot;[bold magenta]\ud83e\udd16 AI Data[/bold magenta]&quot;,\n            box=box.ROUNDED,\n            style=&quot;magenta&quot;,\n        )\n</file>\n    <file path=\"src/siphon/data/synthetic_data.py\">from pydantic import BaseModel, Field\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.context import Context\nfrom siphon.logs.logging_config import get_logger\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from siphon.synthetic_data.synthetic_data_classes import SyntheticDataUnion\n\nlogger = get_logger(__name__)\n\n\nclass SyntheticData(BaseModel):\n    &quot;&quot;&quot;\n    AI-generated enrichments, applied as a &quot;finishing step&quot; to the content.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType\n\n    title: str = Field(\n        default=&quot;&quot;, description=&quot;Title of the content, either extracted or generated&quot;\n    )\n    description: str = Field(\n        default=&quot;&quot;, description=&quot;Short description or summary of the content&quot;\n    )\n    summary: str = Field(default=&quot;&quot;, description=&quot;Detailed summary of the content&quot;)\n    topics: list[str] = Field(\n        default_factory=list,\n        description=&quot;List of topics or keywords associated with the content, an area liable to change with cluster analyses.&quot;,\n    )\n    entities: list[str] = Field(\n        default_factory=list,\n        description=&quot;List of entities (people, places, organizations) mentioned in the content.&quot;,\n    )\n\n    @classmethod\n    def from_context(\n        cls,\n        context: Context,\n        cloud: bool = False,\n        model_str: str = &quot;gemini2.5&quot;,\n        server_side: bool = False,\n    ) -&gt; &quot;SyntheticDataUnion&quot;:\n        &quot;&quot;&quot;\n        Create an instance of SyntheticData from llm context, typically generated by an LLM.\n        Subclasses for each SourceType may override this.\n        &quot;&quot;&quot;\n        if not server_side:  # Remote by default\n            return cls._from_context_remote(context, cloud, model_str)\n        else:  # Direct call, e.g. server side\n            return cls._from_context_direct(context, cloud, model_str)\n\n    @classmethod\n    def _from_context_direct(cls, context, cloud, model_str) -&gt; &quot;SyntheticDataUnion&quot;:\n        &quot;&quot;&quot;\n        Directly create SyntheticData on this machine. (as opposed to the server call)\n        Can be cloud or local LLMs.\n        NOTE: currently hardcodes gpt-oss:latest for local generation.\n        &quot;&quot;&quot;\n        if cloud:\n            logger.info(&quot;Using cloud synthetic data generation&quot;)\n        else:\n            model_str = &quot;gpt-oss:latest&quot;\n            logger.info(&quot;Using local synthetic data generation&quot;)\n\n        from siphon.synthetic_data.synthetic_data_classes import (\n            SyntheticDataClasses,\n        )\n\n        source_type = context.sourcetype\n        for synthetic_data_class in SyntheticDataClasses:\n            # Match class name to source type value\n            expected_class_name = f&quot;{source_type.value}SyntheticData&quot;\n            if synthetic_data_class.__name__ == expected_class_name:\n                logger.info(\n                    f&quot;Using SyntheticData class: {synthetic_data_class.__name__}&quot;\n                )\n                return synthetic_data_class.from_context(\n                    context=context, model_str=model_str\n                )\n\n        raise ValueError(f&quot;No SyntheticData class found for source type {source_type}.&quot;)\n\n    @classmethod\n    def _from_context_remote(cls, context, cloud, model_str) -&gt; &quot;SyntheticDataUnion&quot;:\n        &quot;&quot;&quot;\n        Create SyntheticData by calling a remote service (e.g., siphonserver).\n        Fun fact: this turns into a call of the other method (_from_context_direct) on the server side.\n        &quot;&quot;&quot;\n        _ = cloud  # currently unused, as model_str contains the model choice\n        logger.info(&quot;Using local synthetic data generation&quot;)\n        from siphonserver.client.siphonclient import SiphonClient\n\n        from siphonserver.server.api.requests import SyntheticDataRequest\n\n        client = SiphonClient()\n        request = SyntheticDataRequest(context=context, model=model_str)\n        return client.generate_synthetic_data(request)\n</file>\n    <file path=\"src/siphon/data/processed_content.py\">&quot;&quot;&quot;\nProcessedContent is our final output format for content processed by siphon.\nIt combines our URI, LLM context, and synthetic data into a structured format, for storage / caching / retrieval from postgres.\nOur CLI also has a nice display method (.pretty_print()), which we add as a mixin to this class.\n&quot;&quot;&quot;\n\nfrom siphon.data.uri import URI\nfrom siphon.data.synthetic_data import SyntheticData\nfrom siphon.context.context_classes import ContextUnion\nfrom siphon.synthetic_data.synthetic_data_classes import SyntheticDataUnion\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.processed_content_display import ProcessedContentDisplayMixin\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Any, override\nimport time\n\n\nclass ProcessedContent(BaseModel, ProcessedContentDisplayMixin):\n    # Primary identifiers\n    uri: URI = Field(..., description=&quot;Original URI of the content, used for retrieval&quot;)\n\n    # AI context for LLM consumption\n    llm_context: ContextUnion = Field(\n        ..., description=&quot;Processed content + metadata ready for LLM consumption&quot;\n    )\n\n    # Synthetic data (title, description, summary, topics, entities) -- added post init\n    synthetic_data: SyntheticDataUnion = Field(\n        ..., description=&quot;AI-generated enrichments applied to the content&quot;\n    )\n\n    tags: list[str] = Field(\n        default_factory=list,\n        description=&quot;List of user-generated and auto-generated tags applied to the content for categorization&quot;,\n    )\n\n    # Timestamps created on creation and update\n    created_at: int = Field(\n        default_factory=lambda: int(time.time()),\n        description=&quot;Unix epoch timestamp when content was created&quot;,\n    )\n    updated_at: int = Field(\n        default_factory=lambda: int(time.time()),\n        description=&quot;Unix epoch timestamp when content was last updated&quot;,\n    )\n\n    @override\n    def model_post_init(self, __context: Any) -&gt; None:\n        &quot;&quot;&quot;\n        Validate that all component classes match the declared sourcetype.\n        &quot;&quot;&quot;\n        expected_sourcetype = self.uri.sourcetype\n\n        # Check URI class matches sourcetype\n        expected_uri_class = f&quot;{expected_sourcetype.value}URI&quot;\n        actual_uri_class = self.uri.__class__.__name__\n        if expected_uri_class != actual_uri_class:\n            raise ValueError(\n                f&quot;URI class mismatch for {expected_sourcetype.value}: &quot;\n                f&quot;expected {expected_uri_class}, got {actual_uri_class}&quot;\n            )\n\n        # Check Context class matches sourcetype\n        expected_context_class = f&quot;{expected_sourcetype.value}Context&quot;\n        actual_context_class = self.llm_context.__class__.__name__\n        if expected_context_class != actual_context_class:\n            raise ValueError(\n                f&quot;Context class mismatch for {expected_sourcetype.value}: &quot;\n                f&quot;expected {expected_context_class}, got {actual_context_class}&quot;\n            )\n\n        # Only check sourcetype fields if they exist\n        if (\n            hasattr(self.llm_context, &quot;sourcetype&quot;)\n            and self.llm_context.sourcetype != expected_sourcetype\n        ):\n            raise ValueError(\n                f&quot;Context sourcetype field mismatch: expected {expected_sourcetype.value}, &quot;\n                f&quot;got {self.llm_context.sourcetype.value}&quot;\n            )\n\n        # Check SyntheticData if present\n        if self.synthetic_data:\n            expected_synthetic_class = f&quot;{expected_sourcetype.value}SyntheticData&quot;\n            actual_synthetic_class = self.synthetic_data.__class__.__name__\n            if expected_synthetic_class != actual_synthetic_class:\n                raise ValueError(\n                    f&quot;SyntheticData class mismatch for {expected_sourcetype.value}: &quot;\n                    f&quot;expected {expected_synthetic_class}, got {actual_synthetic_class}&quot;\n                )\n\n            if (\n                hasattr(self.synthetic_data, &quot;sourcetype&quot;)\n                and self.synthetic_data.sourcetype != expected_sourcetype\n            ):\n                raise ValueError(\n                    f&quot;SyntheticData sourcetype field mismatch: expected {expected_sourcetype.value}, &quot;\n                    f&quot;got {self.synthetic_data.sourcetype.value}&quot;\n                )\n\n    def touch(self) -&gt; None:\n        &quot;&quot;&quot;\n        Update the updated_at timestamp\n        &quot;&quot;&quot;\n        self.updated_at = int(time.time())\n\n    @classmethod\n    def create_with_embeddings(\n        cls, content: &quot;ProcessedContent&quot;\n    ) -&gt; tuple[&quot;ProcessedContent&quot;, dict]:\n        &quot;&quot;&quot;\n        Class method that generates embeddings for database insertion.\n\n        Returns:\n            tuple: (ProcessedContent object, embedding_data dict)\n        &quot;&quot;&quot;\n        from sentence_transformers import SentenceTransformer\n\n        model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)\n        embeddings = {}\n\n        if content.synthetic_data:\n            try:\n                if content.synthetic_data.description:\n                    embeddings[&quot;description_embedding&quot;] = model.encode(\n                        content.synthetic_data.description\n                    ).tolist()\n            except:\n                pass\n\n            try:\n                if content.synthetic_data.summary:\n                    embeddings[&quot;summary_embedding&quot;] = model.encode(\n                        content.synthetic_data.summary\n                    ).tolist()\n            except:\n                pass\n\n        return content, embeddings\n\n    @property\n    def context(self) -&gt; str:\n        &quot;&quot;&quot;\n        Full context text.\n        &quot;&quot;&quot;\n        return self.llm_context.context\n\n    @property\n    def title(self) -&gt; str:\n        &quot;&quot;&quot;Title of the content, derived from synthetic data if available.&quot;&quot;&quot;\n        return self.synthetic_data.title if self.synthetic_data else &quot;&quot;\n\n    @property\n    def description(self) -&gt; str:\n        &quot;&quot;&quot;Title of the content, derived from synthetic data if available.&quot;&quot;&quot;\n        return self.synthetic_data.description if self.synthetic_data else &quot;&quot;\n\n    @property\n    def summary(self) -&gt; str:\n        &quot;&quot;&quot;Summary of the content, derived from synthetic data if available.&quot;&quot;&quot;\n        return self.synthetic_data.summary if self.synthetic_data else &quot;&quot;\n\n    @property\n    def id(self) -&gt; str:\n        &quot;&quot;&quot;Unique identifier for this processed content, derived from URI.&quot;&quot;&quot;\n        return self.uri.uri\n\n    def model_dump_for_cache(self) -&gt; dict[str, Any]:\n        &quot;&quot;&quot;Simplified serialization for caching - stores inputs to factories.&quot;&quot;&quot;\n        return {\n            # Store the raw source - we can reconstruct URI from this\n            &quot;source&quot;: self.uri.source,\n            &quot;sourcetype&quot;: self.uri.sourcetype.value,\n            # Store context data + type info\n            &quot;context_data&quot;: self.llm_context.model_dump(),\n            # Store synthetic data\n            &quot;synthetic_data&quot;: self.synthetic_data.model_dump()\n            if self.synthetic_data\n            else None,\n            # Store tags\n            &quot;tags&quot;: self.tags,\n        }\n\n    @classmethod\n    def model_validate_from_cache(cls, data: dict[str, Any]) -&gt; &quot;ProcessedContent&quot;:\n        &quot;&quot;&quot;Reconstruct ProcessedContent using your existing factories.&quot;&quot;&quot;\n\n        # 1. Reconstruct URI using your existing from_source method\n        uri = URI.from_source(\n            data[&quot;source&quot;], skip_checksum=True\n        )  # skip checksum since paths are host-specific\n        if not uri:\n            raise ValueError(f&quot;Could not reconstruct URI from source: {data['source']}&quot;)\n\n        # 2. Reconstruct Context using your existing class loading\n        from siphon.context.context_classes import ContextClasses\n\n        sourcetype = SourceType(data[&quot;sourcetype&quot;])\n        context_class = None\n\n        for cls_candidate in ContextClasses:\n            if cls_candidate.__name__.replace(&quot;Context&quot;, &quot;&quot;) == sourcetype.value:\n                context_class = cls_candidate\n                break\n        if not context_class:\n            raise ValueError(\n                f&quot;Could not find Context class for sourcetype: {sourcetype.value}&quot;\n            )\n\n        # Reconstruct context with stored data\n        llm_context = context_class.model_validate(data[&quot;context_data&quot;])\n\n        # 3. Reconstruct SyntheticData\n        synthetic_data = None\n        if data.get(&quot;synthetic_data&quot;):\n            from siphon.synthetic_data.synthetic_data_classes import (\n                SyntheticDataClasses,\n            )\n\n            # Find the right SyntheticData subclass\n            synthetic_data_class = None\n            for cls_candidate in SyntheticDataClasses:\n                if (\n                    cls_candidate.__name__.replace(&quot;SyntheticData&quot;, &quot;&quot;)\n                    == sourcetype.value\n                ):\n                    synthetic_data_class = cls_candidate\n                    break\n\n            if not synthetic_data_class:\n                # Fallback to base SyntheticData class\n                synthetic_data_class = SyntheticData\n\n            synthetic_data = synthetic_data_class.model_validate(data[&quot;synthetic_data&quot;])\n\n        return cls(\n            uri=uri,\n            llm_context=llm_context,\n            synthetic_data=synthetic_data,\n            tags=data.get(&quot;tags&quot;, []),\n        )\n\n    @override\n    def __str__(self) -&gt; str:\n        # Return a pretty printed json representation for easy logging -- truncate strings &gt; 100 chars\n        import json\n\n        json_dict = self.model_dump()\n        for key, value in json_dict.items():\n            if isinstance(value, str) and len(value) &gt; 100:\n                json_dict[key] = value[:100] + &quot;...&quot;\n        indented = json.dumps(json_dict, indent=2)\n        return indented\n</file>\n    <file path=\"src/siphon/data/context_call.py\">from siphon.data.type_definitions.extensions import Extensions as extensions\nfrom pydantic import BaseModel, Field\nimport re, base64\nfrom pathlib import Path\n\n\ndef is_base64_simple(s):\n    &quot;&quot;&quot;\n    Simple validation for base64 strings.\n    &quot;&quot;&quot;\n    return bool(re.match(r&quot;^[A-Za-z0-9+/]*={0,2}$&quot;, s)) and len(s) % 4 == 0\n\n\nclass ContextCall(BaseModel):\n    &quot;&quot;&quot;\n    ContextCall is a Pydantic model that represents a context to SiphonServer.\n    Take an extension and base64 and it will process, returning the llm_context as a string.\n    We can build on this, but for now, it is a simple model that takes a file path and processes it.\n    &quot;&quot;&quot;\n\n    extension: str = Field(description=&quot;The extension to use for processing the file.&quot;)\n    base64_data: str = Field(description=&quot;The base64 encoded content to process.&quot;)\n\n    def model_post_init(self, __context):\n        &quot;&quot;&quot;\n        Post-initialization processing to validate the extension and process the base64 data.\n        &quot;&quot;&quot;\n        _ = __context\n        if (\n            self.extension\n            not in extensions[&quot;audio&quot;] + extensions[&quot;video&quot;] + extensions[&quot;image&quot;]\n        ):\n            raise ValueError(f&quot;Extension '{self.extension}' is not supported.&quot;)\n        if not is_base64_simple(self.base64_data):\n            raise ValueError(&quot;Base64 data is not valid.&quot;)\n\n\ndef create_ContextCall_from_file(file_path: str | Path) -&gt; ContextCall:\n    &quot;&quot;&quot;\n    Create a ContextCall object from a file path.\n    The file is read, encoded in base64, and the extension is extracted.\n    &quot;&quot;&quot;\n    if not isinstance(file_path, Path):\n        file_path = Path(file_path)\n    if not file_path.exists():\n        raise FileNotFoundError(f&quot;File not found: {file_path}&quot;)\n\n    with open(file_path, &quot;rb&quot;) as f:\n        file_data = f.read()\n\n    base64_data = base64.b64encode(file_data).decode(&quot;utf-8&quot;)\n    extension = file_path.suffix.lower()\n\n    return ContextCall(extension=extension, base64_data=base64_data)\n</file>\n    <file path=\"src/siphon/data/uri.py\">&quot;&quot;&quot;\nTakes a source (string representation of file_path, url, etc.) and returns a URI object, which is necessary for our ProcessedContent and in fact our main identifier for caching.\nTwo essential artifacts are created here:\n- the URI (which is main identifier)\n- the SourceType (which routes our pipeline from e2e)\n&quot;&quot;&quot;\n\nfrom functools import lru_cache\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.logs.logging_config import get_logger\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nfrom pathlib import Path\n\nlogger = get_logger(__name__)\n\n\nclass URI(BaseModel):\n    source: str = Field(..., description=&quot;The original source URL, filepath, etc.&quot;)\n    sourcetype: SourceType = Field(\n        ..., description=&quot;The type of source this URI represents.&quot;\n    )\n    uri: str = Field(..., description=&quot;The URI string representation of the source.&quot;)\n\n    # For files, we can store a checksum to verify integrity\n    checksum: str | None = Field(\n        default=None,\n        description=&quot;Checksum of the text file for integrity verification.&quot;,\n    )\n\n    @classmethod\n    def identify(cls, source: str) -&gt; bool: ...  # type: ignore\n\n    @classmethod\n    @lru_cache(maxsize=1000)\n    def from_source(\n        cls, source: str | Path, skip_checksum: bool = False\n    ) -&gt; Optional[&quot;URI&quot;]:\n        &quot;&quot;&quot;\n        Create a URI object from a source string.\n        Tries each URI subclass until one can handle the source.\n        &quot;&quot;&quot;\n        from siphon.uri.uri_classes import URIClasses\n\n        logger.info(&quot;Source string received.&quot;)\n\n        if isinstance(source, Path):\n            source = str(source)\n\n        # Try each URI class to see if it can handle this source\n        for uri_class in URIClasses:\n            if uri_class.identify(source):  # Each subclass has its own identify method\n                logger.info(f&quot;Using URI class: {uri_class.__name__}&quot;)\n                return uri_class.from_source(source, skip_checksum=skip_checksum)\n\n        # No class could handle this source\n        logger.warning(f&quot;Unsupported source format: {source}&quot;)\n        return None\n\n    @classmethod\n    def get_checksum(cls, file_path: str | Path) -&gt; str:\n        &quot;&quot;&quot;\n        Calculate the checksum of a file using the specified algorithm.\n        &quot;&quot;&quot;\n        # Coerce to Path if a string is provided\n        file_path = Path(file_path) if isinstance(file_path, str) else file_path\n\n        # Ensure the file exists and is not empty\n        assert file_path.is_file(), f&quot;File does not exist: {file_path}&quot;\n        assert file_path.stat().st_size &gt; 0, f&quot;File is empty: {file_path}&quot;\n\n        # Calculate checksum\n        import hashlib\n\n        h = hashlib.new(&quot;sha256&quot;)\n        with file_path.open(&quot;rb&quot;) as f:\n            for chunk in iter(lambda: f.read(8192), b&quot;&quot;):  # read in chunks\n                h.update(chunk)\n        assert h.hexdigest(), &quot;Checksum calculation failed.&quot;\n        return h.hexdigest()\n\n    def __repr__(self) -&gt; str:\n        &quot;&quot;&quot;Clean, informative representation showing type and processed URI&quot;&quot;&quot;\n        return f&quot;URI({self.sourcetype.value}: '{self.uri}')&quot;\n\n    def __str__(self) -&gt; str:\n        &quot;&quot;&quot;String representation is just the processed URI&quot;&quot;&quot;\n        return self.uri\n\n    def __hash__(self) -&gt; int:\n        &quot;&quot;&quot;Make URI hashable for caching purposes&quot;&quot;&quot;\n        return hash((self.uri, self.sourcetype))\n\n    def __eq__(self, other) -&gt; bool:\n        &quot;&quot;&quot;Define equality for hashing&quot;&quot;&quot;\n        if isinstance(other, URI):\n            return self.uri == other.uri and self.sourcetype == other.sourcetype\n        return False\n</file>\n    <file path=\"src/siphon/data/type-definitions/uri_schemes.py\">&quot;&quot;&quot;\nOur URI schemes for different types of content. I.e. &quot;text:///path/to/file.txt&quot;.\n&quot;&quot;&quot;\n\nURISchemes = {\n    &quot;Text&quot;: &quot;text&quot;,\n    &quot;Doc&quot;: &quot;doc&quot;,\n    &quot;Audio&quot;: &quot;audio&quot;,\n    &quot;Video&quot;: &quot;video&quot;,\n    &quot;Image&quot;: &quot;image&quot;,\n    &quot;GitHub&quot;: &quot;github&quot;,\n    &quot;Email&quot;: &quot;email&quot;,\n    &quot;YouTube&quot;: &quot;youtube&quot;,\n    &quot;Obsidian&quot;: &quot;obsidian&quot;,\n    &quot;Article&quot;: &quot;article&quot;,\n    &quot;Drive&quot;: &quot;drive&quot;,\n}\n</file>\n    <file path=\"src/siphon/data/type-definitions/extensions.py\">&quot;&quot;&quot;\nThis module defines various file extensions categorized into different types.\nThis routes incoming requests to the appropriate handler based on the file extension.\n&quot;&quot;&quot;\n\nExtensions = {\n    &quot;Text&quot;: [\n        &quot;.csv&quot;,\n        &quot;.json&quot;,\n        &quot;.xml&quot;,\n        &quot;.txt&quot;,\n        &quot;.md&quot;,\n        &quot;.yaml&quot;,\n        &quot;.yml&quot;,\n        &quot;.toml&quot;,\n        &quot;.ini&quot;,\n        &quot;.py&quot;,\n        &quot;.js&quot;,\n        &quot;.html&quot;,\n        &quot;.css&quot;,\n        &quot;.java&quot;,\n        &quot;.cpp&quot;,\n        &quot;.c&quot;,\n        &quot;.go&quot;,\n        &quot;.rs&quot;,\n        &quot;.rb&quot;,\n        &quot;.php&quot;,\n        &quot;.log&quot;,\n        &quot;.env&quot;,\n        &quot;.sh&quot;,\n    ],\n    &quot;Doc&quot;: [\n        &quot;.docx&quot;,\n        &quot;.pptx&quot;,\n        &quot;.xlsx&quot;,\n        &quot;.xls&quot;,\n        &quot;.pdf&quot;,\n        &quot;.msg&quot;,\n        &quot;.html&quot;,\n        &quot;.rtf&quot;,\n    ],\n    &quot;Audio&quot;: [\n        &quot;.wav&quot;,\n        &quot;.mp3&quot;,\n        &quot;.m4a&quot;,\n        &quot;.ogg&quot;,\n        &quot;.flac&quot;,\n    ],\n    &quot;Video&quot;: [\n        &quot;.mp4&quot;,\n        &quot;.avi&quot;,\n        &quot;.mov&quot;,\n        &quot;.webm&quot;,\n        &quot;.mkv&quot;,\n    ],\n    &quot;Image&quot;: [\n        &quot;.jpg&quot;,\n        &quot;.jpeg&quot;,\n        &quot;.png&quot;,\n        &quot;.gif&quot;,\n        &quot;.bmp&quot;,\n        &quot;.tiff&quot;,\n        &quot;.svg&quot;,\n        &quot;.webp&quot;,\n        &quot;.ico&quot;,\n    ],\n    # &quot;archive&quot;: [&quot;.zip&quot;, &quot;.tar&quot;, &quot;.gz&quot;, &quot;.rar&quot;, &quot;.7z&quot;],\n    # &quot;specialized&quot;: [&quot;.epub&quot;, &quot;.mobi&quot;],\n}\n</file>\n    <file path=\"src/siphon/data/type-definitions/source_type.py\">&quot;&quot;&quot;\nThis is our main enum for all source types, and the canonical SOT for source types supported by siphon.\n&quot;&quot;&quot;\n\nfrom enum import Enum\n\n\nclass SourceType(str, Enum):\n    # Annotations for source types\n    IMAGE = &quot;Image&quot;  # Image files like JPEG, PNG, etc.\n    AUDIO = &quot;Audio&quot;  # Audio files like MP3, WAV, etc.\n    VIDEO = &quot;Video&quot;  # Video files like MP4, AVI, etc.\n    TEXT = &quot;Text&quot;  # Text files like TXT, MD, etc.\n    DOC = &quot;Doc&quot;  # Document files like DOCX, PPTX, XLSX, PDF, etc.\n    ARTICLE = &quot;Article&quot;  # Online articles or blog posts\n    YOUTUBE = &quot;YouTube&quot;  # YouTube videos\n    GITHUB = &quot;GitHub&quot;  # GitHub repositories or files\n    OBSIDIAN = &quot;Obsidian&quot;  # Obsidian notes or vaults\n    DRIVE = &quot;Drive&quot;  # Google Drive files (Docs, Sheets, etc.)\n    EMAIL = &quot;Email&quot;  # Email messages or threads\n\n\n# Park future implementation ideas here\n&quot;&quot;&quot;\nBOOK (for example those oreilly books I bought)\nEPHEMERAL (stdin, clipboard, and other temporary sources like web scraping)\nARXIV\nTODO\nTEXTMESSAGES\nCALENDAR\nPODCAST (maybe)\nRSS (the first &quot;self-updating&quot; source type)\nREDDIT (scrape an entire subreddit)\n&quot;&quot;&quot;\n</file>\n    <file path=\"src/siphon/data/type_definitions/uri_schemes.py\">&quot;&quot;&quot;\nOur URI schemes for different type_definitions of content. I.e. &quot;text:///path/to/file.txt&quot;.\n&quot;&quot;&quot;\n\nURISchemes = {\n    &quot;Text&quot;: &quot;text&quot;,\n    &quot;Doc&quot;: &quot;doc&quot;,\n    &quot;Audio&quot;: &quot;audio&quot;,\n    &quot;Video&quot;: &quot;video&quot;,\n    &quot;Image&quot;: &quot;image&quot;,\n    &quot;GitHub&quot;: &quot;github&quot;,\n    &quot;Email&quot;: &quot;email&quot;,\n    &quot;YouTube&quot;: &quot;youtube&quot;,\n    &quot;Obsidian&quot;: &quot;obsidian&quot;,\n    &quot;Article&quot;: &quot;article&quot;,\n    &quot;Drive&quot;: &quot;drive&quot;,\n}\n</file>\n    <file path=\"src/siphon/data/type_definitions/extensions.py\">&quot;&quot;&quot;\nThis module defines various file extensions categorized into different type_definitions.\nThis routes incoming requests to the appropriate handler based on the file extension.\n&quot;&quot;&quot;\n\nExtensions = {\n    &quot;Text&quot;: [\n        &quot;.csv&quot;,\n        &quot;.json&quot;,\n        &quot;.xml&quot;,\n        &quot;.txt&quot;,\n        &quot;.md&quot;,\n        &quot;.yaml&quot;,\n        &quot;.yml&quot;,\n        &quot;.toml&quot;,\n        &quot;.ini&quot;,\n        &quot;.py&quot;,\n        &quot;.js&quot;,\n        &quot;.html&quot;,\n        &quot;.css&quot;,\n        &quot;.java&quot;,\n        &quot;.cpp&quot;,\n        &quot;.c&quot;,\n        &quot;.go&quot;,\n        &quot;.rs&quot;,\n        &quot;.rb&quot;,\n        &quot;.php&quot;,\n        &quot;.log&quot;,\n        &quot;.env&quot;,\n        &quot;.sh&quot;,\n    ],\n    &quot;Doc&quot;: [\n        &quot;.docx&quot;,\n        &quot;.pptx&quot;,\n        &quot;.xlsx&quot;,\n        &quot;.xls&quot;,\n        &quot;.pdf&quot;,\n        &quot;.msg&quot;,\n        &quot;.html&quot;,\n        &quot;.rtf&quot;,\n    ],\n    &quot;Audio&quot;: [\n        &quot;.wav&quot;,\n        &quot;.mp3&quot;,\n        &quot;.m4a&quot;,\n        &quot;.ogg&quot;,\n        &quot;.flac&quot;,\n    ],\n    &quot;Video&quot;: [\n        &quot;.mp4&quot;,\n        &quot;.avi&quot;,\n        &quot;.mov&quot;,\n        &quot;.webm&quot;,\n        &quot;.mkv&quot;,\n    ],\n    &quot;Image&quot;: [\n        &quot;.jpg&quot;,\n        &quot;.jpeg&quot;,\n        &quot;.png&quot;,\n        &quot;.gif&quot;,\n        &quot;.bmp&quot;,\n        &quot;.tiff&quot;,\n        &quot;.svg&quot;,\n        &quot;.webp&quot;,\n        &quot;.ico&quot;,\n    ],\n    # &quot;archive&quot;: [&quot;.zip&quot;, &quot;.tar&quot;, &quot;.gz&quot;, &quot;.rar&quot;, &quot;.7z&quot;],\n    # &quot;specialized&quot;: [&quot;.epub&quot;, &quot;.mobi&quot;],\n}\n</file>\n    <file path=\"src/siphon/data/type_definitions/source_type.py\">&quot;&quot;&quot;\nThis is our main enum for all source type_definitions, and the canonical SOT for source type_definitions supported by siphon.\n&quot;&quot;&quot;\n\nfrom enum import Enum\n\n\nclass SourceType(str, Enum):\n    # Annotations for source type_definitions\n    IMAGE = &quot;Image&quot;  # Image files like JPEG, PNG, etc.\n    AUDIO = &quot;Audio&quot;  # Audio files like MP3, WAV, etc.\n    VIDEO = &quot;Video&quot;  # Video files like MP4, AVI, etc.\n    TEXT = &quot;Text&quot;  # Text files like TXT, MD, etc.\n    DOC = &quot;Doc&quot;  # Document files like DOCX, PPTX, XLSX, PDF, etc.\n    ARTICLE = &quot;Article&quot;  # Online articles or blog posts\n    YOUTUBE = &quot;YouTube&quot;  # YouTube videos\n    GITHUB = &quot;GitHub&quot;  # GitHub repositories or files\n    OBSIDIAN = &quot;Obsidian&quot;  # Obsidian notes or vaults\n    DRIVE = &quot;Drive&quot;  # Google Drive files (Docs, Sheets, etc.)\n    EMAIL = &quot;Email&quot;  # Email messages or threads\n\n\n# Park future implementation ideas here\n&quot;&quot;&quot;\nBOOK (for example those oreilly books I bought)\nEPHEMERAL (stdin, clipboard, and other temporary sources like web scraping)\nARXIV\nTODO\nTEXTMESSAGES\nCALENDAR\nPODCAST (maybe)\nRSS (the first &quot;self-updating&quot; source type)\nREDDIT (scrape an entire subreddit)\n&quot;&quot;&quot;\n</file>\n    <file path=\"src/siphon/context/context_classes.py\">import importlib\nfrom siphon.data.context import Context\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom typing import Union\n\n\ndef load_context_class(sourcetype: SourceType) -&gt; type[Context]:\n    &quot;&quot;&quot;\n    Dynamically import the Context class corresponding to a SourceType.\n    Assumes:\n      - module path: siphon.context.classes.{sourcetype.name.lower()}_context\n      - class name: {sourcetype.value}Context\n    &quot;&quot;&quot;\n    module_name = f&quot;siphon.context.classes.{sourcetype.name.lower()}_context&quot;\n    class_name = f&quot;{sourcetype.value}Context&quot;\n\n    try:\n        module = importlib.import_module(module_name)\n    except ModuleNotFoundError as e:\n        raise ImportError(\n            f&quot;Missing module for SourceType.{sourcetype.name}.\\n&quot;\n            f&quot;Expected file: `{module_name.replace('.', '/')}.py`&quot;\n        ) from e\n\n    try:\n        context_class = getattr(module, class_name)\n    except AttributeError as e:\n        raise ImportError(\n            f&quot;Module `{module_name}` exists but does not define `{class_name}`.\\n&quot;\n            f&quot;Please ensure the class name matches and is defined.&quot;\n        ) from e\n\n    if not issubclass(context_class, Context):\n        raise TypeError(f&quot;{class_name} exists but does not inherit from `URI`.&quot;)\n\n    return context_class\n\n\n# Build the list of valid URI subclasses\nContextClasses = [load_context_class(st) for st in SourceType]\n\n# A union type for all context classes\nContextUnion = Union[tuple(ContextClasses)]\n</file>\n    <file path=\"src/siphon/context/classes/image_context.py\">from siphon.context.classes.text_context import TextContext\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.uri import URI\nfrom typing import override, Literal\n\n\nclass ImageContext(TextContext):\n    &quot;&quot;&quot;\n    Context class for handling image files.\n    Inherits from TextContext to provide common functionality; from_uri, _validate_uri, and _get_metadata work under the hood.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.IMAGE\n\n    @override\n    @classmethod\n    def _get_context(cls, uri: URI, model: Literal[&quot;local&quot;, &quot;cloud&quot;] = &quot;cloud&quot;) -&gt; str:\n        &quot;&quot;&quot;\n        Get the context from the image file.\n        This method is overridden to provide specific functionality for image files.\n        &quot;&quot;&quot;\n        _ = model\n\n        from siphon.ingestion.image.retrieve_image import retrieve_image\n\n        image_description = retrieve_image(uri.source, model=model)\n        return image_description\n</file>\n    <file path=\"src/siphon/context/classes/video_context.py\">from siphon.context.classes.text_context import TextContext\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.uri import URI\nfrom typing import override\n\n\nclass VideoContext(TextContext):\n    &quot;&quot;&quot;\n    Context class for handling video files.\n    Inherits from TextContext to provide common functionality; from_uri, _validate_uri, and _get_metadata work under the hood.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.VIDEO\n\n    @override\n    @classmethod\n    def _get_context(cls, uri: URI, model: str = &quot;cloud&quot;) -&gt; str:\n        &quot;&quot;&quot;\n        Get the context from the video file.\n        This method is overridden to provide specific functionality for video files.\n        &quot;&quot;&quot;\n        _, _ = uri, model\n        raise NotImplementedError(&quot;VideoContext._get_context is not implemented yet&quot;)\n</file>\n    <file path=\"src/siphon/context/classes/youtube_context.py\">from siphon.context.classes.article_context import ArticleContext\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.uri import URI\nfrom typing import override, Optional\n\n\nclass YouTubeContext(ArticleContext):\n    sourcetype: SourceType = SourceType.YOUTUBE\n\n    # Youtube specific metadata fields\n    video_id: Optional[str]\n    channel: Optional[str]\n    duration: Optional[int]\n    view_count: Optional[int]\n    description: Optional[str]\n    tags: Optional[list[str]]\n    like_count: Optional[int]\n    comment_count: Optional[int]\n\n    @override\n    @classmethod\n    def _get_context(cls, uri: URI) -&gt; tuple[str, dict]:\n        &quot;&quot;&quot;\n        Get the text content + metadata from the YouTube video.\n        &quot;&quot;&quot;\n        from siphon.ingestion.youtube.retrieve_youtube import retrieve_youtube\n\n        llm_context, metadata = retrieve_youtube(uri.source)\n        return llm_context, metadata\n</file>\n    <file path=\"src/siphon/context/classes/github_context.py\">from siphon.context.classes.article_context import ArticleContext\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.uri import URI\nfrom typing import override, Optional\n\n\nclass GitHubContext(ArticleContext):\n    &quot;&quot;&quot;\n    Context class for handling GitHub articles.\n    Inherits from ArticleContext to handle GitHub-specific content.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.GITHUB\n\n    # GitHub specific metadata fields\n    stars: int\n    language: str\n    topics: list[str]\n    description: Optional[str]\n    updated_at: int\n    pushed_at: int\n    size: int\n\n    @override\n    @classmethod\n    def _get_context(cls, uri: URI) -&gt; tuple[str, dict]:\n        &quot;&quot;&quot;\n        Get the text content + metadata from the GitHub article.\n        &quot;&quot;&quot;\n        from siphon.ingestion.github.retrieve_github import retrieve_github\n\n        llm_context, metadata = retrieve_github(uri.source)\n        return llm_context, metadata\n</file>\n    <file path=\"src/siphon/context/classes/obsidian_context.py\">from siphon.data.context import Context\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.uri import URI\nfrom typing import override\n\n\nclass ObsidianContext(Context):\n    sourcetype: SourceType = SourceType.OBSIDIAN\n\n    @override\n    @classmethod\n    def from_uri(cls, uri: &quot;ObsidianURI&quot;) -&gt; &quot;ObsidianContext&quot;:  # type: ignore\n        &quot;&quot;&quot;\n        Create an ObsidianContext from a URI.\n        &quot;&quot;&quot;\n        from siphon.uri.classes.obsidian_uri import ObsidianURI\n\n        if not isinstance(uri, ObsidianURI):\n            raise TypeError(&quot;Expected uri to be an instance of ObsidianURI.&quot;)\n        raise NotImplementedError(&quot;ObsidianContext.from_uri is not implemented yet&quot;)\n</file>\n    <file path=\"src/siphon/context/classes/audio_context.py\">from siphon.context.classes.text_context import TextContext\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.uri import URI\nfrom siphon.logs.logging_config import get_logger\nfrom typing import override, Literal\n\nlogger = get_logger(__name__)\n\n\nclass AudioContext(TextContext):\n    &quot;&quot;&quot;\n    Context class for handling audio files.\n    Inherits from TextContext to provide common functionality; from_uri, _validate_uri, and _get_metadata work under the hood.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.AUDIO\n\n    @override\n    @classmethod\n    def _get_context(cls, uri: URI, model: Literal[&quot;local&quot;, &quot;cloud&quot;]) -&gt; str:\n        from pathlib import Path\n\n        audio_file = Path(uri.source)\n        from siphon.ingestion.audio.retrieve_audio import retrieve_audio\n\n        logger.info(&quot;Starting transcription process.&quot;)\n        llm_context = retrieve_audio(audio_file, model=model)\n        return llm_context\n</file>\n    <file path=\"src/siphon/context/classes/doc_context.py\">from siphon.context.classes.text_context import TextContext\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.uri import URI\nfrom typing import override, Literal\n\n\nclass DocContext(TextContext):\n    &quot;&quot;&quot;\n    Context class for handling document files (e.g., .doc, .docx).\n    Validation, and metadata inherit from TextContext.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.DOC\n\n    @override\n    @classmethod\n    def _get_context(cls, uri: URI, model: Literal[&quot;local&quot;, &quot;cloud&quot;]) -&gt; str:  # type: ignore\n        _ = model\n\n        from markitdown import MarkItDown\n        from pathlib import Path\n\n        file_path = Path(uri.source)\n\n        md = MarkItDown()\n        llm_context = md.convert(file_path)\n        return llm_context.text_content\n</file>\n    <file path=\"src/siphon/context/classes/text_context.py\">from siphon.data.context import Context\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.uri import URI\nfrom pathlib import Path\nfrom typing import override, Optional, Literal\n\n\nclass TextContext(Context):\n    &quot;&quot;&quot;\n    Context class for handling text files.\n    This is also the base class for audio, image, video, and obsidian contexts.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.TEXT\n\n    # Metadata field\n    file_path: str | Path\n    file_size: int\n    mime_type: str\n    file_extension: str\n    content_created_at: Optional[int]\n    content_modified_at: Optional[int]\n\n    @override\n    @classmethod\n    def from_uri(cls, uri: URI, model: Literal[&quot;local&quot;, &quot;cloud&quot;] = &quot;cloud&quot;) -&gt; Context:  # type: ignore\n        &quot;&quot;&quot;\n        Create a Context from a URI.\n        &quot;&quot;&quot;\n        if not cls._validate_uri(uri):\n            raise ValueError(&quot;Invalid URI provided.&quot;)\n\n        # Get metadata from the URI\n        metadata = cls._get_metadata(uri)\n\n        # Get context from the file\n        context = cls._get_context(uri, model=model)\n\n        # Assemble and return the Context instance\n        return cls(context=context, **metadata)\n\n    @classmethod\n    def _validate_uri(cls, uri: URI) -&gt; bool:\n        &quot;&quot;&quot;\n        Validate that we received the correct subclass of URI and that the file exists.\n        Inheritable by other file-based contexts like AudioContext, ImageContext, etc.\n        &quot;&quot;&quot;\n        # Assert that uri class name matches the expected format\n        sourcetype_value = uri.sourcetype.value\n        if uri.__class__.__name__ != f&quot;{sourcetype_value}URI&quot;:\n            raise TypeError(\n                f&quot;Expected uri to be an instance of {sourcetype_value}URI, &quot;\n                f&quot;but got {uri.__class__.__name__}.&quot;\n            )\n\n        from pathlib import Path\n\n        if not uri.source:\n            raise ValueError(&quot;URI source cannot be empty.&quot;)\n        if not isinstance(uri.source, str) or isinstance(uri.source, Path):\n            raise TypeError(&quot;URI source must be a string or Path object.&quot;)\n\n        path = Path(uri.source)\n        if not path.is_file():\n            raise FileNotFoundError(f&quot;The file {path} does not exist.&quot;)\n\n        from siphon.data.type_definitions.extensions import Extensions\n\n        if path.suffix not in Extensions[sourcetype_value]:\n            raise ValueError(f&quot;Unsupported file type: {path.suffix}.&quot;)\n        return True\n\n    @classmethod\n    def _get_context(cls, uri: URI, model: Literal[&quot;local&quot;, &quot;cloud&quot;]) -&gt; str:\n        &quot;&quot;&quot;\n        Get the text content from the file.\n        The most customized method for each context type.\n        Model is available for audio, image, and video contexts.\n        &quot;&quot;&quot;\n        _ = model\n\n        from pathlib import Path\n\n        path = Path(uri.source)\n        if not path.is_file():\n            raise FileNotFoundError(f&quot;The file {path} does not exist.&quot;)\n        with open(path, &quot;r&quot;) as file:\n            return file.read()\n\n    @classmethod\n    def _get_metadata(cls, uri: URI) -&gt; dict:\n        &quot;&quot;&quot;\n        Get metadata for the text file.\n        Inheritable by other file-based contexts like AudioContext, ImageContext, etc.\n        &quot;&quot;&quot;\n        from pathlib import Path\n        import mimetypes\n\n        path = Path(uri.source)\n        return {\n            &quot;file_path&quot;: str(path.resolve()),\n            &quot;file_size&quot;: path.stat().st_size,\n            &quot;mime_type&quot;: mimetypes.guess_type(path)[0] or &quot;text/plain&quot;,\n            &quot;file_extension&quot;: path.suffix,\n            &quot;content_created_at&quot;: int(path.stat().st_ctime),\n            &quot;content_modified_at&quot;: int(path.stat().st_mtime),\n        }\n</file>\n    <file path=\"src/siphon/context/classes/email_context.py\">from siphon.data.context import Context\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.uri import URI\nfrom typing import override\n\n\nclass EmailContext(Context):\n    sourcetype: SourceType = SourceType.EMAIL\n\n    @override\n    @classmethod\n    def from_uri(cls, uri: &quot;EmailURI&quot;) -&gt; &quot;EmailContext&quot;:  # type: ignore\n        &quot;&quot;&quot;\n        Create an EmailContext from a URI.\n        &quot;&quot;&quot;\n        from siphon.uri.classes.email_uri import EmailURI\n\n        if not isinstance(uri, EmailURI):\n            raise TypeError(&quot;Expected uri to be an instance of EmailURI.&quot;)\n\n        raise NotImplementedError(\n            &quot;EmailContext.from_uri is not implemented. EmailContext is not supported yet.&quot;\n        )\n</file>\n    <file path=\"src/siphon/context/classes/drive_context.py\">from siphon.data.context import Context\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.uri import URI\nfrom typing import override\n\n\nclass DriveContext(Context):\n    sourcetype: SourceType = SourceType.DRIVE\n\n    @override\n    @classmethod\n    def from_uri(cls, uri: &quot;DriveURI&quot;) -&gt; &quot;DriveContext&quot;:  # type: ignore\n        &quot;&quot;&quot;\n        Create a DriveContext from a URI.\n        &quot;&quot;&quot;\n        from siphon.uri.classes.drive_uri import DriveURI\n\n        if not isinstance(uri, DriveURI):\n            raise TypeError(&quot;Expected uri to be an instance of DriveURI.&quot;)\n\n        raise NotImplementedError(&quot;DriveContext.from_uri is not implemented yet&quot;)\n</file>\n    <file path=\"src/siphon/context/classes/article_context.py\">from siphon.data.context import Context\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.uri import URI\nfrom typing import override, Optional\nimport re\n\nurl_pattern = re.compile(r&quot;^(https?|ftp)://[^\\s/$.?#].[^\\s]*$&quot;)\n\n\nclass ArticleContext(Context):\n    &quot;&quot;&quot;\n    Context class for handling articles.\n    This is also the base class for other online content contexts (YouTube, GitHub, etc.).\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.ARTICLE\n\n    # Metadata fields\n    url: Optional[str] = None\n    domain: Optional[str] = None\n    title: Optional[str] = None\n    published_date: Optional[int] = None\n\n    @override\n    @classmethod\n    def from_uri(cls, uri: URI) -&gt; Context:  # type: ignore\n        &quot;&quot;&quot;\n        Create a ArticleContext from a URI.\n        This is also the base class for other online content contexts.\n        Unlike our file-based contexts, get_context returns both the context and metadata.\n        This is because api calls provide both the context and metadata in one go.\n        &quot;&quot;&quot;\n        if not cls._validate_uri(uri):\n            raise ValueError(&quot;Invalid URI provided.&quot;)\n\n        # Get context + metadata from the URI\n        context, metadata = cls._get_context(uri)\n\n        # Assemble and return the Context instance\n        return cls(context=context, **metadata)\n\n    @classmethod\n    def _validate_uri(cls, uri: URI) -&gt; bool:\n        &quot;&quot;&quot;\n        Validate that we received the correct subclass of URI and that the file exists.\n        Inheritable by other file-based contexts like AudioContext, ImageContext, etc.\n        &quot;&quot;&quot;\n        if not uri.source:\n            raise ValueError(&quot;URI source cannot be empty.&quot;)\n        if not isinstance(uri.source, str):\n            raise TypeError(&quot;URI source must be a string object.&quot;)\n\n        # Assert that uri class name matches the expected format\n        sourcetype_value = uri.sourcetype.value\n        if uri.__class__.__name__ != f&quot;{sourcetype_value}URI&quot;:\n            raise TypeError(\n                f&quot;Expected uri to be an instance of {sourcetype_value}URI, &quot;\n                f&quot;but got {uri.__class__.__name__}.&quot;\n            )\n\n        # Validate that source is a proper URL\n        if not url_pattern.match(uri.source):\n            raise ValueError(&quot;URI source must be a valid URL.&quot;)\n\n        return True\n\n    @classmethod\n    def _get_context(cls, uri: URI) -&gt; tuple[str, dict]:\n        &quot;&quot;&quot;\n        Get the text content + metadata from the article.\n        The most customized method for each context type (GitHub, YouTube, Drive, etc.)\n        &quot;&quot;&quot;\n\n        from siphon.ingestion.article.retrieve_article import retrieve_article\n\n        article_obj = retrieve_article(uri.source)\n        # Get context\n        context = article_obj.text\n        # Get metadata\n        metadata = {\n            &quot;url&quot;: article_obj.source_url,\n            &quot;domain&quot;: article_obj.source_url.split(&quot;/&quot;)[2],\n            &quot;title&quot;: article_obj.title,\n            &quot;published_date&quot;: int(article_obj.publish_date.timestamp())\n            if article_obj.publish_date\n            else None,  # type: ignore\n        }\n        return context, metadata\n</file>\n    <file path=\"src/siphon/main/siphon.py\">&quot;&quot;&quot;\nUpdated main/siphon.py with PostgreSQL cache integration\n&quot;&quot;&quot;\n\nfrom siphon.data.uri import URI\nfrom siphon.data.context import Context\nfrom siphon.data.synthetic_data import SyntheticData\nfrom siphon.cli.cli_params import CLIParams\nfrom siphon.data.processed_content import ProcessedContent\nfrom siphon.database.postgres.PGRES_processed_content import (\n    get_cached_content,\n    cache_processed_content,\n)\nfrom siphon.logs.logging_config import get_logger\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from conduit.message.imagemessage import ImageMessage\n\nlogger = get_logger(__name__)\n\n\ndef siphon(cli_params: CLIParams | Path | str) -&gt; ProcessedContent:\n    &quot;&quot;&quot;\n    Siphon orchestrates the process of converting a source string (file path or URL).\n    Now includes PostgreSQL caching for performance.\n    &quot;&quot;&quot;\n    cloud = cli_params.cloud if isinstance(cli_params, CLIParams) else False\n    # Coerce Path to string\n    if isinstance(cli_params, Path):\n        source = str(cli_params)\n        cache_options = &quot;c&quot;\n        tags = []\n    # Grab local flag -- do we use cloud or SiphonServer?\n    # Validate input\n    elif isinstance(cli_params, str):\n        source = cli_params\n        cache_options = &quot;c&quot;  # Default, cache it.\n        tags = []\n    elif isinstance(cli_params, CLIParams):\n        source = cli_params.source\n        cache_options = cli_params.cache_options\n        tags = cli_params.tags\n        cloud = cli_params.cloud\n    else:\n        raise TypeError(\n            f&quot;Expected a string or CLIParams object, got: {cli_params.__class__.__name__}&quot;\n        )\n\n    # 1. Parse source into structured URI\n    uri = URI.from_source(source)\n    if not uri:\n        raise ValueError(f&quot;Invalid source: {source}. Must be a valid file path or URL.&quot;)\n\n    if cache_options == &quot;c&quot;:\n        logger.info(f&quot;Checking cache for URI: {uri.uri}&quot;)\n        try:\n            cached_content = get_cached_content(uri.uri)\n            if cached_content:\n                logger.info(&quot;Cache HIT! Returning cached content&quot;)\n                return cached_content\n            else:\n                logger.info(&quot;Cache MISS - no content found&quot;)\n        except Exception as e:\n            logger.warning(f&quot;Cache lookup failed: {e}&quot;)\n\n    # 3. Generate LLM context from the URI (retrieving text content)\n    logger.info(&quot;Generating context from URI...&quot;)\n    context = Context.from_uri(uri)\n\n    # 4. Generate SyntheticData (post-processing)\n    logger.info(f&quot;Generating synthetic data from context...&quot;)\n    synthetic_data = SyntheticData.from_context(context, cloud=cloud)\n    if not synthetic_data:\n        raise ValueError(&quot;Failed to generate synthetic data from context.&quot;)\n    logger.info(&quot;Synthetic data generation complete.&quot;)\n\n    # 5. Construct ProcessedContent object\n    processed_content = ProcessedContent(\n        uri=uri,\n        llm_context=context,\n        synthetic_data=synthetic_data,\n        tags=tags or [],\n    )\n\n    if cache_options in [&quot;c&quot;, &quot;r&quot;]:\n        try:\n            logger.info(f&quot;Attempting to cache content for URI: {uri.uri}&quot;)\n            result = cache_processed_content(processed_content)\n            logger.info(f&quot;Successfully cached with key: {result}&quot;)\n        except Exception as e:\n            logger.warning(f&quot;Cache save failed: {e}&quot;)\n\n    # 7. Return the processed content\n    return processed_content\n</file>\n    <file path=\"src/siphon/ingestion/online_context.py\">&quot;&quot;&quot;\nThis is adapted from leviathan main script. Within Siphon, this routes a url (passed with -u to Siphon) to either YouTube or an online article.\n&quot;&quot;&quot;\n\nfrom siphon.ingestion.youtube.retrieve_youtube import retrieve_youtube\nfrom siphon.ingestion.github.retrieve_github import retrieve_github\nfrom siphon.ingestion.googledrive.retrieve_google_doc import retrieve_google_doc\nfrom siphon.ingestion.article.retrieve_article import retrieve_article\n\n\ndef categorize_url(url: str) -&gt; str:\n    &quot;&quot;&quot;\n    Determine the type of URL.\n    &quot;&quot;&quot;\n    if &quot;youtube&quot; in url:\n        return &quot;youtube&quot;\n    if &quot;github&quot; in url:\n        return &quot;github&quot;\n    if &quot;docs.google.com&quot; in url:\n        return &quot;drive&quot;\n    if &quot;arxib&quot; in url:\n        return &quot;arxiv&quot;\n    elif &quot;http&quot; in url:\n        return &quot;article&quot;\n    else:\n        raise ValueError(&quot;Input must be a YouTube URL or an article URL.&quot;)\n\n\ndef retrieve_online_context(url: str) -&gt; str:\n    &quot;&quot;&quot;\n    This function takes a URL and returns the text.\n    &quot;&quot;&quot;\n    mode = categorize_url(url)\n    match mode:\n        case &quot;youtube&quot;:\n            return retrieve_youtube(url)\n        case &quot;github&quot;:\n            return retrieve_github(url)\n        case &quot;drive&quot;:\n            return retrieve_google_doc(url)\n        case &quot;article&quot;:\n            return retrieve_article(url)\n        case &quot;arxiv&quot;:\n            raise NotImplementedError(&quot;Arxiv not implemented yet.&quot;)\n        case _:\n            raise ValueError(f&quot;Unsupported URL type: {mode}&quot;)\n</file>\n    <file path=\"src/siphon/ingestion/file_context.py\">from siphon.data.type_definitions.extensions import Extensions\nfrom pathlib import Path\n\n\ndir_path = Path(__file__).parent\nasset_dir = dir_path / &quot;assets&quot;\nasset_files = list(asset_dir.glob(&quot;*.*&quot;))\n\n\n# Our functions\ndef route_file(file_path: Path):\n    &quot;&quot;&quot;Route the file to the appropriate handler based on its extension.&quot;&quot;&quot;\n    ext = file_path.suffix.lower()\n    for category, exts in Extensions.items():\n        if ext in exts:\n            return category\n    return &quot;unknown&quot;\n\n\ndef convert_markitdown(file_path: Path):\n    &quot;&quot;&quot;Convert a file using MarkItDown.&quot;&quot;&quot;\n    from markitdown import MarkItDown\n\n    if not file_path.exists():\n        raise FileNotFoundError(f&quot;File not found: {file_path}&quot;)\n    if not file_path.suffix.lower() in Extensions[&quot;markitdown&quot;]:\n        raise ValueError(f&quot;File type not supported for MarkItDown: {file_path.suffix}&quot;)\n    # Do the conversion\n    md = MarkItDown()\n    return md.convert(file_path)\n\n\ndef convert_raw(file_path: Path):\n    &quot;&quot;&quot;Convert raw files (CSV, JSON, etc.) to text.&quot;&quot;&quot;\n    if not file_path.exists():\n        raise FileNotFoundError(f&quot;File not found: {file_path}&quot;)\n    if not file_path.suffix.lower() in Extensions[&quot;raw&quot;]:\n        raise ValueError(\n            f&quot;File type not supported for raw conversion: {file_path.suffix}&quot;\n        )\n    # Implement raw conversion logic here\n    with open(file_path, &quot;r&quot;) as f:\n        return f.read()\n\n\ndef convert_code(file_path: Path):\n    &quot;&quot;&quot;Convert code files to text.&quot;&quot;&quot;\n    if not file_path.exists():\n        raise FileNotFoundError(f&quot;File not found: {file_path}&quot;)\n    if not file_path.suffix.lower() in Extensions[&quot;code&quot;]:\n        raise ValueError(\n            f&quot;File type not supported for code conversion: {file_path.suffix}&quot;\n        )\n    # Implement code conversion logic here\n    with open(file_path, &quot;r&quot;) as f:\n        return f.read()\n\n\ndef convert_audio(file_path: Path):\n    &quot;&quot;&quot;Convert audio/video files using Whisper.&quot;&quot;&quot;\n    from siphon.ingestion.audio.retrieve_audio import retrieve_audio\n\n    if not file_path.exists():\n        raise FileNotFoundError(f&quot;File not found: {file_path}&quot;)\n    if not file_path.suffix.lower() in Extensions[&quot;audio&quot;]:\n        raise ValueError(\n            f&quot;File type not supported for Whisper conversion: {file_path.suffix}&quot;\n        )\n    output = retrieve_audio(file_path)\n    return output\n\n\ndef convert_video(file_path: Path):\n    &quot;&quot;&quot;Convert video files to text.&quot;&quot;&quot;\n    if not file_path.exists():\n        raise FileNotFoundError(f&quot;File not found: {file_path}&quot;)\n    if not file_path.suffix.lower() in Extensions[&quot;video&quot;]:\n        raise ValueError(\n            f&quot;File type not supported for video conversion: {file_path.suffix}&quot;\n        )\n    # Implement video conversion logic here\n    # Placeholder for actual video conversion implementation\n    raise NotImplementedError(&quot;Video conversion not implemented yet.&quot;)\n\n\ndef convert_image(file_path: Path):\n    &quot;&quot;&quot;Convert image files using OCR.&quot;&quot;&quot;\n    if not file_path.exists():\n        raise FileNotFoundError(f&quot;File not found: {file_path}&quot;)\n    if not file_path.suffix.lower() in Extensions[&quot;image&quot;]:\n        raise ValueError(\n            f&quot;File type not supported for OCR conversion: {file_path.suffix}&quot;\n        )\n    from siphon.ingestion.image.retrieve_image import retrieve_image\n\n    output = retrieve_image(file_path)\n    return output\n\n\ndef convert_archive(file_path: Path):\n    &quot;&quot;&quot;Extract and convert archive files.&quot;&quot;&quot;\n    if not file_path.exists():\n        raise FileNotFoundError(f&quot;File not found: {file_path}&quot;)\n    if not file_path.suffix.lower() in Extensions[&quot;archive&quot;]:\n        raise ValueError(\n            f&quot;File type not supported for archive conversion: {file_path.suffix}&quot;\n        )\n    # Implement archive extraction logic here\n    # Placeholder for actual archive extraction implementation\n    raise NotImplementedError(&quot;Archive extraction not implemented yet.&quot;)\n\n\ndef convert_specialized(file_path: Path):\n    &quot;&quot;&quot;Convert specialized files (e.g., ePub, mobi).&quot;&quot;&quot;\n    if not file_path.exists():\n        raise FileNotFoundError(f&quot;File not found: {file_path}&quot;)\n    if not file_path.suffix.lower() in Extensions[&quot;specialized&quot;]:\n        raise ValueError(\n            f&quot;File type not supported for specialized conversion: {file_path.suffix}&quot;\n        )\n    # Implement specialized file conversion logic here\n    # Placeholder for actual specialized file conversion implementation\n    raise NotImplementedError(&quot;Specialized file conversion not implemented yet.&quot;)\n\n\ndef retrieve_file_context(file_path: Path) -&gt; str:\n    category = route_file(file_path)\n    &quot;&quot;&quot;Convert a file based on its type.&quot;&quot;&quot;\n    output = &quot;&quot;\n    match category:\n        case &quot;markitdown&quot;:\n            output = convert_markitdown(file_path)\n        case &quot;raw&quot;:\n            output = convert_raw(file_path)\n        case &quot;code&quot;:\n            output = convert_code(file_path)\n        case &quot;audio&quot;:\n            output = convert_audio(file_path)\n        case &quot;video&quot;:\n            output = convert_video(file_path)\n        case &quot;image&quot;:\n            output = convert_image(file_path)\n        case &quot;archive&quot;:\n            output = convert_archive(file_path)\n        case &quot;specialized&quot;:\n            output = convert_specialized(file_path)\n        case &quot;unknown&quot;:\n            raise ValueError(f&quot;Unknown file type for: {file_path}&quot;)\n        case _:\n            raise ValueError(f&quot;Unsupported file type: {file_path.suffix}&quot;)\n\n    if output:\n        return output\n    else:\n        raise ValueError(f&quot;Failed to convert file: {file_path}&quot;)\n</file>\n    <file path=\"src/siphon/ingestion/retrieve.py\">&quot;&quot;&quot;\nHigh level wrapper for retrieving LLM context from various sources.\n&quot;&quot;&quot;\n\nfrom siphon.ingestion.file_context import retrieve_file_context\nfrom siphon.ingestion.online_context import retrieve_online_context\nfrom siphon.data.uri import URI\nfrom pathlib import Path\n\n\ndef retrieve_llm_context(uri: URI) -&gt; str:\n    &quot;&quot;&quot;\n    Generate llm_context from a file or online resource.\n    Accepts a URI object, returns a rich string representation of the context.\n    &quot;&quot;&quot;\n    if uri.source_type in [&quot;ARTICLE&quot;, &quot;YOUTUBE&quot;, &quot;DRIVE&quot;, &quot;GITHUB&quot;]:\n        return retrieve_online_context(uri.source)\n    elif uri.source_type in [&quot;OBSIDIAN&quot;, &quot;FILE&quot;]:\n        file_path = Path(uri.source)\n        return retrieve_file_context(file_path)\n    elif uri.source_type == &quot;email&quot;:\n        raise NotImplementedError(&quot;Email source type is not yet implemented.&quot;)\n    else:\n        raise ValueError(\n            f&quot;Unsupported source type: {uri.source_type}. Supported types are: article, youtube, drive, github, obsidian, file.&quot;\n        )\n</file>\n    <file path=\"src/siphon/ingestion/obsidian/vault.py\">&quot;&quot;&quot;\nSimple logic for identifying the Obsidian vault path from an environment variable.\nWill also add host-based logic since this is only local to Petrosian host, not Caruana or AlphaBlue.\n&quot;&quot;&quot;\nfrom pathlib import Path\nimport os\n\nvault = os.getenv(&quot;OBSIDIAN_VAULT&quot;)\nvault = Path(vault)\nif not vault:\n    raise ValueError(&quot;OBSIDIAN_VAULT environment variable is not set.&quot;)\n\nprint(vault)\n</file>\n    <file path=\"src/siphon/ingestion/article/retrieve_article.py\">from siphon.ingestion.article.download_article import download_article\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from newspaper import Article\n\n\ndef retrieve_article(url: str) -&gt; &quot;Article&quot;:\n    &quot;&quot;&quot;\n    Retrieve the article from a given URL.\n    Note: this is an Article object; Article.text will give you the text of the article.\n    &quot;&quot;&quot;\n    article = download_article(url)\n    return article\n</file>\n    <file path=\"src/siphon/ingestion/article/download_article.py\">&quot;&quot;&quot;\nThis module scrapes articles from the web.\n&quot;&quot;&quot;\n\nimport sys\nfrom newspaper import Article\n\nheaders = {\n    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&quot;\n}\n\n\ndef is_url(url):\n    if url.startswith(&quot;http&quot;) and len(url) &gt; 10:\n        return &quot;URL&quot;\n    elif url.startswith(&quot;[&quot;) and url.endswith(&quot;)&quot;):\n        return &quot;MARKDOWN_LINK&quot;\n    else:\n        return &quot;NOT_A_URL&quot;\n\n\ndef validate_url(url):\n    status = is_url(url)\n    match status:\n        case &quot;URL&quot;:\n            return url\n        case &quot;MARKDOWN_LINK&quot;:\n            return convert_markdown_link(url)\n        case &quot;NOT_A_URL&quot;:\n            return ValueError(&quot;Invalid URL&quot;)\n\n\ndef convert_markdown_link(link):\n    cleaned_link = link\n    if cleaned_link[-1] == &quot;,&quot;:\n        cleaned_link = cleaned_link[:-1]\n    cleaned_link = cleaned_link.replace(&quot;[&quot;, &quot;&quot;)\n    cleaned_link = cleaned_link.replace(&quot;)&quot;, &quot;&quot;)\n    cleaned_link = cleaned_link.split(&quot;](&quot;)\n    return cleaned_link\n\n\ndef download_article(url) -&gt; Article:\n    &quot;&quot;&quot;\n    Feed this a url for an article, and it will return the text of the article.\n    &quot;&quot;&quot;\n    url = validate_url(url)\n    article = Article(url, request_headers=headers)\n    article.download()\n    article.parse()\n    return article\n\n\nif __name__ == &quot;__main__&quot;:\n    url = &quot;https://link.springer.com/article/10.1007/s00146-023-01740-y&quot;\n    if len(sys.argv) &gt; 1:\n        url = sys.argv[1]\n    result = download_article(url)\n    print(result)\n</file>\n    <file path=\"src/siphon/ingestion/youtube/retrieve_youtube.py\">from siphon.ingestion.youtube.download_youtube_transcript import (\n    download_youtube_transcript,\n)\n# from siphon.ingestion.youtube.format_youtube import format_youtube\n\n\ndef retrieve_youtube(url: str) -&gt; tuple[str, dict]:\n    &quot;&quot;&quot;\n    Retrieve the YouTube transcript from a given URL.\n    Returns a tuple containing the formatted transcript and a metadata dict.\n    &quot;&quot;&quot;\n    transcript, metadata = download_youtube_transcript(url)\n    if not transcript:\n        raise ValueError(f&quot;No transcript found for URL: {url}&quot;)\n    # formatted_transcript = format_youtube(transcript)\n    return transcript, metadata\n</file>\n    <file path=\"src/siphon/ingestion/youtube/download_youtube_transcript.py\">&quot;&quot;&quot;\nFrom module creator:\n\n    *This code uses an undocumented part of the YouTube API, which is called by the YouTube web-client.\n    So there is no guarantee that it won't stop working tomorrow, if they change how things work. I will\n    however do my best to make things working again as soon as possible if that happens. So if it stops\n    working, let me know!*\n\n&quot;&quot;&quot;\n\nfrom siphon.logs.logging_config import get_logger\nfrom youtube_transcript_api import YouTubeTranscriptApi\nimport yt_dlp\nimport sys\nimport re\n\nlogger = get_logger(__name__)\n\nvideo_id = &quot;5xb6uWLtCsI&quot;\nexample = &quot;https://www.youtube.com/watch?v=VctsqOo8wsc&amp;t=628s&quot;\n\n\ndef validate_video_id(input):\n    &quot;&quot;&quot;\n    Validates that user has entered a valid YouTube Video ID, or a valid YouTube URL.\n    &quot;&quot;&quot;\n    if re.match(r&quot;^[a-zA-Z0-9_-]{11}$&quot;, input):\n        return input\n    elif re.match(\n        r&quot;^https?:\\/\\/(www\\.)?youtube\\.com\\/.*[?&amp;]v=([a-zA-Z0-9_-]{11})&quot;, input\n    ):\n        return re.match(\n            r&quot;^https?:\\/\\/(www\\.)?youtube\\.com\\/.*[?&amp;]v=([a-zA-Z0-9_-]{11})&quot;, input\n        ).group(2)\n    # also match for urls that have &quot;live&quot; instead of the standard &quot;watch&quot; https://www.youtube.com/live/OOdtmCMSOo4\n    elif re.match(\n        r&quot;^https?:\\/\\/(www\\.)?youtube\\.com\\/live\\/([a-zA-Z0-9_-]{11})&quot;, input\n    ):\n        return re.match(\n            r&quot;^https?:\\/\\/(www\\.)?youtube\\.com\\/live\\/([a-zA-Z0-9_-]{11})&quot;, input\n        ).group(2)\n    else:\n        raise ValueError(&quot;Invalid YouTube URL or Video ID&quot;)\n\n\ndef download_youtube_transcript(video_id) -&gt; tuple[str, dict]:\n    &quot;&quot;&quot;\n    Feed this either a youtube link or a youtube video id, and it will return the transcript of the video.\n    Returns a tuple of the transcript and metadata.\n    &quot;&quot;&quot;\n    logger.info(f&quot;Input video_id: {video_id}&quot;)\n    video_id = validate_video_id(video_id)\n    logger.info(f&quot;Validated video_id: {video_id}&quot;)\n    logger.info(f&quot;Type of video_id: {type(video_id)}&quot;)\n    logger.info(&quot;Getting metadata from GitHub api...&quot;)\n    with yt_dlp.YoutubeDL({&quot;quiet&quot;: True}) as ydl:\n        info = ydl.extract_info(video_id, download=False)\n        metadata = {\n            # Online metadata (standard fields)\n            &quot;url&quot;: info.get(&quot;webpage_url&quot;),\n            &quot;domain&quot;: &quot;youtube.com&quot;,\n            &quot;title&quot;: info.get(&quot;title&quot;),\n            &quot;published_date&quot;: info.get(&quot;upload_date&quot;)\n            if info.get(&quot;upload_date&quot;)\n            else None,\n            # Youtube-specific\n            &quot;video_id&quot;: info.get(&quot;id&quot;),\n            &quot;channel&quot;: info.get(&quot;channel&quot;),\n            &quot;duration&quot;: info.get(&quot;duration&quot;),\n            &quot;view_count&quot;: info.get(&quot;view_count&quot;),\n            &quot;description&quot;: info.get(&quot;description&quot;),\n            &quot;tags&quot;: info.get(&quot;tags&quot;),\n            &quot;like_count&quot;: info.get(&quot;like_count&quot;),\n            &quot;comment_count&quot;: info.get(&quot;comment_count&quot;),\n        }\n    logger.info(f&quot;Metadata: {metadata}&quot;)\n    logger.info(&quot;Downloading transcript...&quot;)\n    # this is the latest logic from the youtube-transcript-api package (v1.2.2)\n    ytt_api = YouTubeTranscriptApi()\n    fetched_transcript = ytt_api.fetch(video_id)\n    t = fetched_transcript.to_raw_data()\n    assert isinstance(t, list), &quot;Transcript should be a list&quot;\n    script = &quot; &quot;.join([line[&quot;text&quot;] for line in t])\n    assert len(script) &gt; 0, &quot;Transcript should not be empty&quot;\n    return script, metadata\n\n\nif __name__ == &quot;__main__&quot;:\n    video_id = &quot;5xb6uWLtCsI&quot;\n    # video_id = &quot;dQw4w9WgXcQ&quot;\n    if len(sys.argv) &gt; 1:\n        video_id = sys.argv[1]\n    print(download_youtube_transcript(video_id))\n</file>\n    <file path=\"src/siphon/ingestion/youtube/format_youtube.py\">from pathlib import Path\n\n\ndir_path = Path(__file__).parent\nprompts_dir = dir_path.parent.parent / &quot;prompts&quot;\nformat_prompt_file = prompts_dir / &quot;format_transcript.jinja2&quot;\n\n\ndef format_youtube(transcript: str, preferred_model: str = &quot;claude&quot;) -&gt; str:\n    &quot;&quot;&quot;\n    This function takes a raw transcript and formats it.\n    &quot;&quot;&quot;\n    from conduit.sync import Model, Prompt, Conduit\n\n    model = Model(preferred_model)\n    prompt = Prompt(format_prompt_file.read_text())\n    conduit = Conduit(prompt=prompt, model=model)\n    response = conduit.run(input_variables={&quot;transcript&quot;: transcript}, verbose=True)\n    return str(response.content)\n</file>\n    <file path=\"src/siphon/ingestion/audio/transcribe.py\">from pathlib import Path\nfrom transformers import pipeline\nimport torch\n\n# Import our centralized logger - no configuration needed here!\nfrom siphon.logs.logging_config import get_logger\n\n# Get logger for this module - will inherit config from retrieve_audio.py\nlogger = get_logger(__name__)\n\n\n# Transcript workflow\ndef transcribe(file_name: str | Path) -&gt; str:\n    &quot;&quot;&quot;\n    Use Whisper to retrieve text content + timestamps.\n    &quot;&quot;&quot;\n    transcriber = pipeline(\n        &quot;automatic-speech-recognition&quot;,\n        model=&quot;openai/whisper-base&quot;,\n        # model=&quot;openai/whisper-large-v3&quot;,\n        return_timestamps=&quot;sentence&quot;,\n        device=0,\n        torch_dtype=torch.float16,\n    )\n    logger.info(f&quot;Transcribing file: {file_name}&quot;)\n    result = transcriber(file_name)\n    return result\n</file>\n    <file path=\"src/siphon/ingestion/audio/clean.py\">from conduit.sync import Prompt, Model, Conduit\nfrom pathlib import Path\n\ndir_path = Path(__file__).parent\nprompt_file = dir_path / &quot;system_message.jinja2&quot;\n\n# Import our centralized logger - no configuration needed here!\nfrom siphon.logs.logging_config import get_logger\n\n# Get logger for this module - will inherit config from retrieve_audio.py\nlogger = get_logger(__name__)\n\n\ndef clean_transcript(formatted_transcript: str) -&gt; str:\n    &quot;&quot;&quot;\n    Clean up the formatted transcript using a language model.\n\n    Args:\n        formatted_transcript (str): The formatted transcript to clean up.\n\n    Returns:\n        str: Cleaned-up transcript.\n    &quot;&quot;&quot;\n    # Load the prompt template\n    prompt = Prompt(prompt_file.read_text())\n    model = Model(&quot;gemini2.5&quot;)\n    conduit = Conduit(model=model, prompt=prompt)\n    response = conduit.run(input_variables={&quot;transcript&quot;: formatted_transcript})\n    return str(response.content).strip()\n\n\nif __name__ == &quot;__main__&quot;:\n    from example import example_file\n    from diarize import diarize\n    from transcribe import transcribe\n    from combine import combine\n    from format import format_transcript\n\n    diarization_result = diarize(example_file)\n    transcript_result = transcribe(example_file)\n    annotated = combine(diarization_result, transcript_result)\n    formatted = format_transcript(annotated, group_by_speaker=True)\n    cleaned = clean_transcript(formatted)\n    print(cleaned)\n</file>\n    <file path=\"src/siphon/ingestion/audio/local_transcript.py\">&quot;&quot;&quot;\nAnnotated transcripts require several steps, and state of the art for open source solutions is this approach:\n- diarize the audio (identified speakers + timestamps)\n- transcribe the audio (text content + timestamps)\n- align the two results (match time stamps)\n- format the results for display\n- clean up the transcript (with LLM call)\n&quot;&quot;&quot;\n\nfrom siphon.ingestion.audio.transcribe import transcribe\nfrom siphon.ingestion.audio.diarize import diarize\nfrom siphon.ingestion.audio.combine import combine\nfrom siphon.ingestion.audio.format import format_transcript\nfrom siphon.ingestion.audio.convert import convert_to_mp3\nfrom pathlib import Path\n\n# Import our centralized logger - no configuration needed here!\nfrom siphon.logs.logging_config import get_logger\n\n# Get logger for this module - will inherit config from retrieve_audio.py\nlogger = get_logger(__name__)\n\n# Example file\ndir_path = Path(__file__).parent\nassets_dir = dir_path.parent / &quot;assets&quot;\nallhands_file = assets_dir / &quot;allhands.mp3&quot;\n\n\ndef get_local_transcript(file_path: Path | str) -&gt; str | None:\n    &quot;&quot;&quot;\n    Converts audio file to a format suitable for processing.\n    &quot;&quot;&quot;\n    converted = False\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n    if not file_path.suffix.lower() == &quot;.mp3&quot;:\n        logger.info(f&quot;Converting {file_path} to MP3 format for processing.&quot;)\n        # Convert to MP3 if not already in that format\n        file_path = convert_to_mp3(file_path)\n        converted = True\n    if file_path.exists() and file_path.suffix.lower() == &quot;.mp3&quot;:\n        logger.info(f&quot;Processing file: {file_path}&quot;)\n        # Diarize\n        logger.info(&quot;Starting diarization...&quot;)\n        diarization_result = diarize(str(file_path))\n        # Transcribe\n        logger.info(&quot;Starting transcription...&quot;)\n        transcript_result = transcribe(str(file_path))\n        # Combine diarization and transcription results by timestamps\n        logger.info(&quot;Combining diarization and transcription results...&quot;)\n        annotated = combine(diarization_result, transcript_result)\n        # Format for display with a dedicate LLM call\n        logger.info(&quot;Formatting transcript for readability...&quot;)\n        readable_transcript = format_transcript(annotated, group_by_speaker=True)\n        if converted:\n            logger.info(&quot;Cleaning up temporary MP3 file created during conversion.&quot;)\n            # Clean up the temporary MP3 file if it was created\n            file_path.unlink(missing_ok=True)\n        return readable_transcript\n    else:\n        if converted:\n            # Clean up the temporary MP3 file if it was created\n            logger.info(&quot;Cleaning up temporary MP3 file created during conversion.&quot;)\n            file_path.unlink(missing_ok=True)\n</file>\n    <file path=\"src/siphon/ingestion/audio/format.py\"># Import our centralized logger - no configuration needed here!\nfrom siphon.logs.logging_config import get_logger\n\n# Get logger for this module - will inherit config from retrieve_audio.py\nlogger = get_logger(__name__)\n\n\ndef format_transcript(annotated_transcript, group_by_speaker=True):\n    &quot;&quot;&quot;\n    Format the annotated transcript into readable text.\n\n    Args:\n        annotated_transcript: Output from combine()\n        group_by_speaker: Whether to group consecutive words by same speaker\n\n    Returns:\n        str: Formatted transcript\n    &quot;&quot;&quot;\n    if not annotated_transcript:\n        return &quot;&quot;\n\n    if not group_by_speaker:\n        logger.warning(\n            &quot;Formatting without grouping by speaker may produce less readable output.&quot;\n        )\n        # Simple word-by-word format\n        lines = []\n        for item in annotated_transcript:\n            timestamp = f&quot;[{item['start_time']:.1f}s]&quot;\n            lines.append(f&quot;{timestamp} {item['speaker']}: {item['word']}&quot;)\n        return &quot;\\n&quot;.join(lines)\n\n    # Group by speaker for more readable output\n    logger.info(&quot;Formatting transcript by grouping words by speaker.&quot;)\n    lines = []\n    current_speaker = None\n    current_words = []\n    current_start_time = None\n\n    for item in annotated_transcript:\n        speaker = item[&quot;speaker&quot;]\n\n        if speaker != current_speaker:\n            # Speaker changed, output previous speaker's text\n            if current_speaker is not None and current_words:\n                timestamp = f&quot;[{current_start_time:.1f}s]&quot;\n                text = &quot; &quot;.join(current_words)\n                lines.append(f&quot;{timestamp} {current_speaker}: {text}&quot;)\n\n            # Start new speaker\n            current_speaker = speaker\n            current_words = [item[&quot;word&quot;]]\n            current_start_time = item[&quot;start_time&quot;]\n        else:\n            # Same speaker, add word\n            current_words.append(item[&quot;word&quot;])\n\n    # Don't forget the last speaker\n    logger.info(&quot;Finalizing transcript for last speaker.&quot;)\n    if current_speaker is not None and current_words:\n        timestamp = f&quot;[{current_start_time:.1f}s]&quot;\n        text = &quot; &quot;.join(current_words)\n        lines.append(f&quot;{timestamp} {current_speaker}: {text}&quot;)\n\n    return &quot;\\n&quot;.join(lines)\n</file>\n    <file path=\"src/siphon/ingestion/audio/combine.py\"># Import our centralized logger - no configuration needed here!\nfrom siphon.logs.logging_config import get_logger\n\n# Get logger for this module - will inherit config from retrieve_audio.py\nlogger = get_logger(__name__)\n\n\ndef combine(diarization_result, transcript_result):\n    &quot;&quot;&quot;\n    Combine diarization and transcription results into an annotated transcript.\n\n    Args:\n        diarization_result: pyannote diarization output\n        transcript_result: transformers transcription output with word timestamps\n\n    Returns:\n        list: List of dictionaries with word, speaker, start_time, end_time\n    &quot;&quot;&quot;\n    # Extract word-level data from transcript\n    words = transcript_result[&quot;chunks&quot;]\n\n    # Convert diarization to list of (start, end, speaker) tuples\n    logger.info(&quot;Extracting diarization segments...&quot;)\n    diarization_segments = []\n    for turn, _, speaker in diarization_result.itertracks(yield_label=True):\n        diarization_segments.append((turn.start, turn.end, speaker))\n\n    # Align words with speakers\n    logger.info(&quot;Aligning words with speakers...&quot;)\n    annotated_transcript = []\n\n    for word_data in words:\n        try:\n            word_text = word_data[&quot;text&quot;].strip()\n            word_start, word_end = word_data[&quot;timestamp&quot;]\n\n            # Find speaker for this word using midpoint timestamp\n            word_midpoint = (word_start + word_end) / 2\n            speaker = find_speaker_at_time(word_midpoint, diarization_segments)\n\n            annotated_transcript.append(\n                {\n                    &quot;word&quot;: word_text,\n                    &quot;speaker&quot;: speaker,\n                    &quot;start_time&quot;: word_start,\n                    &quot;end_time&quot;: word_end,\n                }\n            )\n        except:\n            print(&quot;SINGLE ERROR&quot;)\n\n    return annotated_transcript\n\n\ndef find_speaker_at_time(timestamp, diarization_segments):\n    &quot;&quot;&quot;\n    Find which speaker is talking at a given timestamp.\n\n    Args:\n        timestamp: Time in seconds\n        diarization_segments: List of (start, end, speaker) tuples\n\n    Returns:\n        str: Speaker label or &quot;Unknown&quot;\n    &quot;&quot;&quot;\n    for start, end, speaker in diarization_segments:\n        if start &lt;= timestamp &lt;= end:\n            return speaker\n    return &quot;Unknown&quot;\n</file>\n    <file path=\"src/siphon/ingestion/audio/convert.py\">from pathlib import Path\nfrom pydub import AudioSegment\n\n# Import our centralized logger - no configuration needed here!\nfrom siphon.logs.logging_config import get_logger\n\n# Get logger for this module - will inherit config from retrieve_audio.py\nlogger = get_logger(__name__)\n\n\ndef convert_to_mp3(input_path: Path) -&gt; Path:\n    &quot;&quot;&quot;\n    Convert an audio file to MP3 format.\n\n    Args:\n        input_path: Path object pointing to the input audio file\n\n    Returns:\n        Path object pointing to the converted MP3 file\n\n    Raises:\n        FileNotFoundError: If the input file doesn't exist\n        Exception: If conversion fails\n    &quot;&quot;&quot;\n    if not input_path.exists():\n        raise FileNotFoundError(f&quot;Input file not found: {input_path}&quot;)\n\n    formats = [\n        &quot;.wav&quot;,\n        &quot;.m4a&quot;,\n        &quot;.ogg&quot;,\n        &quot;.flac&quot;,\n    ]\n    if input_path.suffix.lower() not in formats:\n        logger.error(\n            f&quot;Unsupported file type: {input_path.suffix}. Supported formats: {formats}&quot;\n        )\n        raise ValueError(\n            f&quot;Unsupported file type: {input_path.suffix}. Supported formats: {formats}&quot;\n        )\n\n    # Create output path with .mp3 extension\n    output_path = Path(__file__).parent / &quot;temp&quot; / input_path.with_suffix(&quot;.mp3&quot;).name\n\n    # Load the audio file (pydub automatically detects format)\n    logger.info(f&quot;Converting {input_path} to MP3 format.&quot;)\n    audio = AudioSegment.from_file(str(input_path))\n\n    # Export as MP3\n    logger.info(f&quot;Exporting converted audio to {output_path}.&quot;)\n    audio.export(str(output_path), format=&quot;mp3&quot;)\n\n    return output_path\n</file>\n    <file path=\"src/siphon/ingestion/audio/diarize.py\">import torch\nfrom pathlib import Path\nfrom pyannote.audio import Pipeline\n\n# Import our centralized logger - no configuration needed here!\nfrom siphon.logs.logging_config import get_logger\n\n# Get logger for this module - will inherit config from retrieve_audio.py\nlogger = get_logger(__name__)\n# Add this to your diarize.py\nimport torch\nimport warnings\n\n\ndef get_safe_device():\n    &quot;&quot;&quot;Get a device that actually works, falling back to CPU if GPU has issues&quot;&quot;&quot;\n    if not torch.cuda.is_available():\n        return torch.device(&quot;cpu&quot;)\n\n    try:\n        # Test if GPU operations actually work\n        test_tensor = torch.tensor([1.0]).cuda()\n        test_conv = torch.nn.Conv1d(1, 1, 3).cuda()\n        test_input = torch.randn(1, 1, 10).cuda()\n        _ = test_conv(test_input)\n        return torch.device(&quot;cuda&quot;)\n    except RuntimeError as e:\n        if &quot;no kernel image&quot; in str(e) or &quot;sm_120&quot; in str(e):\n            warnings.warn(&quot;RTX 5090 not yet supported by PyTorch. Using CPU for now.&quot;)\n            return torch.device(&quot;cpu&quot;)\n        raise e\n\n\n# Diarization workflow\ndef diarize(file_name: str | Path) -&gt; str:\n    &quot;&quot;&quot;\n    Use Pyannote to identify speakers + timestamps.\n    &quot;&quot;&quot;\n    # Load the pipeline\n    pipeline = Pipeline.from_pretrained(&quot;pyannote/speaker-diarization@2.1&quot;)\n    device = get_safe_device()\n    pipeline.to(device)\n    # Move the pipeline to GPU (CUDA)\n    print(f&quot;Using device: {device}&quot;)\n    # Apply the pipeline to the audio file\n    logger.info(f&quot;Processing diarization for file: {file_name}&quot;)\n    diarization = pipeline(file_name)\n    return diarization\n</file>\n    <file path=\"src/siphon/ingestion/audio/retrieve_audio.py\">from pathlib import Path\nfrom typing import Literal\n\n# Import our centralized logging configuration\nfrom siphon.logs.logging_config import get_logger\n\n\n# Configure logging once at the entry point\nlogger = get_logger(__name__)\n\n\ndef retrieve_audio(\n    audio_file: str | Path, model: Literal[&quot;local&quot;, &quot;cloud&quot;] = &quot;local&quot;\n) -&gt; str:\n    &quot;&quot;&quot;\n    Wrapper script; has option of requesting openai instead of local, defaults to local.\n    &quot;&quot;&quot;\n\n    if model == &quot;local&quot;:\n        from siphon.ingestion.audio.local_transcript import get_local_transcript\n\n        logger.info(&quot;Starting local transcription process.&quot;)\n        return str(get_local_transcript(audio_file))\n    elif model == &quot;cloud&quot;:\n        from siphon.ingestion.audio.openai_transcript import get_openai_transcript\n\n        logger.info(&quot;Starting OpenAI transcription process.&quot;)\n        return get_openai_transcript(audio_file)\n    else:\n        raise ValueError(f&quot;Invalid model option: {model}. Choose 'local' or 'cloud'.&quot;)\n</file>\n    <file path=\"src/siphon/ingestion/audio/openai_transcript.py\">from openai import OpenAI\nfrom pathlib import Path\nimport os\n\n# Import our centralized logger - no configuration needed here!\nfrom siphon.logs.logging_config import get_logger\n\n# Get logger for this module - will inherit config from retrieve_audio.py\nlogger = get_logger(__name__)\n\n\napi_key = os.getenv(&quot;OPENAI_API_KEY&quot;)\nopenai_client = OpenAI(api_key=api_key)\n\n\ndef get_openai_transcript(audio_file: str | Path):\n    &quot;&quot;&quot;\n    Use the transcriptions API endpoint.\n    We didn't implement this in Conduit since it's really tied to a transcription use case.\n    &quot;&quot;&quot;\n    if isinstance(audio_file, str):\n        audio_file = Path(audio_file)\n    extension = audio_file.suffix.lower()\n    if extension[1:] not in [&quot;mp3&quot;, &quot;wav&quot;]:\n        raise ValueError(&quot;Wrong extension; whisper only handles mp3 and wav.&quot;)\n    else:\n        logger.info(f&quot;Transcribing {audio_file} with OpenAI Whisper API.&quot;)\n        with open(audio_file, &quot;rb&quot;) as f:\n            transcript = openai_client.audio.transcriptions.create(\n                file=f,\n                model=&quot;whisper-1&quot;,\n            )\n    return transcript.text\n</file>\n    <file path=\"src/siphon/ingestion/image/describe_image_with_cloud.py\">from pathlib import Path\nfrom conduit.sync import Model\nfrom conduit.message.imagemessage import ImageMessage\n\n# Import our centralized logger - no configuration needed here!\nfrom siphon.logs.logging_config import get_logger\n\n# Get logger for this module - will inherit config from retrieve_audio.py\nlogger = get_logger(__name__)\n\n\ndef describe_image_with_cloud_models(file_path: str | Path, model=&quot;gpt&quot;) -&gt; str:\n    &quot;&quot;&quot;\n    Describe an image using a Conduit model.\n    TBD: implement Ollama.\n    &quot;&quot;&quot;\n    prompt_str = &quot;Please describe this image in detail. If it is full of text, please provide the text verbatim.&quot;\n    imagemessage = ImageMessage.from_image_file(\n        role=&quot;user&quot;, image_file=file_path, text_content=prompt_str\n    )\n\n    logger.info(f&quot;Creating Conduit model with name: {model}&quot;)\n    model = Model(model)\n    logger.info(f&quot;Running query with model: {model.model} and image: {file_path}&quot;)\n    response = model.query(imagemessage)\n    return str(response.content)\n</file>\n    <file path=\"src/siphon/ingestion/image/describe_image_with_ollama.py\">from conduit.message.convert_image import convert_image_file\nfrom pathlib import Path\n\n# Import our centralized logger - no configuration needed here!\nfrom siphon.logs.logging_config import get_logger\n\n# Get logger for this module - will inherit config from retrieve_audio.py\nlogger = get_logger(__name__)\n\n\ndef describe_image_with_ollama_models(file_path: str | Path, model=&quot;gemma3:27b&quot;) -&gt; str:\n    import ollama\n\n    logger.info(f&quot;Converting image file {file_path} to Ollama-compatible format.&quot;)\n    image_data = convert_image_file(file_path)\n    # Send the image to a vision-capable model (e.g., 'llava')\n    logger.info(f&quot;Sending image data to Ollama model: {model}&quot;)\n    response = ollama.generate(\n        model=model,\n        prompt=&quot;Describe this photo in detail. If there is text in the image, return it verbatim.&quot;,\n        images=[image_data],\n        keep_alive=0,\n    )\n    logger.info(&quot;Received response from Ollama model.&quot;)\n    return response[&quot;response&quot;]\n</file>\n    <file path=\"src/siphon/ingestion/image/retrieve_image.py\">from pathlib import Path\n\n\nfrom typing import Literal\n\n# Import our centralized logging configuration\nfrom siphon.logs.logging_config import get_logger\n\n# Configure logging once at the entry point\nlogger = get_logger(__name__)\n\n\ndef retrieve_image(image_path: str | Path, model: Literal[&quot;local&quot;, &quot;cloud&quot;]) -&gt; str:\n    &quot;&quot;&quot;\n    Retrieves an image description using the specified model.\n\n    Args:\n        image_path (str | Path): The path to the image file.\n        model (str): The model to use for image description. Options are &quot;ollama&quot; or any Conduit-supported model.\n\n    Returns:\n        str: The description of the image.\n    &quot;&quot;&quot;\n    if model == &quot;cloud&quot;:\n        from siphon.ingestion.image.describe_image_with_cloud import (\n            describe_image_with_cloud_models,\n        )\n\n        logger.info(f&quot;Starting image description process with Conduit model: {model}&quot;)\n        return describe_image_with_cloud_models(image_path, model=&quot;gpt-4.1-mini&quot;)\n    elif model == &quot;local&quot;:\n        from siphon.ingestion.image.describe_image_with_ollama import (\n            describe_image_with_ollama_models,\n        )\n\n        logger.info(&quot;Starting image description process with Ollama models.&quot;)\n        return describe_image_with_ollama_models(image_path)\n</file>\n    <file path=\"src/siphon/ingestion/googledrive/retrieve_google_doc.py\">def retrieve_google_doc(url: str) -&gt; str:\n    pass\n</file>\n    <file path=\"src/siphon/ingestion/github/flatten_xml.py\">&quot;&quot;&quot;\nShared XML generation logic for the Flatten tool.\nProvides common functionality for creating structured XML output\nfrom both local directories and GitHub repositories.\n&quot;&quot;&quot;\n\nimport xml.dom.minidom as md\nimport xml.etree.ElementTree as ET\nfrom typing import TypeAlias, Callable\nfrom collections.abc import Iterable\n\nXMLString: TypeAlias = str\n\n# Common exclude patterns (gitignore-style)\nEXCLUDE_PATTERNS = [\n    &quot;__pycache__&quot;,\n    &quot;.git&quot;,\n    &quot;.pytest_cache&quot;,\n    &quot;.mypy_cache&quot;,\n    &quot;*.egg-info&quot;,\n    &quot;.DS_Store&quot;,\n    &quot;.venv&quot;,\n    &quot;venv&quot;,\n    &quot;.env&quot;,\n    &quot;cache&quot;,\n    &quot;*.pyc&quot;,\n]\n\n# File extensions to include\nINCLUDE_EXTENSIONS = (&quot;.py&quot;, &quot;.lua&quot;)  # &quot;.md&quot;\n\n\ndef should_exclude_path(path: str) -&gt; bool:\n    &quot;&quot;&quot;Check if a path should be excluded based on EXCLUDE_PATTERNS.&quot;&quot;&quot;\n    for pattern in EXCLUDE_PATTERNS:\n        if pattern.startswith(&quot;*.&quot;):\n            if path.endswith(pattern[1:]):\n                return True\n        elif pattern in path:\n            return True\n    return False\n\n\ndef should_include_file(filename: str) -&gt; bool:\n    &quot;&quot;&quot;Check if a file should be included based on extension.&quot;&quot;&quot;\n    return filename.endswith(INCLUDE_EXTENSIONS)\n\n\ndef create_directory_tree_xml(\n    project_name: str,\n    path_iterator: Callable[\n        [], Iterable[tuple[str, str]]\n    ],  # Returns (file_path, filename) tuples\n) -&gt; ET.Element:\n    &quot;&quot;&quot;\n    Create an XML directory tree from a path iterator.\n\n    Args:\n        project_name: Name of the root project\n        path_iterator: Function that yields (file_path, filename) tuples\n\n    Returns:\n        XML Element representing the directory tree\n    &quot;&quot;&quot;\n    directory_tree = ET.Element(&quot;directory_tree&quot;)\n    root_directory = ET.SubElement(directory_tree, &quot;directory&quot;, name=project_name)\n\n    # Track created directories: path -&gt; element mapping\n    dirs_created = {project_name: root_directory}\n\n    for file_path, filename in path_iterator():\n        # Skip if file should be excluded\n        if should_exclude_path(file_path) or not should_include_file(filename):\n            continue\n\n        # Parse directory structure from file path\n        path_parts = file_path.split(&quot;/&quot;)\n        if len(path_parts) &lt;= 1:\n            # File in root directory\n            current_dir = root_directory\n            clean_path = filename\n        else:\n            # File in subdirectory - create nested structure\n            current_dir = root_directory\n            current_path = project_name\n\n            # Create nested directories as needed\n            for dir_name in path_parts[:-1]:\n                dir_path = current_path + &quot;/&quot; + dir_name\n                if dir_path not in dirs_created:\n                    new_dir = ET.SubElement(current_dir, &quot;directory&quot;, name=dir_name)\n                    dirs_created[dir_path] = new_dir\n                    current_dir = new_dir\n                else:\n                    current_dir = dirs_created[dir_path]\n                current_path = dir_path\n\n            clean_path = &quot;/&quot;.join(path_parts)\n\n        # Add file to the XML structure\n        ET.SubElement(current_dir, &quot;file&quot;, name=filename, path=clean_path)\n\n    return directory_tree\n\n\ndef create_file_contents_xml(\n    file_reader: Callable[[str], str],  # Function that reads file content by path\n    path_iterator: Callable[\n        [], Iterable[tuple[str, str]]\n    ],  # Returns (file_path, filename) tuples\n) -&gt; ET.Element:\n    &quot;&quot;&quot;\n    Create an XML file contents section with CDATA.\n\n    Args:\n        file_reader: Function that takes a file path and returns content string\n        path_iterator: Function that yields (file_path, filename) tuples\n\n    Returns:\n        XML Element with file contents in CDATA sections\n    &quot;&quot;&quot;\n    root = ET.Element(&quot;file_contents&quot;)\n\n    # Collect relevant files\n    relevant_files = []\n    for file_path, filename in path_iterator():\n        if should_exclude_path(file_path) or not should_include_file(filename):\n            continue\n        relevant_files.append(file_path)\n\n    # Create basic XML structure first\n    file_elements = []\n    for file_path in relevant_files:\n        file_elem = ET.SubElement(root, &quot;file&quot;, path=file_path)\n        file_elements.append((file_path, file_elem))\n\n    # Convert to string and parse with minidom to add CDATA\n    xml_str = ET.tostring(root, encoding=&quot;unicode&quot;)\n    dom = md.parseString(xml_str)\n\n    # Add CDATA sections to each file element\n    for i, (file_path, _) in enumerate(file_elements):\n        try:\n            content = file_reader(file_path)\n            file_element = dom.getElementsByTagName(&quot;file&quot;)[i]\n            cdata = dom.createCDATASection(content)\n            file_element.appendChild(cdata)\n        except Exception as e:\n            print(f&quot;Error reading file {file_path}: {e}&quot;)\n\n    # Convert back to ElementTree\n    new_root = ET.fromstring(dom.toxml())\n    return new_root\n\n\ndef package_to_xml(\n    project_name: str,\n    file_reader: Callable[[str], str],\n    path_iterator: Callable[[], Iterable[tuple[str, str]]],\n) -&gt; XMLString:\n    &quot;&quot;&quot;\n    Create a complete XML package with directory tree and file contents.\n\n    Args:\n        project_name: Name of the project\n        file_reader: Function that reads file content by path\n        path_iterator: Function that yields (file_path, filename) tuples\n\n    Returns:\n        Formatted XML string\n    &quot;&quot;&quot;\n    # Create the root project element\n    root = ET.Element(&quot;project&quot;, name=project_name)\n\n    # Get directory structure\n    dir_tree = create_directory_tree_xml(project_name, path_iterator)\n    root.append(dir_tree)\n\n    # Get file contents\n    file_contents = create_file_contents_xml(file_reader, path_iterator)\n    root.append(file_contents)\n\n    # Format and return\n    return format_xml(root)\n\n\ndef format_xml(root_element: ET.Element) -&gt; XMLString:\n    &quot;&quot;&quot;\n    Format an XML element as a pretty-printed string.\n\n    Args:\n        root_element: The root XML element to format\n\n    Returns:\n        Pretty-printed XML string\n    &quot;&quot;&quot;\n    # Convert to string with proper formatting\n    rough_string = ET.tostring(root_element, encoding=&quot;unicode&quot;)\n\n    # Use minidom to prettify\n    dom = md.parseString(rough_string)\n    pretty_xml = dom.toprettyxml(indent=&quot;  &quot;)\n\n    return pretty_xml\n</file>\n    <file path=\"src/siphon/ingestion/github/flatten_directory.py\">&quot;&quot;&quot;\nLocal directory processing for the Flatten tool.\nHandles flattening local project directories into XML format.\n&quot;&quot;&quot;\n\nimport os\nfrom pathlib import Path\nfrom typing import Callable\nfrom collections.abc import Iterable\n\nfrom siphon.ingestion.github.flatten_xml import (\n    package_to_xml,\n    should_exclude_path,\n    should_include_file,\n)\n\n\ndef read_local_file(file_path: str) -&gt; str:\n    &quot;&quot;&quot;Read content from a local file.&quot;&quot;&quot;\n    with open(file_path, encoding=&quot;utf-8&quot;) as f:\n        return f.read()\n\n\ndef get_local_file_iterator(directory: Path) -&gt; Callable:\n    &quot;&quot;&quot;\n    Return a function that iterates over files in a local directory.\n\n    Args:\n        directory: Path to the directory to iterate\n\n    Returns:\n        Function that yields (file_path, filename) tuples\n    &quot;&quot;&quot;\n\n    def iterator() -&gt; Iterable[tuple[str, str]]:\n        for dirpath, dirnames, filenames in os.walk(directory):\n            # Skip excluded directories\n            path_str = str(dirpath)\n            if should_exclude_path(path_str):\n                continue\n\n            # Modify dirnames in-place to prevent os.walk from entering excluded dirs\n            i = 0\n            while i &lt; len(dirnames):\n                dirname = dirnames[i]\n                test_path = os.path.join(dirpath, dirname)\n                if should_exclude_path(test_path):\n                    dirnames.pop(i)\n                else:\n                    i += 1\n\n            # Process files in non-excluded directories\n            for filename in filenames:\n                if should_include_file(filename):\n                    file_path = os.path.join(dirpath, filename)\n                    # Convert to relative path from the base directory\n                    relative_path = os.path.relpath(file_path, directory)\n                    # Normalize path separators for consistency\n                    relative_path = relative_path.replace(os.sep, &quot;/&quot;)\n                    yield (relative_path, filename)\n\n    return iterator\n\n\ndef flatten_directory(directory_path: str = &quot;.&quot;) -&gt; str:\n    &quot;&quot;&quot;\n    Flatten a local directory into XML format.\n\n    Args:\n        directory_path: Path to the directory to flatten (default: current directory)\n\n    Returns:\n        XML string representation of the directory\n    &quot;&quot;&quot;\n    directory = Path(directory_path).resolve()\n    project_name = directory.name\n\n    # Create the path iterator for this directory\n    path_iterator = get_local_file_iterator(directory)\n\n    # Create file reader that handles absolute paths\n    def file_reader(relative_path: str) -&gt; str:\n        absolute_path = directory / relative_path\n        return read_local_file(str(absolute_path))\n\n    return package_to_xml(project_name, file_reader, path_iterator)\n</file>\n    <file path=\"src/siphon/ingestion/github/flatten_url.py\">&quot;&quot;&quot;\nGitHub repository processing for the Flatten tool.\nHandles fetching and flattening GitHub repositories into XML format.\n&quot;&quot;&quot;\n\nimport os\nimport zipfile\nfrom io import BytesIO\nfrom typing import Iterator, Tuple\nimport requests\n\nfrom siphon.ingestion.github.flatten_xml import (\n    package_to_xml,\n    should_exclude_path,\n    should_include_file,\n)\n\n\ndef parse_github_url(github_url: str) -&gt; tuple[str, dict]:\n    &quot;&quot;&quot;\n    Convert a GitHub URL to a zipball API URL with better error handling.\n    Also retrieve metadata through GH API.\n    &quot;&quot;&quot;\n    parts = github_url.rstrip(&quot;/&quot;).split(&quot;/&quot;)\n    if len(parts) &lt; 2:\n        raise ValueError(&quot;Invalid GitHub URL format&quot;)\n\n    owner = parts[-2]\n    repo = parts[-1]\n\n    from github import Github\n    import os\n\n    # Initialize GitHub API client\n    g = Github(os.getenv(&quot;GITHUB_TOKEN&quot;))\n    repo_obj = g.get_repo(f&quot;{owner}/{repo}&quot;)\n    # Extract metadata\n\n    metadata = {\n        # Standard online metadata\n        &quot;url&quot;: repo_obj.url,\n        &quot;domain&quot;: &quot;github.com&quot;,\n        &quot;title&quot;: f&quot;{owner}/{repo}&quot;,\n        &quot;published_date&quot;: int(repo_obj.created_at.timestamp()),\n        # GitHub-specific\n        &quot;stars&quot;: repo_obj.stargazers_count,\n        &quot;language&quot;: repo_obj.language,\n        &quot;topics&quot;: repo_obj.topics,\n        &quot;description&quot;: repo_obj.description,\n        &quot;updated_at&quot;: int(repo_obj.updated_at.timestamp()),\n        &quot;pushed_at&quot;: int(repo_obj.pushed_at.timestamp()),\n        &quot;size&quot;: repo_obj.size,\n    }\n\n    # First try to get the default branch\n    token = os.getenv(&quot;GITHUB_TOKEN&quot;)\n    headers = {&quot;Authorization&quot;: f&quot;token {token}&quot;} if token else {}\n\n    try:\n        # Get repository info to find default branch\n        repo_info_url = f&quot;https://api.github.com/repos/{owner}/{repo}&quot;\n        response = requests.get(repo_info_url, headers=headers)\n\n        if response.status_code == 200:\n            repo_data = response.json()\n            default_branch = repo_data.get(&quot;default_branch&quot;, &quot;main&quot;)\n        else:\n            print(\n                f&quot;Warning: Could not get repo info (status {response.status_code}), defaulting to 'main'&quot;\n            )\n            default_branch = &quot;main&quot;\n    except Exception as e:\n        print(f&quot;Warning: Error getting repo info: {e}, defaulting to 'main'&quot;)\n        default_branch = &quot;main&quot;\n\n    zipball_url = (\n        f&quot;https://api.github.com/repos/{owner}/{repo}/zipball/{default_branch}&quot;\n    )\n    print(f&quot;Using zipball URL: {zipball_url}&quot;)\n\n    return zipball_url, metadata\n\n\ndef download_github_repo(repo_url: str) -&gt; zipfile.ZipFile:\n    &quot;&quot;&quot;\n    Download a GitHub repository as a zip file with better error handling.\n    &quot;&quot;&quot;\n    token = os.getenv(&quot;GITHUB_TOKEN&quot;)\n    headers = {&quot;Authorization&quot;: f&quot;token {token}&quot;} if token else {}\n\n    print(f&quot;Downloading from: {repo_url}&quot;)\n    print(f&quot;Using token: {'Yes' if token else 'No'}&quot;)\n\n    response = requests.get(repo_url, headers=headers)\n\n    print(f&quot;Response status: {response.status_code}&quot;)\n    print(f&quot;Response headers: {dict(response.headers)}&quot;)\n\n    if response.status_code == 401:\n        print(&quot;401 Unauthorized - possible causes:&quot;)\n        print(&quot;1. Token expired or invalid&quot;)\n        print(&quot;2. Repository is private and token lacks access&quot;)\n        print(&quot;3. Token doesn't have 'repo' scope&quot;)\n        raise Exception(f&quot;Authentication failed. Check your GitHub token permissions.&quot;)\n    elif response.status_code == 404:\n        raise Exception(\n            f&quot;Repository not found. Check if repository exists and is accessible.&quot;\n        )\n    elif response.status_code != 200:\n        raise Exception(\n            f&quot;Failed to fetch repository: {response.status_code} - {response.text}&quot;\n        )\n\n    zip_data = BytesIO(response.content)\n    return zipfile.ZipFile(zip_data, &quot;r&quot;)\n\n\ndef get_repo_name_from_zip(zip_file: zipfile.ZipFile) -&gt; str:\n    &quot;&quot;&quot;Extract the repository name from the zip file structure.&quot;&quot;&quot;\n    file_list = zip_file.namelist()\n    if file_list:\n        # GitHub adds a hash prefix like &quot;repo-abc123/&quot;\n        repo_name = file_list[0].split(&quot;/&quot;)[0]\n        # Clean up the repo name (remove hash suffix)\n        if &quot;-&quot; in repo_name:\n            repo_name = repo_name.split(&quot;-&quot;)[0]\n        return repo_name\n    return &quot;unknown_repo&quot;\n\n\ndef get_github_file_iterator(zip_file: zipfile.ZipFile) -&gt; callable:\n    &quot;&quot;&quot;\n    Return a function that iterates over files in a GitHub zip file.\n\n    Args:\n        zip_file: ZipFile object to iterate over\n\n    Returns:\n        Function that yields (file_path, filename) tuples\n    &quot;&quot;&quot;\n\n    def iterator() -&gt; Iterator[Tuple[str, str]]:\n        file_list = zip_file.namelist()\n\n        for file_path in file_list:\n            # Skip directories (they end with /)\n            if file_path.endswith(&quot;/&quot;):\n                continue\n\n            # Skip excluded paths\n            if should_exclude_path(file_path):\n                continue\n\n            # Extract filename\n            filename = file_path.split(&quot;/&quot;)[-1]\n\n            # Include only desired file types\n            if should_include_file(filename):\n                # Remove the GitHub repo prefix (first directory component)\n                path_parts = file_path.split(&quot;/&quot;)\n                if len(path_parts) &gt; 1:\n                    clean_path = &quot;/&quot;.join(path_parts[1:])\n                else:\n                    clean_path = file_path\n\n                yield (clean_path, filename)\n\n    return iterator\n\n\ndef create_github_file_reader(zip_file: zipfile.ZipFile) -&gt; callable:\n    &quot;&quot;&quot;\n    Create a file reader function for GitHub zip files.\n\n    Args:\n        zip_file: ZipFile object to read from\n\n    Returns:\n        Function that reads file content by clean path\n    &quot;&quot;&quot;\n    # Build mapping from clean paths to zip paths\n    path_mapping = {}\n    file_list = zip_file.namelist()\n\n    for zip_path in file_list:\n        if not zip_path.endswith(&quot;/&quot;):\n            path_parts = zip_path.split(&quot;/&quot;)\n            if len(path_parts) &gt; 1:\n                clean_path = &quot;/&quot;.join(path_parts[1:])\n                path_mapping[clean_path] = zip_path\n\n    def file_reader(clean_path: str) -&gt; str:\n        &quot;&quot;&quot;Read file content from zip using clean path.&quot;&quot;&quot;\n        zip_path = path_mapping.get(clean_path, clean_path)\n        with zip_file.open(zip_path) as f:\n            return f.read().decode(&quot;utf-8&quot;)\n\n    return file_reader\n\n\ndef flatten_github_repo(github_url: str) -&gt; tuple[str, dict]:\n    &quot;&quot;&quot;\n    Flatten a GitHub repository into XML format.\n\n    Args:\n        github_url: GitHub repository URL\n\n    Returns:\n        A tuple of:\n            XML string representation of the repository\n            Metadata dictionary with keys: url, domain, title, published_date\n\n    Raises:\n        Exception: If the repository cannot be fetched or processed\n    &quot;&quot;&quot;\n    # Convert URL to API endpoint\n    repo_url, metadata = parse_github_url(github_url)\n\n    # Download and process the repository\n    with download_github_repo(repo_url) as zip_file:\n        # Extract project name\n        project_name = get_repo_name_from_zip(zip_file)\n\n        # Create iterator and reader functions\n        path_iterator = get_github_file_iterator(zip_file)\n        file_reader = create_github_file_reader(zip_file)\n\n        # Generate XML\n        return package_to_xml(project_name, file_reader, path_iterator), metadata\n</file>\n    <file path=\"src/siphon/ingestion/github/retrieve_github.py\">from siphon.ingestion.github.flatten_url import flatten_github_repo\n\n\ndef retrieve_github(url: str) -&gt; tuple[str, dict]:\n    &quot;&quot;&quot;\n    Retrieve and flatten a GitHub repository URL.\n\n    Args:\n        url: GitHub repository URL\n    Returns:\n        A tuple containing:\n            Flattened XML string of the repository\n            Metadata dictionary with repository details\n    &quot;&quot;&quot;\n    llm_context, metadata = flatten_github_repo(url)\n    return llm_context, metadata\n</file>\n    <file path=\"src/siphon/ingestion/arxiv/retrieve_arxiv.py\"/>\n    <file path=\"src/siphon/prompts/synthetic_data_prompts.py\">&quot;&quot;&quot;\nThis module dynamically loads all SyntheticData prompts for each SourceType defined in the siphon.data.types.source_type enum.\n\nFor each sourcetype, we have a dict with the following:\n- `stem`: The lowercase name of the SourceType.\n- `title_prompt`: Path to the title prompt file.\n- `description_prompt`: Path to the description prompt file.\n- `summary_prompt`: Path to the summary prompt file.\n&quot;&quot;&quot;\n\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom pathlib import Path\n\nsynthetic_data_prompts_dir = Path(__file__).parent / &quot;synthetic_data&quot;\nprompts = synthetic_data_prompts_dir.glob(&quot;*.jinja2&quot;)\n\nSyntheticDataPrompts = []\nfor sourcetype in SourceType:\n    stem = sourcetype.value.lower()\n    title_prompt = synthetic_data_prompts_dir / f&quot;{stem}_title.jinja2&quot;\n    description_prompt = synthetic_data_prompts_dir / f&quot;{stem}_description.jinja2&quot;\n    summary_prompt = synthetic_data_prompts_dir / f&quot;{stem}_summary.jinja2&quot;\n    if (\n        not title_prompt.exists()\n        or not description_prompt.exists()\n        or not summary_prompt.exists()\n    ):\n        raise FileNotFoundError(\n            f&quot;Missing prompt files for {sourcetype.name}. &quot;\n            f&quot;Expected: {title_prompt}, {description_prompt}, {summary_prompt}&quot;\n        )\n    SyntheticDataPrompts.append(\n        {\n            &quot;stem&quot;: stem,\n            &quot;title_prompt&quot;: title_prompt,\n            &quot;description_prompt&quot;: description_prompt,\n            &quot;summary_prompt&quot;: summary_prompt,\n        }\n    )\n</file>\n    <file path=\"src/siphon/uri/uri_classes.py\">&quot;&quot;&quot;\nThis module dynamically loads all URI handler classes for each SourceType defined in siphon.\n\nEach SourceType enum member (e.g. SourceType.YOUTUBE) is expected to have:\n- A corresponding module located at: `siphon.uri.{lowercase_name}_uri.py`\n  (e.g. `siphon.uri.youtube_uri` for SourceType.YOUTUBE)\n- A class defined in that module named `{SourceType.value}URI`\n  (e.g. `YouTubeURI` for &quot;YouTube&quot;, `GitHubURI` for &quot;GitHub&quot;)\n\nThis script:\n1. Iterates through all values of the SourceType enum.\n2. Dynamically imports each corresponding URI class.\n3. Validates that the class exists and inherits from the base `URI` class.\n4. Builds a list of all successfully loaded URI subclasses as `URIClasses`.\n\nIf any SourceType is missing a corresponding module or class, or if the class does not inherit from URI, an informative error is raised.\n\nThis ensures strict one-to-one enforcement between defined source types and implemented URI resolvers.\n&quot;&quot;&quot;\n\nimport importlib\nfrom siphon.data.uri import URI\nfrom siphon.data.type_definitions.source_type import SourceType\n\n\ndef load_uri_class(source_type: SourceType) -&gt; type[URI]:\n    &quot;&quot;&quot;\n    Dynamically import the URI class corresponding to a SourceType.\n    Assumes:\n      - module path: siphon.uri.{source_type.name.lower()}_uri\n      - class name: {source_type.value}URI\n    &quot;&quot;&quot;\n    module_name = f&quot;siphon.uri.classes.{source_type.name.lower()}_uri&quot;\n    class_name = f&quot;{source_type.value}URI&quot;\n\n    try:\n        module = importlib.import_module(module_name)\n    except ModuleNotFoundError as e:\n        raise ImportError(\n            f&quot;Missing module for SourceType.{source_type.name}.\\n&quot;\n            f&quot;Expected file: `{module_name.replace('.', '/')}.py`&quot;\n        ) from e\n\n    try:\n        uri_class = getattr(module, class_name)\n    except AttributeError as e:\n        raise ImportError(\n            f&quot;Module `{module_name}` exists but does not define `{class_name}`.\\n&quot;\n            f&quot;Please ensure the class name matches and is defined.&quot;\n        ) from e\n\n    if not issubclass(uri_class, URI):\n        raise TypeError(f&quot;{class_name} exists but does not inherit from `URI`.&quot;)\n\n    return uri_class\n\n\n# Build the list of valid URI subclasses\nURIClasses = [load_uri_class(st) for st in SourceType]\n</file>\n    <file path=\"src/siphon/uri/classes/doc_uri.py\">from siphon.data.uri import URI\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.logs.logging_config import get_logger\nfrom siphon.data.type_definitions.uri_schemes import URISchemes\nfrom siphon.data.type_definitions.extensions import Extensions\nfrom pydantic import Field\nfrom typing import override\nfrom pathlib import Path\n\nlogger = get_logger(__name__)\n\n\nclass DocURI(URI):\n    &quot;&quot;&quot;\n    Represents an article URI with metadata.\n    Inherits from Metadata to include additional metadata fields.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = Field(\n        default=SourceType.DOC,\n        description=&quot;The type of source this URI represents.&quot;,\n    )\n\n    @override\n    @classmethod\n    def identify(cls, source: str) -&gt; bool:\n        &quot;&quot;&quot;\n        Check if the source string matches the doc URI format.\n        &quot;&quot;&quot;\n        try:\n            if &quot;http&quot; in source or &quot;https&quot; in source:\n                logger.info(f&quot;This is a URL, not a DocURI: {source}&quot;)\n                return False\n            source_path = Path(source)\n            extension = source_path.suffix.lower()\n            if extension in Extensions[&quot;Doc&quot;]:\n                logger.info(f&quot;Identified as DocURI: {source}&quot;)\n                return True\n            return False\n        except Exception:\n            return False\n\n    @override\n    @classmethod\n    def from_source(cls, source: str, skip_checksum: bool = False) -&gt; &quot;DocURI | None&quot;:\n        &quot;&quot;&quot;\n        Create a DocURI object from a source string.\n        &quot;&quot;&quot;\n        if not cls.identify(source):\n            logger.warning(f&quot;Source does not match DocURI format: {source}&quot;)\n            return None\n\n        # Coerce to Path if a string is provided\n        source_path = Path(source) if isinstance(source, str) else source_path\n\n        # Calculate checksum\n        checksum = None\n        if not skip_checksum:\n            logger.info(f&quot;Calculating checksum for: {source_path}&quot;)\n            checksum = cls.get_checksum(source_path)\n            logger.info(f&quot;Checksum calculated: {checksum}&quot;)\n\n        # Always convert to absolute path for consistency\n        absolute_source = str(Path(source).resolve())\n\n        return cls(\n            source=absolute_source,\n            uri=f&quot;{URISchemes['Doc']}://{Path(absolute_source).as_posix()}&quot;,\n            checksum=checksum,\n        )\n</file>\n    <file path=\"src/siphon/uri/classes/obsidian_uri.py\">from siphon.data.uri import URI\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom pydantic import Field\nfrom typing import override\n\n\nclass ObsidianURI(URI):\n    &quot;&quot;&quot;\n    Represents an article URI with metadata.\n    Inherits from Metadata to include additional metadata fields.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = Field(\n        default=SourceType.OBSIDIAN,\n        description=&quot;The type of source this URI represents.&quot;,\n    )\n\n    @override\n    @classmethod\n    def identify(cls, source: str) -&gt; bool: ...\n\n    @override\n    @classmethod\n    def from_source(\n        cls, source: str, skip_checksum: bool = False\n    ) -&gt; &quot;ObsidianURI | None&quot;:\n        &quot;&quot;&quot;\n        Create an ArticleURI object from a source string.\n        &quot;&quot;&quot;\n        ...\n</file>\n    <file path=\"src/siphon/uri/classes/email_uri.py\">from siphon.data.uri import URI\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom pydantic import Field\nfrom typing import override\n\n\nclass EmailURI(URI):\n    &quot;&quot;&quot;\n    Represents an article URI with metadata.\n    Inherits from Metadata to include additional metadata fields.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = Field(\n        default=SourceType.EMAIL,\n        description=&quot;The type of source this URI represents.&quot;,\n    )\n\n    @override\n    @classmethod\n    def identify(cls, source: str) -&gt; bool: ...\n\n    @override\n    @classmethod\n    def from_source(cls, source: str, skip_checksum: bool = False) -&gt; &quot;EmailURI&quot;:\n        &quot;&quot;&quot;\n        Create an ArticleURI object from a source string.\n        &quot;&quot;&quot;\n        _ = skip_checksum  # currently unused\n        ...\n</file>\n    <file path=\"src/siphon/uri/classes/github_uri.py\">from siphon.data.uri import URI\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.type_definitions.uri_schemes import URISchemes\nfrom siphon.logs.logging_config import get_logger\nfrom urllib.parse import urlparse\nfrom pydantic import Field\nfrom typing import override\n\nlogger = get_logger(__name__)\n\n\nclass GitHubURI(URI):\n    &quot;&quot;&quot;\n    Represents an article URI with metadata.\n    Inherits from Metadata to include additional metadata fields.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = Field(\n        default=SourceType.GITHUB,\n        description=&quot;The type of source this URI represents.&quot;,\n    )\n\n    @override\n    @classmethod\n    def identify(cls, source: str) -&gt; bool:\n        &quot;&quot;&quot;\n        Check if the source string matches the GitHub URI format.\n        &quot;&quot;&quot;\n        return &quot;github.com&quot; in source\n\n    @override\n    @classmethod\n    def from_source(\n        cls, source: str, skip_checksum: bool = False\n    ) -&gt; &quot;GitHubURI | None&quot;:  # type: ignore\n        &quot;&quot;&quot;\n        Create an GitHubURI object from a source string.\n        &quot;&quot;&quot;\n        _ = skip_checksum  # Unused for GitHub URIs\n        parsed = urlparse(source)\n        path_parts = parsed.path.strip(&quot;/&quot;).split(&quot;/&quot;)\n\n        if len(path_parts) &lt; 2:\n            raise ValueError(&quot;Invalid GitHub URL format - missing owner/repo&quot;)\n\n        owner = path_parts[0]\n        repo = path_parts[1]\n\n        # Handle different GitHub URL formats:\n        # https://github.com/owner/repo\n        # https://github.com/owner/repo/blob/branch/file.py\n        # https://github.com/owner/repo/tree/branch/path\n\n        if len(path_parts) &gt; 3 and path_parts[2] in [&quot;blob&quot;, &quot;tree&quot;]:\n            # Format: /owner/repo/blob/branch/path/to/file\n            file_path = &quot;/&quot;.join(path_parts[4:]) if len(path_parts) &gt; 4 else &quot;&quot;\n        elif len(path_parts) &gt; 2:\n            # Format: /owner/repo/path/to/file (direct path)\n            file_path = &quot;/&quot;.join(path_parts[2:])\n        else:\n            # Format: /owner/repo (no file path)\n            file_path = &quot;&quot;\n\n        # Fix 3: Don't add trailing slash when no file path\n        uri = f&quot;{URISchemes['GitHub']}://{owner}/{repo}&quot;\n        if file_path:\n            uri += f&quot;/{file_path}&quot;\n\n        # Add fragment if present (for line numbers, etc.)\n        if parsed.fragment:\n            uri += f&quot;#{parsed.fragment}&quot;\n\n        return cls(\n            source=source,\n            uri=uri,\n        )\n</file>\n    <file path=\"src/siphon/uri/classes/image_uri.py\">from siphon.data.uri import URI\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.type_definitions.extensions import Extensions\nfrom siphon.data.type_definitions.uri_schemes import URISchemes\nfrom siphon.logs.logging_config import get_logger\nfrom pydantic import Field\nfrom typing import override\nfrom pathlib import Path\n\n\nlogger = get_logger(__name__)\n\n\nclass ImageURI(URI):\n    &quot;&quot;&quot;\n    Represents an article URI with metadata.\n    Inherits from Metadata to include additional metadata fields.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = Field(\n        default=SourceType.IMAGE,\n        description=&quot;The type of source this URI represents.&quot;,\n    )\n\n    @override\n    @classmethod\n    def identify(cls, source: str) -&gt; bool:\n        &quot;&quot;&quot;\n        Check if the source string matches the image URI format.\n        &quot;&quot;&quot;\n        try:\n            if &quot;http&quot; in source or &quot;https&quot; in source:\n                logger.info(f&quot;This is a URL, not an ImageURI: {source}&quot;)\n                return False\n            source_path = Path(source)\n            extension = source_path.suffix.lower()\n            if extension in Extensions[&quot;Image&quot;]:\n                logger.info(f&quot;Identified as ImageURI: {source}&quot;)\n                return True\n            return False\n        except Exception:\n            return False\n\n    @override\n    @classmethod\n    def from_source(cls, source: str, skip_checksum: bool = False) -&gt; &quot;ImageURI | None&quot;:\n        &quot;&quot;&quot;\n        Create an ImageURI object from a source string.\n        &quot;&quot;&quot;\n        if not cls.identify(source):\n            logger.warning(f&quot;Source does not match ImageURI format: {source}&quot;)\n            return None\n\n        # Coerce to Path if a string is provided\n        source_path = Path(source) if isinstance(source, str) else source_path\n\n        # Calculate checksum\n        checksum = None\n        if not skip_checksum:\n            logger.info(f&quot;Calculating checksum for: {source_path}&quot;)\n            checksum = cls.get_checksum(source_path)\n            logger.info(f&quot;Checksum calculated: {checksum}&quot;)\n\n        # Always convert to absolute path for consistency\n        absolute_source = str(Path(source).resolve())\n\n        return cls(\n            source=absolute_source,\n            uri=f&quot;{URISchemes['Image']}://{Path(absolute_source).as_posix()}&quot;,\n            checksum=checksum,\n        )\n</file>\n    <file path=\"src/siphon/uri/classes/video_uri.py\">from siphon.data.uri import URI\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.type_definitions.extensions import Extensions\nfrom siphon.data.type_definitions.uri_schemes import URISchemes\nfrom siphon.logs.logging_config import get_logger\nfrom pydantic import Field\nfrom typing import override\nfrom pathlib import Path\n\nlogger = get_logger(__name__)\n\n\nclass VideoURI(URI):\n    &quot;&quot;&quot;\n    Represents an article URI with metadata.\n    Inherits from Metadata to include additional metadata fields.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = Field(\n        default=SourceType.VIDEO,\n        description=&quot;The type of source this URI represents.&quot;,\n    )\n\n    @override\n    @classmethod\n    def identify(cls, source: str) -&gt; bool:\n        &quot;&quot;&quot;\n        Check if the source string matches the video URI format.\n        &quot;&quot;&quot;\n        try:\n            if &quot;http&quot; in source or &quot;https&quot; in source:\n                logger.info(f&quot;This is a URL, not a VideoURI: {source}&quot;)\n                return False\n            source_path = Path(source)\n            extension = source_path.suffix.lower()\n            if extension in Extensions[&quot;Video&quot;]:\n                logger.info(f&quot;Identified as VideoURI: {source}&quot;)\n                return True\n            return False\n        except Exception:\n            return False\n\n    @override\n    @classmethod\n    def from_source(cls, source: str, skip_checksum: bool = False) -&gt; &quot;VideoURI | None&quot;:\n        &quot;&quot;&quot;\n        Create a VideoURI object from a source string.\n        &quot;&quot;&quot;\n        if not cls.identify(source):\n            logger.warning(f&quot;Source does not match VideoURI format: {source}&quot;)\n            return None\n\n        # Coerce to Path if a string is provided\n        source_path = Path(source) if isinstance(source, str) else source_path\n\n        # Calculate checksum\n        checksum = None\n        if not skip_checksum:\n            logger.info(f&quot;Calculating checksum for: {source_path}&quot;)\n            checksum = cls.get_checksum(source_path)\n            logger.info(f&quot;Checksum calculated: {checksum}&quot;)\n\n        # Always convert to absolute path for consistency\n        absolute_source = str(Path(source).resolve())\n\n        return cls(\n            source=absolute_source,\n            uri=f&quot;{URISchemes['Video']}://{Path(absolute_source).as_posix()}&quot;,\n            checksum=checksum,\n        )\n</file>\n    <file path=\"src/siphon/uri/classes/drive_uri.py\">from siphon.data.uri import URI\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom pydantic import Field\nimport re\nfrom typing import override\n\n\nclass DriveURI(URI):\n    &quot;&quot;&quot;\n    Represents an article URI with metadata.\n    Inherits from Metadata to include additional metadata fields.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = Field(\n        default=SourceType.DRIVE,\n        description=&quot;The type of source this URI represents.&quot;,\n    )\n\n    @override\n    @classmethod\n    def identify(cls, source: str) -&gt; bool:\n        ...\n        # def is_drive_url(url: str) -&gt; bool:\n        #     # Fix 2: Include docs.google.com in detection\n        #     return (\n        #         &quot;docs.google.com&quot; in url\n        #         or &quot;drive.google.com&quot; in url\n        #         or &quot;drive.googleusercontent.com&quot; in url\n        #     )\n\n    @override\n    @classmethod\n    def from_source(cls, source: str, skip_checksum: bool = False) -&gt; &quot;DriveURI | None&quot;:\n        &quot;&quot;&quot;\n        Create an ArticleURI object from a source string.\n        &quot;&quot;&quot;\n        _ = skip_checksum  # currently unused\n        ...\n\n    @classmethod\n    def _parse_drive_url(cls, url: str) -&gt; tuple[str, SourceType, str]:\n        &quot;&quot;&quot;\n        Parse Google Drive/Docs URLs - Fix 5: Extract file ID properly\n        &quot;&quot;&quot;\n\n        # Google URLs format: https://docs.google.com/document/d/FILE_ID/edit\n        # Or: https://drive.google.com/file/d/FILE_ID/view\n\n        # Extract file ID using regex\n        file_id_pattern = r&quot;/d/([a-zA-Z0-9-_]+)&quot;\n        file_id_match = re.search(file_id_pattern, url)\n\n        if not file_id_match:\n            raise ValueError(f&quot;Cannot extract file ID from Google URL: {url}&quot;)\n\n        file_id = file_id_match.group(1)\n\n        # Determine file type from URL path\n        if &quot;/spreadsheets/&quot; in url:\n            file_type = &quot;sheet&quot;\n        elif &quot;/presentation/&quot; in url:\n            file_type = &quot;slide&quot;\n        elif &quot;/document/&quot; in url:\n            file_type = &quot;doc&quot;\n        elif &quot;/forms/&quot; in url:\n            file_type = &quot;form&quot;\n        else:\n            # Default to doc if we can't determine\n            file_type = &quot;doc&quot;\n\n        uri = f&quot;drive://{file_type}/{file_id}&quot;\n        return url, SourceType.DRIVE, uri\n</file>\n    <file path=\"src/siphon/uri/classes/audio_uri.py\">from siphon.data.uri import URI\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.type_definitions.extensions import Extensions\nfrom siphon.data.type_definitions.uri_schemes import URISchemes\nfrom siphon.logs.logging_config import get_logger\nfrom pydantic import Field\nfrom typing import override\nfrom pathlib import Path\n\nlogger = get_logger(__name__)\n\n\nclass AudioURI(URI):\n    &quot;&quot;&quot;\n    Represents an article URI with metadata.\n    Inherits from Metadata to include additional metadata fields.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = Field(\n        default=SourceType.AUDIO,\n        description=&quot;The type of source this URI represents.&quot;,\n    )\n\n    @override\n    @classmethod\n    def identify(cls, source: str) -&gt; bool:\n        &quot;&quot;&quot;\n        Check if the source string matches the audio URI format.\n        &quot;&quot;&quot;\n        try:\n            if &quot;http&quot; in source or &quot;https&quot; in source:\n                logger.info(f&quot;This is a URL, not an AudioURI: {source}&quot;)\n                return False\n            source_path = Path(source)\n            extension = source_path.suffix.lower()\n            if extension in Extensions[&quot;Audio&quot;]:\n                logger.info(f&quot;Identified as AudioURI: {source}&quot;)\n                return True\n            return False\n        except Exception:\n            return False\n\n    @override\n    @classmethod\n    def from_source(cls, source: str, skip_checksum: bool = False) -&gt; &quot;AudioURI | None&quot;:\n        &quot;&quot;&quot;\n        Create an AudioURI object from a source string.\n        &quot;&quot;&quot;\n        if not cls.identify(source):\n            logger.warning(f&quot;Source does not match AudioURI format: {source}&quot;)\n            return None\n\n        # Coerce to Path if a string is provided\n        source_path = Path(source) if isinstance(source, str) else source_path\n\n        # Calculate checksum\n        checksum = None\n        if not skip_checksum:\n            logger.info(f&quot;Calculating checksum for: {source_path}&quot;)\n            checksum = cls.get_checksum(source_path)\n            logger.info(f&quot;Checksum calculated: {checksum}&quot;)\n\n        # Always convert to absolute path for consistency\n        absolute_source = str(Path(source).resolve())\n\n        return cls(\n            source=absolute_source,\n            uri=f&quot;{URISchemes['Audio']}://{Path(absolute_source).as_posix()}&quot;,\n            checksum=checksum,\n        )\n</file>\n    <file path=\"src/siphon/uri/classes/youtube_uri.py\">from siphon.data.uri import URI\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.type_definitions.uri_schemes import URISchemes\nfrom pydantic import Field\nfrom urllib.parse import urlparse, parse_qs\nimport re\n\n\nfrom typing import override\n\n\nclass YouTubeURI(URI):\n    &quot;&quot;&quot;\n    Represents an article URI with metadata.\n    Inherits from Metadata to include additional metadata fields.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = Field(\n        default=SourceType.YOUTUBE,\n        description=&quot;The type of source this URI represents.&quot;,\n    )\n\n    @override\n    @classmethod\n    def identify(cls, source: str) -&gt; bool:\n        return &quot;youtube.com&quot; in source or &quot;youtu.be&quot; in source\n\n    @override\n    @classmethod\n    def from_source(\n        cls, source: str, skip_checksum: bool = False\n    ) -&gt; &quot;YouTubeURI | None&quot;:  # type: ignore\n        &quot;&quot;&quot;\n        Create an YouTube object from a source string.\n        &quot;&quot;&quot;\n        _ = skip_checksum  # Unused parameter\n        # Handle different YouTube URL formats:\n        # https://www.youtube.com/watch?v=VIDEO_ID\n        # https://youtu.be/VIDEO_ID\n        # https://youtube.com/watch?v=VIDEO_ID&amp;t=123\n        if &quot;youtu.be/&quot; in source:\n            # Short format: https://youtu.be/VIDEO_ID\n            video_id = source.split(&quot;youtu.be/&quot;)[1].split(&quot;?&quot;)[0].split(&quot;&amp;&quot;)[0]\n        else:\n            # Long format: extract from query parameter\n            parsed = urlparse(source)\n            query_params = parse_qs(parsed.query)\n\n            if &quot;v&quot; not in query_params:\n                raise ValueError(f&quot;Cannot find video ID in YouTube URL: {source}&quot;)\n\n            video_id = query_params[&quot;v&quot;][0]\n\n        # Validate video ID format (11 characters, alphanumeric + - and _)\n        if not re.match(r&quot;^[a-zA-Z0-9_-]{11}$&quot;, video_id):\n            raise ValueError(f&quot;Invalid YouTube video ID format: {video_id}&quot;)\n\n        uri = f&quot;{URISchemes['YouTube']}://{video_id}&quot;\n        return cls(\n            source=source,\n            uri=uri,\n        )\n</file>\n    <file path=\"src/siphon/uri/classes/article_uri.py\">from siphon.data.uri import URI\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.type_definitions.uri_schemes import URISchemes\nfrom siphon.logs.logging_config import get_logger\nfrom pydantic import Field\nfrom typing import override\nimport re\n\nlogger = get_logger(__name__)\n\n\nclass ArticleURI(URI):\n    &quot;&quot;&quot;\n    Represents an article URI with metadata.\n    Inherits from Metadata to include additional metadata fields.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = Field(\n        default=SourceType.ARTICLE,\n        description=&quot;The type of source this URI represents.&quot;,\n    )\n\n    @override\n    @classmethod\n    def identify(cls, source: str) -&gt; bool:\n        &quot;&quot;&quot;\n        &quot;Article&quot; type is a catchall for any generic article URL.\n        We just want to validate that the source is a URL, and that it is not:\n        - github\n        - youtube\n        - drive\n        &quot;&quot;&quot;\n        is_url_regex = re.compile(r&quot;^(https?://)?([a-zA-Z0-9.-]+)(:[0-9]+)?(/.*)?$&quot;)\n        is_article = re.compile(r&quot;^(?!.*(github|youtube|drive)).*$&quot;, re.IGNORECASE)\n        if is_url_regex.match(source) and is_article.match(source):\n            return True\n\n    @override\n    @classmethod\n    def from_source(cls, source: str, skip_checksum: bool = False) -&gt; &quot;ArticleURI&quot;:  # type: ignore\n        &quot;&quot;&quot;\n        Create an ArticleURI object from a source string.\n        &quot;&quot;&quot;\n        _ = skip_checksum  # currently unused\n        if not cls.identify(source):\n            raise ValueError(f&quot;Source does not match ArticleURI format: {source}&quot;)\n        return cls(\n            source=source,\n            uri=f&quot;{URISchemes['Article']}://{source}&quot;,\n        )\n</file>\n    <file path=\"src/siphon/uri/classes/text_uri.py\">from siphon.data.uri import URI\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.logs.logging_config import get_logger\nfrom siphon.data.type_definitions.uri_schemes import URISchemes\nfrom siphon.data.type_definitions.extensions import Extensions\nfrom pydantic import Field\nfrom typing import override\nfrom pathlib import Path\n\nlogger = get_logger(__name__)\n\n\nclass TextURI(URI):\n    &quot;&quot;&quot;\n    Represents an article URI with metadata.\n    Inherits from Metadata to include additional metadata fields.\n    Includes a hash as an additional cache key. (since we may have multiple files with the same name across different directories of hosts.)\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = Field(\n        default=SourceType.TEXT,\n    )\n\n    @override\n    @classmethod\n    def identify(cls, source: str) -&gt; bool:\n        &quot;&quot;&quot;\n        Check if the source string matches the text URI format.\n        No file access required - just check the extension.\n        &quot;&quot;&quot;\n        try:\n            if &quot;http&quot; in source or &quot;https&quot; in source:\n                logger.info(f&quot;This is a URL, not an TextURI: {source}&quot;)\n                return False\n            source_path = Path(source)\n            extension = source_path.suffix.lower()\n            if extension in Extensions[&quot;Text&quot;]:\n                logger.info(f&quot;Identified as TextURI: {source}&quot;)\n                return True\n            return False\n        except Exception:\n            return False\n\n    @override\n    @classmethod\n    def from_source(cls, source: str, skip_checksum: bool = False) -&gt; &quot;TextURI | None&quot;:\n        &quot;&quot;&quot;\n        Create a TextURI object from a source string.\n        &quot;&quot;&quot;\n        if not cls.identify(source):\n            logger.warning(f&quot;Source does not match TextURI format: {source}&quot;)\n            return None\n\n        # Coerce to Path if a string is provided\n        source_path = Path(source) if isinstance(source, str) else source_path\n\n        # Calculate checksum\n        checksum = None\n        if not skip_checksum:\n            logger.info(f&quot;Calculating checksum for: {source_path}&quot;)\n            checksum = cls.get_checksum(source_path)\n            logger.info(f&quot;Checksum calculated: {checksum}&quot;)\n\n        # Always convert to absolute path for consistency\n        absolute_source = str(Path(source).resolve())\n\n        logger.info(f&quot;Creating File URI object for: {absolute_source}&quot;)\n\n        return cls(\n            source=absolute_source,\n            uri=f&quot;{URISchemes['Text']}://{Path(absolute_source).as_posix()}&quot;,\n            checksum=checksum,\n        )\n</file>\n    <file path=\"src/siphon/synthetic_data/synthetic_data_classes.py\">import importlib\nfrom siphon.data.synthetic_data import SyntheticData\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.logs.logging_config import get_logger\nfrom typing import Union\n\nlogger = get_logger(__name__)\n\n\ndef load_synthetic_data_class(source_type: SourceType) -&gt; type[SyntheticData]:\n    &quot;&quot;&quot;\n    Dynamically import the SyntheticData class corresponding to a SourceType.\n    Assumes:\n      - module path: siphon.synthetic_data.classes.{source_type.name.lower()}_synthetic_data\n      - class name: {source_type.value}SyntheticData\n    &quot;&quot;&quot;\n    module_name = (\n        f&quot;siphon.synthetic_data.classes.{source_type.name.lower()}_synthetic_data&quot;\n    )\n    class_name = f&quot;{source_type.value}SyntheticData&quot;\n\n    try:\n        module = importlib.import_module(module_name)\n    except ModuleNotFoundError as e:\n        raise ImportError(\n            f&quot;Missing module for SourceType.{source_type.name}.\\n&quot;\n            f&quot;Expected file: `{module_name.replace('.', '/')}.py`&quot;\n        ) from e\n\n    try:\n        synthetic_data_class = getattr(module, class_name)\n    except AttributeError as e:\n        raise ImportError(\n            f&quot;Module `{module_name}` exists but does not define `{class_name}`.\\n&quot;\n            f&quot;Please ensure the class name matches and is defined.&quot;\n        ) from e\n\n    if not issubclass(synthetic_data_class, SyntheticData):\n        raise TypeError(f&quot;{class_name} exists but does not inherit from `URI`.&quot;)\n\n    return synthetic_data_class\n\n\n# Build the list of valid URI subclasses\nSyntheticDataClasses = [load_synthetic_data_class(st) for st in SourceType]\n\n# A union type for all SyntheticData classes\nSyntheticDataUnion = Union[tuple(SyntheticDataClasses)]\n</file>\n    <file path=\"src/siphon/synthetic_data/classes/email_synthetic_data.py\">from siphon.data.type_definitions.source_type import SourceType\nfrom siphon.synthetic_data.classes.text_synthetic_data import TextSyntheticData\n\n\nclass EmailSyntheticData(TextSyntheticData):\n    &quot;&quot;&quot;\n    AI-generated enrichments, applied as a &quot;finishing step&quot; to the content.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.EMAIL\n</file>\n    <file path=\"src/siphon/synthetic_data/classes/github_synthetic_data.py\">from siphon.data.type_definitions.source_type import SourceType\nfrom siphon.synthetic_data.classes.text_synthetic_data import TextSyntheticData\n\n\nclass GitHubSyntheticData(TextSyntheticData):\n    &quot;&quot;&quot;\n    AI-generated enrichments, applied as a &quot;finishing step&quot; to the content.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.GITHUB\n</file>\n    <file path=\"src/siphon/synthetic_data/classes/obsidian_synthetic_data.py\">from siphon.data.type_definitions.source_type import SourceType\nfrom siphon.synthetic_data.classes.text_synthetic_data import TextSyntheticData\n\n\nclass ObsidianSyntheticData(TextSyntheticData):\n    &quot;&quot;&quot;\n    AI-generated enrichments, applied as a &quot;finishing step&quot; to the content.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.OBSIDIAN\n</file>\n    <file path=\"src/siphon/synthetic_data/classes/article_synthetic_data.py\">from siphon.data.type_definitions.source_type import SourceType\nfrom siphon.synthetic_data.classes.text_synthetic_data import TextSyntheticData\n\n\nclass ArticleSyntheticData(TextSyntheticData):\n    &quot;&quot;&quot;\n    AI-generated enrichments, applied as a &quot;finishing step&quot; to the content.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.ARTICLE\n</file>\n    <file path=\"src/siphon/synthetic_data/classes/audio_synthetic_data.py\">from siphon.data.type_definitions.source_type import SourceType\nfrom siphon.synthetic_data.classes.text_synthetic_data import TextSyntheticData\n\n\nclass AudioSyntheticData(TextSyntheticData):\n    &quot;&quot;&quot;\n    AI-generated enrichments, applied as a &quot;finishing step&quot; to the content.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.AUDIO\n</file>\n    <file path=\"src/siphon/synthetic_data/classes/youtube_synthetic_data.py\">from siphon.data.type_definitions.source_type import SourceType\nfrom siphon.synthetic_data.classes.text_synthetic_data import TextSyntheticData\n\n\nclass YouTubeSyntheticData(TextSyntheticData):\n    &quot;&quot;&quot;\n    AI-generated enrichments, applied as a &quot;finishing step&quot; to the content.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.YOUTUBE\n</file>\n    <file path=\"src/siphon/synthetic_data/classes/video_synthetic_data.py\">from siphon.data.type_definitions.source_type import SourceType\nfrom siphon.synthetic_data.classes.text_synthetic_data import TextSyntheticData\n\n\nclass VideoSyntheticData(TextSyntheticData):\n    &quot;&quot;&quot;\n    AI-generated enrichments, applied as a &quot;finishing step&quot; to the content.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.VIDEO\n</file>\n    <file path=\"src/siphon/synthetic_data/classes/image_synthetic_data.py\">from siphon.data.type_definitions.source_type import SourceType\nfrom siphon.synthetic_data.classes.text_synthetic_data import TextSyntheticData\n\n\nclass ImageSyntheticData(TextSyntheticData):\n    &quot;&quot;&quot;\n    AI-generated enrichments, applied as a &quot;finishing step&quot; to the content.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.IMAGE\n</file>\n    <file path=\"src/siphon/synthetic_data/classes/text_synthetic_data.py\">&quot;&quot;&quot;\nInherit from this one if you don't need custom logic.\n&quot;&quot;&quot;\n\nfrom siphon.data.context import Context\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.synthetic_data import SyntheticData\nfrom typing import override\n\n\nclass TextSyntheticData(SyntheticData):\n    &quot;&quot;&quot;\n    AI-generated enrichments, applied as a &quot;finishing step&quot; to the content.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.TEXT\n\n    @override\n    @classmethod\n    def from_context(\n        cls, context: Context, model_str: str = &quot;gemini2.5&quot;\n    ) -&gt; &quot;TextSyntheticData&quot;:\n        &quot;&quot;&quot;\n        Create a TextSyntheticData instance from a Context object.\n        &quot;&quot;&quot;\n        cls._validate_context(context)\n        prompt_templates: dict = cls._source_prompts(context=context)\n        rendered_prompts: list[str] = cls._render_prompts(\n            context=context, prompt_templates=prompt_templates\n        )\n        # Run async conduits\n        from conduit.batch import AsyncConduit, ModelAsync\n        from rich.console import Console\n\n        ModelAsync._console = Console()\n        model = ModelAsync(model_str)\n        conduit = AsyncConduit(model=model)\n        responses = conduit.run(prompt_strings=rendered_prompts)\n        # Create the TextSyntheticData instance\n        title = responses[0].message.content.strip()\n        description = responses[1].message.content.strip()\n        summary = responses[2].message.content.strip()\n        return cls(\n            title=title,\n            description=description,\n            summary=summary,\n        )\n\n    @classmethod\n    def _validate_context(cls, context: Context) -&gt; None:\n        &quot;&quot;&quot;\n        Validate the context for TextSyntheticData.\n        &quot;&quot;&quot;\n        assert cls.__name__.replace(\n            &quot;SyntheticData&quot;, &quot;&quot;\n        ) == context.__class__.__name__.replace(&quot;Context&quot;, &quot;&quot;), (\n            f&quot;Expected context type {cls.__name__.replace('SyntheticData', '')}Context, &quot;\n            f&quot;got {context.__class__.__name__}.&quot;\n        )\n        return\n\n    @classmethod\n    def _source_prompts(cls, context: Context) -&gt; dict:\n        &quot;&quot;&quot;\n        Retrieve prompt templates.\n        &quot;&quot;&quot;\n        from siphon.prompts.synthetic_data_prompts import (\n            SyntheticDataPrompts,\n        )\n\n        stem = context.__class__.__name__.replace(&quot;Context&quot;, &quot;&quot;).lower()\n\n        for syntheticdataprompt in SyntheticDataPrompts:\n            if stem == syntheticdataprompt[&quot;stem&quot;].lower():\n                return syntheticdataprompt\n        raise ValueError(f&quot;No prompt templates found for context type {stem}.&quot;)\n\n    @classmethod\n    def _render_prompts(cls, context: Context, prompt_templates: dict) -&gt; list[str]:\n        &quot;&quot;&quot;\n        Render the prompts using the context.\n        &quot;&quot;&quot;\n        from jinja2 import Template\n\n        # Turn the prompt templates into a list, excluding the stem.\n        template_list = []\n        template_list.append(prompt_templates[&quot;title_prompt&quot;])\n        template_list.append(prompt_templates[&quot;description_prompt&quot;])\n        template_list.append(prompt_templates[&quot;summary_prompt&quot;])\n        # Render each template with the context data.\n        rendered_prompts = []\n        for template in template_list:\n            jinja_template = Template(template.read_text())\n            rendered_prompt = jinja_template.render(\n                context=context.model_dump_json(indent=2)\n            )\n            rendered_prompts.append(rendered_prompt)\n        return rendered_prompts\n</file>\n    <file path=\"src/siphon/synthetic_data/classes/drive_synthetic_data.py\">from siphon.data.type_definitions.source_type import SourceType\nfrom siphon.synthetic_data.classes.text_synthetic_data import TextSyntheticData\n\n\nclass DriveSyntheticData(TextSyntheticData):\n    &quot;&quot;&quot;\n    AI-generated enrichments, applied as a &quot;finishing step&quot; to the content.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.DRIVE\n</file>\n    <file path=\"src/siphon/synthetic_data/classes/doc_synthetic_data.py\">from siphon.data.context import Context\nfrom siphon.synthetic_data.classes.text_synthetic_data import TextSyntheticData\nfrom siphon.data.type_definitions.source_type import SourceType\nfrom siphon.data.synthetic_data import SyntheticData\nfrom typing import override\n\n\nclass DocSyntheticData(TextSyntheticData):\n    &quot;&quot;&quot;\n    AI-generated enrichments, applied as a &quot;finishing step&quot; to the content.\n    Inherits from TextSyntheticData to leverage text-based synthetic data generation.\n    &quot;&quot;&quot;\n\n    sourcetype: SourceType = SourceType.DOC\n</file>\n    <file path=\"src/siphon/enrich/generate_title.py\">from pathlib import Path\nfrom conduit.sync import Model, Prompt, Conduit\nfrom siphon.data.processed_content import ProcessedContent\n\ndir_path = Path(__file__).parent\nprompts_dir = dir_path.parent / &quot;prompts&quot;\ntitle_prompt_file = prompts_dir / &quot;enrich_title.jinja2&quot;\n\n\ndef generate_title(\n    processed_content: ProcessedContent, model: str = &quot;llama3.3:latest&quot;\n) -&gt; str:\n    # Get attributes from processed content\n    uri = processed_content.uri\n    llm_context = processed_content.llm_context\n    input_variables = {&quot;uri&quot;: uri, &quot;llm_context&quot;: llm_context}\n    # Build our conduit\n    prompt = Prompt(title_prompt_file.read_text())\n    model_obj = Model(model)\n    conduit = Conduit(prompt=prompt, model=model_obj)\n    response = conduit.run(input_variables=input_variables)\n    # Return the generated title\n    return str(response.content)\n</file>\n    <file path=\"src/siphon/enrich/enrich.py\">&quot;&quot;&quot;\nWrapper script for the various enrichment scripts. (title, description, summary)\n&quot;&quot;&quot;\n\nfrom siphon.data.processed_content import ProcessedContent\nfrom siphon.data.synthetic_data import SyntheticData\nfrom siphon.enrich.generate_title import generate_title\nfrom siphon.enrich.generate_description import generate_description\nfrom siphon.enrich.generate_summary import generate_summary\n\n\ndef enrich_content(\n    processed_content: ProcessedContent, model: str = &quot;cogito:32b&quot;\n) -&gt; SyntheticData:\n    &quot;&quot;&quot;\n    Enrich the processed content with title, description, and summary.\n    Returns a SyntheticData object containing all enrichments.\n    &quot;&quot;&quot;\n\n    # Generate title\n    print(&quot;Generating title...&quot;)\n    try:\n        title = generate_title(processed_content, model=model)\n    except Exception as e:\n        raise Exception(f&quot;Failed to generate title: {e}&quot;)\n\n    # Generate description\n    print(&quot;Generating description...&quot;)\n    try:\n        description = generate_description(processed_content, model=model)\n    except Exception as e:\n        raise Exception(f&quot;Failed to generate description: {e}&quot;)\n\n    # Generate summary\n    print(&quot;Generating summary...&quot;)\n    try:\n        summary = generate_summary(processed_content, model=model)\n    except Exception as e:\n        raise Exception(f&quot;Failed to generate summary: {e}&quot;)\n\n    # Create synthetic data object with all enrichments\n    synthetic_data = SyntheticData(\n        title=title,\n        description=description,\n        summary=summary,\n        topics=[],\n        entities=[],\n    )\n\n    return synthetic_data\n\n\nif __name__ == &quot;__main__&quot;:\n    from siphon.tests.fixtures.example_ProcessedContent import content\n\n    syntheticdata = enrich_content(content)\n    print(syntheticdata.model_dump_json(indent=2))\n</file>\n    <file path=\"src/siphon/enrich/generate_summary.py\">from pathlib import Path\nfrom conduit.sync import Model, Prompt, Conduit\nfrom siphon.data.processed_content import ProcessedContent\n\ndir_path = Path(__file__).parent\nprompts_dir = dir_path.parent / &quot;prompts&quot;\ntitle_prompt_file = prompts_dir / &quot;enrich_summary.jinja2&quot;\n\n\ndef calculate_target_summary_length(content_length: int) -&gt; tuple[int, int]:\n    &quot;&quot;&quot;\n    Calculate target summary length based on content length.\n    Returns (target_words, percentage)\n    &quot;&quot;&quot;\n    if content_length &lt; 300:\n        percentage = 70\n        target_words = int(content_length * percentage / 100)\n    elif content_length &lt; 1000:\n        percentage = 60  # Increased from 40%\n        target_words = int(content_length * percentage / 100)\n    elif content_length &lt; 3000:\n        percentage = 40  # Increased from 20%\n        target_words = int(content_length * percentage / 100)\n    elif content_length &lt; 8000:\n        percentage = 30  # Increased from 12%\n        target_words = int(content_length * percentage / 100)\n    else:\n        # Fixed length for very long content\n        target_words = 2000  # Your preferred cap\n        percentage = int((target_words / content_length) * 100)\n\n    target_words = max(50, target_words)  # Minimum 50 words\n    return target_words, percentage\n\n\ndef generate_summary(\n    processed_content: ProcessedContent, model: str = &quot;llama3.3:latest&quot;\n) -&gt; str:\n    # Get attributes from processed content\n    uri = processed_content.uri\n    llm_context = processed_content.llm_context\n    target_words, percentage = calculate_target_summary_length(len(llm_context.split()))\n    input_variables = {\n        &quot;uri&quot;: uri,\n        &quot;llm_context&quot;: llm_context,\n        &quot;target_length&quot;: str(target_words),\n        &quot;length_percentage&quot;: str(percentage),\n    }\n    # Build our conduit\n    prompt = Prompt(title_prompt_file.read_text())\n    model_obj = Model(model)\n    conduit = Conduit(prompt=prompt, model=model_obj)\n    response = conduit.run(input_variables=input_variables)\n    # Return the generated title\n    return str(response.content)\n</file>\n    <file path=\"src/siphon/enrich/test_runner.py\">&quot;&quot;&quot;\nEarly testing suggests cogito:32b and qwen2.5:32b are the best models for this use case.\n&quot;&quot;&quot;\n\nfrom siphon.enrich.enrich import enrich_content\nfrom siphon.tests.fixtures.example_ProcessedContent import dummy_content\nfrom time import time\n\nmodels = &quot;&quot;&quot;\ngemma3:27b\ncogito:32b\ncogito:14b\nphi4:14b\nqwen2.5:32b\nqwen2.5:14b\nqwen3:30b\nllama3.1:latest\nllama4:16x17b\nllama3.2:latest\ndeepseek-r1:32b\nmistral:latest\nmistral-nemo:12b\n&quot;&quot;&quot;.strip().split(&quot;\\n&quot;)\n\n\noutput = {}\nfor index, model in enumerate(models):\n    print(f&quot;Processing model {index + 1}/{len(models)}: {model}&quot;)\n    try:\n        start_time = time()\n        syntheticdata = enrich_content(content, model=model)\n        end_time = time()\n        duration = end_time - start_time\n        print(f&quot;Model {model} processed in {duration:.2f} seconds&quot;)\n        output[model] = {\n            &quot;llm_output&quot;: syntheticdata.model_dump_json(indent=2),\n            &quot;duration&quot;: duration,\n        }\n    except:\n        print(f&quot;Error processing model {model}&quot;)\n\nprompt_str = &quot;&quot;&quot;\nI've asked a set of Ollama models to generate synthetic data based on a given content.\nIn particular, a title, a description, and a summary.\n\nHere's the context I provided:\n&lt;content&gt;\n{{content}}\n&lt;/content&gt;\n\nHere's all the generated data from the models:\n&lt;model_outputs&gt;\n{{model_outputs}}\n&lt;/model_outputs&gt;\n\nPlease provide an assesment of the quality of the generated data, reviewing each model's output individually and identifying the best and worst models.\n&quot;&quot;&quot;\n\n\ndef analyze_output(output: dict, content, model: str = &quot;gemini2.5&quot;) -&gt; str:\n    from conduit.sync import Prompt, Model, Conduit\n    import json\n\n    model_outputs = json.dumps(output, indent=2)\n    content_str = content.model_dump_json(indent=2)\n    input_variables = {&quot;model_outputs&quot;: model_outputs, &quot;content&quot;: content_str}\n    model_obj = Model(model)\n    prompt = Prompt(prompt_str)\n    conduit = Conduit(model=model_obj, prompt=prompt)\n    response = conduit.run(input_variables=input_variables)\n    return str(response.content)\n\n\nanalysis = analyze_output(output, dummy_content, model=&quot;o4-mini&quot;)\nprint(analysis)\n</file>\n    <file path=\"src/siphon/enrich/generate_description.py\">from pathlib import Path\nfrom conduit.sync import Model, Prompt, Conduit\nfrom siphon.data.processed_content import ProcessedContent\n\ndir_path = Path(__file__).parent\nprompts_dir = dir_path.parent / &quot;prompts&quot;\ntitle_prompt_file = prompts_dir / &quot;enrich_description.jinja2&quot;\n\n\ndef generate_description(\n    processed_content: ProcessedContent, model: str = &quot;llama3.3:latest&quot;\n) -&gt; str:\n    # Get attributes from processed content\n    uri = processed_content.uri\n    llm_context = processed_content.llm_context\n    input_variables = {&quot;uri&quot;: uri, &quot;llm_context&quot;: llm_context}\n    # Build our conduit\n    prompt = Prompt(title_prompt_file.read_text())\n    model_obj = Model(model)\n    conduit = Conduit(prompt=prompt, model=model_obj)\n    response = conduit.run(input_variables=input_variables)\n    # Return the generated title\n    return str(response.content)\n\n\nif __name__ == &quot;__main__&quot;:\n    from siphon.tests.fixtures.example_ProcessedContent import content\n\n    title = generate_description(content)\n    print(f&quot;Generated Description: {title}&quot;)\n</file>\n  </file_contents>\n</project>\n\n</code>"
      }
    ],
    "temperature": null,
    "response_model": null,
    "provider": "google",
    "client_params": null,
    "stream": false,
    "verbose": 1
  },
  "input_tokens": 117255,
  "output_tokens": 647,
  "duration": 27.195930004119873,
  "timestamp": "2025-10-17T23:23:48.675260"
}