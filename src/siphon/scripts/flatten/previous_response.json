{
  "message": {
    "message_type": "text",
    "role": "assistant",
    "content": "This project, Conduit, is a unified abstraction layer for interacting with multiple Large Language Model (LLM) providers through a consistent Python interface. It standardizes both synchronous and asynchronous operations for text, image, and audio generation and analysis, abstracting away the provider-specific API formats and protocols. The library is designed around a set of Pydantic data models that represent the core entities of an LLM interaction: `Message`, `Request`, `Response`, and `ConduitError`. This structure ensures type safety, facilitates serialization for caching and history, and provides a clear data contract between internal components. The primary entry points are the `SyncConduit` and `AsyncConduit` orchestrator classes, which combine a `Prompt` (a Jinja2 template) and a `Model` to execute LLM calls.\n\nThe architecture is layered to separate concerns. The top-level `Conduit` classes manage the overall workflow, including prompt rendering and input validation. The `Model` class acts as a facade, determining the correct LLM provider (e.g., OpenAI, Anthropic, Ollama) by consulting the `ModelStore` registry. Based on the provider, it dispatches a standardized `Request` object to the appropriate provider-specific `Client` located in `model/clients/`. These clients are responsible for translating the generic `Request` into the provider's native API format (e.g., `request.to_openai()`, `request.to_anthropic()`), handling authentication, and executing the HTTP request. The `instructor` library is used under the hood to manage API calls, providing structured data parsing into Pydantic models, automatic retries, and validation.\n\nKey implementation details include a robust data flow and state management system. All parameters for an LLM call are encapsulated within a `Request` object, which simplifies the `Model.query` method signature and enables the generation of a deterministic cache key. Conversation history is managed by `MessageStore`, a list-like class that uses TinyDB for persistent storage in an XDG-compliant data directory. A separate system, the `Odometer`, tracks token usage through an event-driven architecture. Upon the creation of a `Response` object, a `TokenEvent` is emitted to the `OdometerRegistry`, a singleton which routes the event to session-level (in-memory) and persistent odometers. The persistent odometer relies on a PostgreSQL backend for long-term storage and analysis of usage data; the SQLite backend is not implemented. Concurrency is handled by `ModelAsync` and `AsyncConduit`, which leverage `asyncio` to execute batch operations in parallel, with optional `asyncio.Semaphore` support for rate limiting.\n\nSeveral technical decisions were made to handle specific constraints. The `ModelStore` system separates static model definitions (in `models.json`) from environment-specific ones like local Ollama models, which are discovered at runtime and stored in a state file. This avoids hardcoding local model availability. The `progress` module implements a tiered `Verbosity` enum and `DisplayMixin` classes, which decouple the core logic from the user-facing output. This allows `Request`, `Response`, and `ConduitError` objects to render themselves differently (from a simple spinner to a full debug JSON) based on the verbosity level, without cluttering the main application logic. A footgun exists in the `MessageStore` class, where standard list-modification methods like `append()` and `extend()` trigger immediate database writes, which can have performance implications if not handled carefully. The library also includes a full command-line interface (`conduit.cli`) and a `conduitml` YAML-based workflow definition system for composing multi-step LLM chains."
  },
  "request": {
    "model": "gemini-2.5-pro-preview-05-06",
    "messages": [
      {
        "message_type": "text",
        "role": "user",
        "content": "Write a technical README for this codebase. Follow these requirements strictly:\n\n1. NO icons, emojis, or decorative elements\n2. NO sections for Contributing, License, Acknowledgments, or similar boilerplate\n3. NO installation instructions unless the setup is non-trivial\n4. NO marketing language or enthusiasm\n\nStructure:\n- Start with a direct technical description (2-4 paragraphs)\n- Explain what the code does, how it works architecturally, and why key decisions were made\n- Use precise technical terminology\n- Focus on information density - every sentence should add value\n\nThe description should cover:\n- The core purpose and role in the larger system (if applicable)\n- Main technical phases/components and how they interact\n- Key implementation details (concurrency model, error handling, data flow)\n- Rationale for non-obvious technical decisions (e.g., \"uses X instead of Y because Z constraint\")\n- Any footguns, limitations, or \"tried this approach first but failed because...\" context\n- Dependencies on or relationships with other parts of the system\n- Intended use case and operational context\n\nWrite in a matter-of-fact tone as if documenting for your future self. Assume the reader is technically competent. Be specific about what the code actually does rather than what problems it theoretically solves.\n\nDo not include: usage examples, quickstart guides, or feature lists unless the code has genuine complexity that requires explanation.\n\nHere's the code project:\n<code>\n<?xml version=\"1.0\" ?>\n<project name=\"conduit\">\n  <directory_tree>\n    <directory name=\"conduit\">\n      <file name=\"batch.py\" path=\"batch.py\"/>\n      <file name=\"__init__.py\" path=\"__init__.py\"/>\n      <file name=\"sync.py\" path=\"sync.py\"/>\n      <directory name=\"scripts\">\n        <file name=\"update_ollama_list.py\" path=\"scripts/update_ollama_list.py\"/>\n        <file name=\"imagegen_cli.py\" path=\"scripts/imagegen_cli.py\"/>\n        <file name=\"conduit_cli.py\" path=\"scripts/conduit_cli.py\"/>\n        <file name=\"update_modelstore.py\" path=\"scripts/update_modelstore.py\"/>\n        <file name=\"__init__.py\" path=\"scripts/__init__.py\"/>\n        <file name=\"chat_cli.py\" path=\"scripts/chat_cli.py\"/>\n        <file name=\"models_cli.py\" path=\"scripts/models_cli.py\"/>\n      </directory>\n      <directory name=\"conduit\">\n        <file name=\"sync_conduit.py\" path=\"conduit/sync_conduit.py\"/>\n        <file name=\"async_conduit.py\" path=\"conduit/async_conduit.py\"/>\n        <file name=\"__init__.py\" path=\"conduit/__init__.py\"/>\n      </directory>\n      <directory name=\"cli\">\n        <file name=\"config_loader.py\" path=\"cli/config_loader.py\"/>\n        <file name=\"cli_class.py\" path=\"cli/cli_class.py\"/>\n        <file name=\"handlers.py\" path=\"cli/handlers.py\"/>\n        <file name=\"__init__.py\" path=\"cli/__init__.py\"/>\n        <file name=\"query_function.py\" path=\"cli/query_function.py\"/>\n        <file name=\"cli_factory.py\" path=\"cli/cli_factory.py\"/>\n      </directory>\n      <directory name=\"embeddings\">\n        <file name=\"embedding_model.py\" path=\"embeddings/embedding_model.py\"/>\n        <file name=\"chroma_batch.py\" path=\"embeddings/chroma_batch.py\"/>\n      </directory>\n      <directory name=\"parser\">\n        <file name=\"__init__.py\" path=\"parser/__init__.py\"/>\n        <file name=\"parser.py\" path=\"parser/parser.py\"/>\n      </directory>\n      <directory name=\"prompt\">\n        <file name=\"prompt_loader.py\" path=\"prompt/prompt_loader.py\"/>\n        <file name=\"__init__.py\" path=\"prompt/__init__.py\"/>\n        <file name=\"prompt.py\" path=\"prompt/prompt.py\"/>\n      </directory>\n      <directory name=\"logs\">\n        <file name=\"logging_config.py\" path=\"logs/logging_config.py\"/>\n      </directory>\n      <directory name=\"message\">\n        <file name=\"imagemessage.py\" path=\"message/imagemessage.py\"/>\n        <file name=\"messagestore.py\" path=\"message/messagestore.py\"/>\n        <file name=\"message.py\" path=\"message/message.py\"/>\n        <file name=\"convert_image.py\" path=\"message/convert_image.py\"/>\n        <file name=\"audiomessage.py\" path=\"message/audiomessage.py\"/>\n        <file name=\"__init__.py\" path=\"message/__init__.py\"/>\n        <file name=\"messages.py\" path=\"message/messages.py\"/>\n        <file name=\"textmessage.py\" path=\"message/textmessage.py\"/>\n        <file name=\"videomessage.py\" path=\"message/videomessage.py\"/>\n      </directory>\n      <directory name=\"chat\">\n        <file name=\"chat.py\" path=\"chat/chat.py\"/>\n        <file name=\"__init__.py\" path=\"chat/__init__.py\"/>\n        <file name=\"audio_chat.py\" path=\"chat/audio_chat.py\"/>\n      </directory>\n      <directory name=\"llm_decorator\">\n        <file name=\"decorator.py\" path=\"llm_decorator/decorator.py\"/>\n        <file name=\"__init__.py\" path=\"llm_decorator/__init__.py\"/>\n      </directory>\n      <directory name=\"compose\">\n        <file name=\"conduitml.py\" path=\"compose/conduitml.py\"/>\n        <file name=\"__init__.py\" path=\"compose/__init__.py\"/>\n        <file name=\"conduit_compose.py\" path=\"compose/conduit_compose.py\"/>\n      </directory>\n      <directory name=\"model\">\n        <file name=\"model.py\" path=\"model/model.py\"/>\n        <file name=\"model_async.py\" path=\"model/model_async.py\"/>\n        <file name=\"model_client.py\" path=\"model/model_client.py\"/>\n        <file name=\"__init__.py\" path=\"model/__init__.py\"/>\n        <directory name=\"clients\">\n          <file name=\"perplexity_content.py\" path=\"model/clients/perplexity_content.py\"/>\n          <file name=\"ollama_client.py\" path=\"model/clients/ollama_client.py\"/>\n          <file name=\"anthropic_client.py\" path=\"model/clients/anthropic_client.py\"/>\n          <file name=\"openai_client.py\" path=\"model/clients/openai_client.py\"/>\n          <file name=\"load_env.py\" path=\"model/clients/load_env.py\"/>\n          <file name=\"perplexity_client.py\" path=\"model/clients/perplexity_client.py\"/>\n          <file name=\"__init__.py\" path=\"model/clients/__init__.py\"/>\n          <file name=\"google_client.py\" path=\"model/clients/google_client.py\"/>\n          <file name=\"client.py\" path=\"model/clients/client.py\"/>\n        </directory>\n        <directory name=\"models\">\n          <file name=\"providerstore.py\" path=\"model/models/providerstore.py\"/>\n          <file name=\"research_models.py\" path=\"model/models/research_models.py\"/>\n          <file name=\"modelspec.py\" path=\"model/models/modelspec.py\"/>\n          <file name=\"modelspecs_CRUD.py\" path=\"model/models/modelspecs_CRUD.py\"/>\n          <file name=\"provider.py\" path=\"model/models/provider.py\"/>\n          <file name=\"validate_capabilities.py\" path=\"model/models/validate_capabilities.py\"/>\n          <file name=\"__init__.py\" path=\"model/models/__init__.py\"/>\n          <file name=\"providerspec.py\" path=\"model/models/providerspec.py\"/>\n          <file name=\"modelstore.py\" path=\"model/models/modelstore.py\"/>\n        </directory>\n      </directory>\n      <directory name=\"odometer\">\n        <file name=\"TokenEvent.py\" path=\"odometer/TokenEvent.py\"/>\n        <file name=\"SessionOdometer.py\" path=\"odometer/SessionOdometer.py\"/>\n        <file name=\"ConversationOdometer.py\" path=\"odometer/ConversationOdometer.py\"/>\n        <file name=\"PersistentOdometer.py\" path=\"odometer/PersistentOdometer.py\"/>\n        <file name=\"__init__.py\" path=\"odometer/__init__.py\"/>\n        <file name=\"OdometerRegistry.py\" path=\"odometer/OdometerRegistry.py\"/>\n        <file name=\"Odometer.py\" path=\"odometer/Odometer.py\"/>\n        <directory name=\"database\">\n          <file name=\"PersistenceBackend.py\" path=\"odometer/database/PersistenceBackend.py\"/>\n          <directory name=\"pgres\">\n            <file name=\"wipe_database.py\" path=\"odometer/database/pgres/wipe_database.py\"/>\n            <file name=\"PostgresBackend.py\" path=\"odometer/database/pgres/PostgresBackend.py\"/>\n            <file name=\"snapshot.py\" path=\"odometer/database/pgres/snapshot.py\"/>\n          </directory>\n          <directory name=\"sqlite\">\n            <file name=\"SqliteBackend.py\" path=\"odometer/database/sqlite/SqliteBackend.py\"/>\n          </directory>\n        </directory>\n      </directory>\n      <directory name=\"request\">\n        <file name=\"outputtype.py\" path=\"request/outputtype.py\"/>\n        <file name=\"__init__.py\" path=\"request/__init__.py\"/>\n        <file name=\"clientparams.py\" path=\"request/clientparams.py\"/>\n        <file name=\"request.py\" path=\"request/request.py\"/>\n      </directory>\n      <directory name=\"result\">\n        <file name=\"response.py\" path=\"result/response.py\"/>\n        <file name=\"result.py\" path=\"result/result.py\"/>\n        <file name=\"__init__.py\" path=\"result/__init__.py\"/>\n        <file name=\"error.py\" path=\"result/error.py\"/>\n      </directory>\n      <directory name=\"progress\">\n        <file name=\"handlers.py\" path=\"progress/handlers.py\"/>\n        <file name=\"display_mixins.py\" path=\"progress/display_mixins.py\"/>\n        <file name=\"wrappers.py\" path=\"progress/wrappers.py\"/>\n        <file name=\"tracker.py\" path=\"progress/tracker.py\"/>\n        <file name=\"__init__.py\" path=\"progress/__init__.py\"/>\n        <file name=\"verbosity.py\" path=\"progress/verbosity.py\"/>\n      </directory>\n    </directory>\n  </directory_tree>\n  <file_contents>\n    <file path=\"batch.py\">\"\"\"\nConvenience script for the async use case:\n\ninput_variables_list = list[dict]\nprompt = Prompt(prompt_str)\nmodel = ModelAsync(preferred_model)\nconduit = AsyncConduit(prompt-prompt, model=model)\nresponses = conduit.run(input_variables_list)\n\"\"\"\n\n# Orchestration classes\nfrom conduit.conduit.async_conduit import AsyncConduit\nfrom conduit.model.model_async import ModelAsync\nfrom conduit.prompt.prompt import Prompt\n\n# Cache\nfrom conduit.cache.cache import ConduitCache\n\n# Primitives: dataclasses / enums\nfrom conduit.result.response import Response\nfrom conduit.progress.verbosity import Verbosity\nfrom conduit.message.message import Message\nfrom conduit.request.request import Request\n\n__all__ = [\n    \"AsyncConduit\",\n    \"ConduitCache\",\n    \"Message\",\n    \"ModelAsync\",\n    \"Prompt\",\n    \"Request\",\n    \"Response\",\n    \"Verbosity\",\n]\n</file>\n    <file path=\"__init__.py\">\n</file>\n    <file path=\"sync.py\">\"\"\"\nConvenience imports for the most common Conduit use case:\n\nprompt = Prompt(prompt_str)\nmodel = Model(preferred_model)\nconduit = Conduit(prompt, model)\nresponse = conduit.run(input_data)\n\"\"\"\n\n# Orchestration classes\nfrom conduit.conduit.sync_conduit import SyncConduit\nfrom conduit.model.model import Model\nfrom conduit.prompt.prompt import Prompt\n\n# Cache\nfrom conduit.cache.cache import ConduitCache\n\n# Primitives: dataclasses / enums\nfrom conduit.result.response import Response\nfrom conduit.progress.verbosity import Verbosity\nfrom conduit.message.message import Message\nfrom conduit.request.request import Request\n\nConduit = SyncConduit  # Alias for easier imports\n\n\n__all__ = [\n    \"Conduit\",\n    \"ConduitCache\",\n    \"Message\",\n    \"Model\",\n    \"Prompt\",\n    \"Request\",\n    \"Response\",\n    \"SyncConduit\",\n    \"Verbosity\",\n]\n</file>\n    <file path=\"scripts/update_ollama_list.py\">\"\"\"\nThis script updates the list of Ollama models in the Conduit module.\nUse this when switching environments, ssh tunnels, or when new models are added.\nNeed to figure out where to automatically implement this in my Conduit package to avoid manual updates but also preserve lazy loading.\n\"\"\"\n\nfrom conduit.logs.logging_config import get_logger\nfrom conduit.model.clients.ollama_client import OllamaClientSync\nfrom conduit.model.models.modelstore import ModelStore\nfrom rich import console\n\nlogger = get_logger(__name__)\nconsole = console.Console(width=80)\n\n\ndef main():\n    console.print(\"[green]Updating Ollama Models...[/green]\")\n    client = OllamaClientSync()\n    client.update_ollama_models()\n    console.print(\n        f\"[green]Model list updated: [/green][yellow]{ModelStore.models()['ollama']}[/yellow]\"\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</file>\n    <file path=\"scripts/imagegen_cli.py\">from conduit.message.imagemessage import ImageMessage\nfrom diffusers import FluxPipeline\nimport torch, io, base64, argparse\n\n\ndef generate_image(prompt_str: str) -&gt; ImageMessage:\n    from conduit.model.model import Model\n    from conduit.conduit.sync_conduit import SyncConduit\n    from conduit.prompt.prompt import Prompt\n\n    model = Model(\"gemini-2.5-flash-image-preview\")\n    prompt = Prompt(prompt_str)\n    sync_conduit = SyncConduit(model=model, prompt=prompt)\n    response = sync_conduit.run()\n    assert isinstance(response, ImageMessage)\n    return response\n\n\ndef generate_image_local(prompt: str) -&gt; ImageMessage:\n    pipe = FluxPipeline.from_pretrained(\n        \"Jlonge4/flux-dev-fp8\", torch_dtype=torch.bfloat16\n    )\n    pipe.enable_sequential_cpu_offload()\n\n    image = pipe(\n        prompt,\n        height=1024,\n        width=1024,\n        guidance_scale=3.5,\n        num_inference_steps=50,\n        max_sequence_length=512,\n        generator=torch.Generator(\"cpu\").manual_seed(0),\n    ).images[0]\n\n    # Convert PIL image to base64 string\n    buffered = io.BytesIO()\n    image.save(buffered, format=\"PNG\")  # or \"JPEG\"\n    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n\n    return ImageMessage.from_base64(\n        image_content=img_str, text_content=\"generated image\"\n    )\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate an image from a prompt.\")\n    parser.add_argument(\n        \"prompt\", type=str, help=\"The prompt to generate the image from.\"\n    )\n    args = parser.parse_args()\n    im = generate_image(args.prompt)\n    im.display()\n\n\nif __name__ == \"__main__\":\n    main()\n</file>\n    <file path=\"scripts/conduit_cli.py\">from conduit.cli.cli_class import ConduitCLI\n\n\ndef main():\n    conduit_cli = ConduitCLI()\n    conduit_cli.run()\n\n\nif __name__ == \"__main__\":\n    main()\n</file>\n    <file path=\"scripts/update_modelstore.py\">from conduit.model.models.modelstore import ModelStore\n\n\ndef main():\n    modelstore = ModelStore()\n    modelstore.update()\n\n\nif __name__ == \"__main__\":\n    main()\n</file>\n    <file path=\"scripts/__init__.py\"/>\n    <file path=\"scripts/chat_cli.py\">from conduit.chat.chat import Chat\nfrom conduit.conduit.sync_conduit import SyncConduit\nfrom conduit.model.model import Model\nfrom conduit.message.messagestore import MessageStore\nfrom conduit.logs.logging_config import get_logger\nfrom conduit.cache.cache import ConduitCache\nfrom pathlib import Path\nfrom rich.console import Console\nimport readline\nimport argparse\n\n# Constants\ndir_path = Path(__file__).parent\n_ = readline.get_current_history_length()  # Gaming the type hints.\nlogger = get_logger(__name__)  # Our logger\nconsole = Console()\nSyncConduit._console = console\nModel._console = console\nModel.conduit_cache = ConduitCache()  # Caching set up.\nSyncConduit.message_store = MessageStore(\n    pruning=True\n)  # Non-persistant, but we should prune\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Chat CLI\")\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        default=\"llama3.1:latest\",\n        help=\"The model to use for chatting (default: llama3.1:latest)\",\n    )\n    args = parser.parse_args()\n    model_name = args.model\n    # Validate model name\n    if Model.validate_model(model_name) is False:\n        console.print(f\"[red]Invalid model name: {model_name}[/red]\")\n        return\n\n    # Set the model based on command line argument\n    c = Chat(Model(model_name))\n    c.chat()\n\n\nif __name__ == \"__main__\":\n    main()\n</file>\n    <file path=\"scripts/models_cli.py\">import argparse\nfrom conduit.model.models.modelstore import ModelStore\n\nmodels = ModelStore.list_models()\nmodeltypes = ModelStore.list_model_types()\nproviders = ModelStore.list_providers()\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"CLI for managing models.\")\n    parser.add_argument(\n        \"-m\", \"--model\", type=str, help=\"Name of the model to retrieve details for.\"\n    )\n    parser.add_argument(\n        \"-t\", \"--type\", type=str, help=\"Type of the model to filter by.\"\n    )\n    parser.add_argument(\n        \"-p\", \"--provider\", type=str, help=\"Provider of the model to filter by.\"\n    )\n    args = parser.parse_args()\n    # Validate arguments\n    if args.type:\n        if args.type not in modeltypes:\n            raise ValueError(\n                f\"Invalid model type: {args.type}. Must be one of: {' | '.join(modeltypes)}.\"\n            )\n    if args.provider:\n        if args.provider not in providers:\n            raise ValueError(\n                f\"Invalid provider: {args.provider}. Must be one of: {' | '.join(providers)}.\"\n            )\n    if args.model:\n        if args.model not in models:\n            raise ValueError(\n                f\"Model {args.model} not found. Available models: {', '.join(models)}.\"\n            )\n    # Run commands\n    if args.model:\n        modelspec = ModelStore.get_model(args.model)\n        modelspec.card\n    elif args.type:\n        modelspecs = ModelStore.by_type(args.type)\n        for model in modelspecs:\n            print(model.model)\n    elif args.provider:\n        modelspecs = ModelStore.by_provider(args.provider)\n        for model in modelspecs:\n            print(model.model)\n    else:\n        ModelStore.display()\n\n\nif __name__ == \"__main__\":\n    main()\n</file>\n    <file path=\"conduit/sync_conduit.py\">\"\"\"\nA Conduit is a convenience wrapper for models, prompts, parsers, messages, and response objects.\nA conduit needs to have at least a prompt and a model.\nConduits are immutable, treat them like tuples.\n\"\"\"\n\n# The rest of our package.\nfrom conduit.prompt.prompt import Prompt\nfrom conduit.model.model import Model\nfrom conduit.result.response import Response\nfrom conduit.result.error import ConduitError\nfrom conduit.result.result import ConduitResult\nfrom conduit.parser.parser import Parser\nfrom conduit.message.message import Message\nfrom conduit.message.textmessage import TextMessage\nfrom conduit.message.messages import Messages\nfrom conduit.message.messagestore import MessageStore\nfrom conduit.progress.verbosity import Verbosity\n\nfrom conduit.logs.logging_config import get_logger\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from rich.console import Console\n\nlogger = get_logger(__name__)\n\n\nclass SyncConduit:\n    \"\"\"\n    How we chain things together.\n    Instantiate with:\n    - a prompt (a string that is ready for jinja2 formatting),\n    - a model (a name of a model (full list of accessible models in Model.models))\n    - a parser (a function that takes a string and returns a string)\n    \"\"\"\n\n    message_store: MessageStore | None = None\n    console: \"Console | None\" = None\n\n    def __init__(\n        self,\n        model: Model,\n        prompt: Prompt | None = None,\n        parser: Parser | None = None,\n        pretty: bool = True,\n    ):\n        self.prompt = prompt\n        self.model = model\n        self.parser = parser\n        if self.prompt:\n            self.input_schema = self.prompt.input_schema  # this is a set\n        else:\n            self.input_schema = set()\n        if pretty:\n            from rich.console import Console\n\n            SyncConduit._console = Console()\n\n    def _validate_input_variables(self, input_variables: dict[str, str]) -&gt; None:\n        \"\"\"\n        Validates that the provided input variables match the expected input schema.\n\n        Args:\n            input_variables (dict): A dictionary of input variables to validate.\n\n        Raises:\n            ValueError: If any required input variables are missing.\n            ValueError: If any extra input variables are provided that are not\n                expected by the prompt.\n        \"\"\"\n        # Determine if prompt is expecting variables that are not provided\n        missing_vars: set = self.input_schema - input_variables.keys()\n        if missing_vars:\n            raise ValueError(\n                f'Prompt is missing required input variable(s): \"{'\", \"'.join(missing_vars)}\"'\n            )\n        # Determine if extra variables are provided that the prompt does not expect\n        extra_vars: set = input_variables.keys() - self.input_schema\n        if extra_vars:\n            raise ValueError(\n                f'Provided input variable(s) are not referenced in prompt: \"{'\", \"'.join(extra_vars)}\"'\n            )\n        return\n\n    def run(\n        self,\n        # Inputs\n        input_variables: dict[str, str] | None = None,\n        messages: Messages | list[Message] | None = None,\n        parser: Parser | None = None,\n        # Configs\n        verbose: Verbosity = Verbosity.PROGRESS,\n        stream: bool = False,\n        cache: bool = True,\n        index: int = 0,\n        total: int = 0,\n        # History / messagestore-related\n        include_history: bool = True,\n        save: bool = True,\n    ) -&gt; ConduitResult:\n        \"\"\"\n        Executes the Conduit, processing the prompt and interacting with the language model.\n\n        whether 'messages' are provided or if a streaming response is requested.\n        It renders the prompt with `input_variables` if a `Prompt` object is\n        associated with the Conduit.\n\n        Args:\n            input_variables (dict | None): A dictionary of variables to render\n                the prompt template. Required if the Conduit's prompt contains\n                Jinja2 placeholders. Defaults to None.\n            messages (list[Message] | None): A list of `Message` objects\n                representing a conversation history or a single message. If\n                provided, the Conduit will operate in chat mode. Defaults to an\n                empty list.\n            parser: (Parser | None): A parser to process the model's output.\n            verbose (bool): If True, displays progress information during the\n                model query. This is managed by the `progress_display` decorator\n                on the underlying `Model.query` call. Defaults to True.\n            stream (bool): If True, attempts to stream the response from the\n                model. Note that streaming requests do not return a `Response`\n                object directly but rather a generator. Defaults to False.\n            cache (bool): If True, the response will be cached if caching is\n                enabled on the `Model` class. Defaults to True.\n            index (int): The current index of the item being processed in a\n                batch operation. Used for progress display (e.g., \"[1/100]\").\n                Requires `total` to be provided. Defaults to 0.\n            total (int): The total number of items in a batch operation. Used\n                for progress display (e.g., \"[1/100]\"). Requires `index` to be\n                provided. Defaults to 0.\n            include_history (bool): If True, the full message history will be\n                included in the query. Defaults to True; does not apply if no\n                message store is associated with the Conduit.\n            save (bool): If True, history is saved to the message store.\n                Defaults to True, as that is the main purpose of message store.\n                If false, this is an ephemeral query that will not be saved.\n\n        Returns:\n            Response: A `Response` object containing the model's output, status,\n            duration, and associated messages. Returns a generator if `stream`\n            is True.\n\n        Raises:\n            ValueError: If neither a prompt nor messages are provided.\n            ValueError: If `index` is provided without `total`, or vice-versa.\n        \"\"\"\n        # Render our prompt with the input_variables if variables are passed.\n        if input_variables and self.prompt:\n            self._validate_input_variables(input_variables)\n            logger.info(\"Rendering prompt with input variables: %s\", input_variables)\n            prompt = self.prompt.render(input_variables=input_variables)\n        elif self.prompt:\n            logger.info(\"Using prompt without input variables.\")\n            prompt = self.prompt.prompt_string\n        else:\n            logger.info(\"No prompt provided, using None.\")\n            prompt = None\n        # Resolve messages: messagestore or user-provided messages. Are we including history?\n        if SyncConduit.message_store and include_history:\n            if messages is not None:\n                raise ValueError(\n                    \"Both messages and message store are provided, please use only one.\"\n                )\n            logger.info(\"Using message store for messages.\")\n            messages = SyncConduit.message_store.messages\n        else:\n            if messages is None:\n                logger.info(\"No message store or messages provided, starting fresh.\")\n                messages = []\n            else:\n                logger.info(\"Using user-provided messages.\")\n        # Coerce messages and query_input into a list of Message objects\n        logger.info(\"Coercing messages and prompt into a list of Message objects.\")\n        messages = self._coerce_messages_and_prompt(prompt=prompt, messages=messages)\n        assert len(messages) &gt; 0, \"No messages provided, cannot run conduit.\"\n        # Route input; if string, if message\n        logger.info(\"Querying model.\")\n        # Save this for later\n        result = self.model.query(\n            query_input=messages,\n            response_model=self.parser.pydantic_model if self.parser else None,\n            verbose=verbose,\n            cache=cache,\n            index=index,\n            total=total,\n            stream=stream,\n        )\n        logger.info(f\"Model query completed, return type: {type(result)}.\")\n        # Save to messagestore if we have one and if we have a response.\n        if SyncConduit.message_store and save:\n            if isinstance(result, Response):\n                logger.info(\"Saving response to message store.\")\n                if not include_history:\n                    # For a one-off query, the user message that initiated it was not\n                    # in the message store. We must add it now to maintain history integrity.\n                    user_message = messages[\n                        -1\n                    ]  # The user message is the last in the list sent to the model\n                    SyncConduit.message_store.append(user_message)\n                SyncConduit.message_store.append(result.message)\n            elif isinstance(result, ConduitError):\n                logger.error(\"ConduitError encountered, not saving to message store.\")\n                SyncConduit.message_store.query_failed()\n        else:\n            if not SyncConduit.message_store:\n                logger.info(\"No message store associated with conduit, skipping save.\")\n            if not save:\n                logger.info(\"'save' is False, skipping save to message store.\")\n        if not isinstance(result, ConduitResult):\n            logger.warning(\n                \"Result is not a Response or ConduitError: type {type(result)}.\"\n            )\n        return result\n\n    def _coerce_messages_and_prompt(\n        self, prompt: str | Message | None, messages: Messages | list[Message] | None\n    ) -&gt; list[Message] | Messages:\n        \"\"\"\n        We want a list of messages to submit to Model.query.\n        If we have a prompt, we want to convert it into a user message and append it to messages.\n        WARNING: This function will mutate its inputs. If messages = a MessageStore, you might see doubled messages if you don't take this into account. We are taking advantage of mutability and inheritance here but beware -- especially if MessageStore is persistent, as this can also kick off database writes.\n        \"\"\"\n        if not messages:\n            messages = []\n        if isinstance(prompt, Message):\n            # If we have a message, just append it\n            messages.append(prompt)\n        elif isinstance(prompt, str):\n            # If we have a string, convert it to a TextMessage\n            messages.append(TextMessage(role=\"user\", content=prompt))\n        elif prompt is None:\n            # If we have no prompt, do nothing\n            pass\n        else:\n            raise ValueError(f\"Unsupported query_input type: {type(query_input)}\")\n\n        return messages\n</file>\n    <file path=\"conduit/async_conduit.py\">from conduit.conduit.sync_conduit import SyncConduit, Prompt\nfrom conduit.model.model_async import ModelAsync\nfrom conduit.result.response import Response\nfrom conduit.result.error import ConduitError\nfrom conduit.logs.logging_config import configure_logging, logging\nfrom conduit.parser.parser import Parser\nfrom conduit.progress.verbosity import Verbosity\nfrom conduit.message.messagestore import MessageStore\nfrom typing import TYPE_CHECKING, Optional\nimport asyncio\n\n# Our TYPE_CHECKING imports, these ONLY load for IDEs, so you can still lazy load in production.\nif TYPE_CHECKING:\n    from rich.console import Console\n\nlogger = configure_logging(\n    level=logging.INFO,\n)\n\n\nclass AsyncConduit(SyncConduit):\n    message_store: Optional[MessageStore] = None\n    # If you want rich progress reporting, add a rich.console.Console object to Conduit. (also can be added at Model level)\n    _console: Optional[\"Console\"] = None\n\n    def __init__(\n        self,\n        model: ModelAsync,\n        prompt: Prompt | None = None,\n        parser: Parser | None = None,\n    ):\n        if not isinstance(model, ModelAsync):\n            raise TypeError(\"Model must be of type ModelAsync\")\n        if prompt and not isinstance(prompt, Prompt):\n            raise TypeError(\"Prompt must be of type Prompt\")\n        if parser and not isinstance(parser, Parser):\n            raise TypeError(\"Parser must be of type Parser\")\n        \"\"\"Override to use ModelAsync\"\"\"\n        self.prompt = prompt\n        self.model = model\n        self.parser = parser\n        if self.prompt:\n            self.input_schema = self.prompt.input_schema  # this is a set\n        else:\n            self.input_schema = set()\n\n    def run(\n        self,\n        input_variables_list: list[dict] | None = None,\n        prompt_strings: list[str] | None = None,\n        semaphore: Optional[asyncio.Semaphore] = None,\n        cache=True,\n        verbose: Verbosity = Verbosity.PROGRESS,\n        print_response=False,\n    ) -&gt; list[Response]:\n        \"\"\"\n        Asynchronously runs multiple prompt strings or input variable sets in parallel.\n\n        This method facilitates concurrent LLM calls, making it ideal for batch\n        processing and high-throughput applications. It manages an internal\n        asynchronous event loop to execute queries concurrently. Progress\n        display is automatically handled, showing overall batch progress.\n\n        Args:\n            input_variables_list (list[dict] | None): A list of dictionaries,\n                where each dictionary contains input variables for a prompt\n                template. Used when the Conduits's prompt is a template. Defaults\n                to None.\n            prompt_strings (list[str] | None): A list of pre-rendered prompt\n                strings to be sent to the model. Used when no prompt template\n                is involved or prompts are pre-generated. Defaults to None.\n            semaphore (Optional[asyncio.Semaphore]): An optional asyncio.Semaphore\n                to control the maximum number of concurrent requests. If None,\n                requests will run as fast as possible. Defaults to None.\n            cache (bool): If True, responses will be looked up in the cache\n                and saved to it if not found. Defaults to True.\n            verbose (Verbosity): If True, displays real-time progress information\n                for the entire batch of operations. Individual operation progress\n                is suppressed during concurrent execution. Defaults to True.\n            print_response (bool): If True, prints the content of each individual\n                response as it is received. Useful for debugging but can be verbose.\n                Defaults to False.\n\n        Returns:\n            list[Response]: A list of `Response` objects, each corresponding\n            to an individual query in the batch. Exceptions during individual\n            queries are caught and returned as part of the result list.\n\n        Raises:\n            ValueError: If neither `prompt_strings` nor `input_variables_list`\n                are provided.\n            ValueError: If `input_variables_list` is provided but no `Prompt`\n                is assigned to the `AsyncConduit` object.\n\n        Examples:\n            &gt;&gt;&gt; # Process multiple simple prompts concurrently\n            &gt;&gt;&gt; prompts = [\"What is 1+1?\", \"What is 2+2?\"]\n            &gt;&gt;&gt; model_async = ModelAsync(\"gpt-4o-mini\")\n            &gt;&gt;&gt; async_conduit = AsyncConduit(model=model_async)\n            &gt;&gt;&gt; responses = async_conduit.run(prompt_strings=prompts)\n            &gt;&gt;&gt; for resp in responses:\n            &gt;&gt;&gt;     print(resp.content)\n\n            &gt;&gt;&gt; # Process prompts from a template with input variables\n            &gt;&gt;&gt; prompt_template = Prompt(\"Describe a {{color}} {{animal}}.\")\n            &gt;&gt;&gt; input_vars = [{\"color\": \"red\", \"animal\": \"fox\"}, {\"color\": \"blue\", \"animal\": \"whale\"}]\n            &gt;&gt;&gt; model_async = ModelAsync(\"claude-3-haiku\")\n            &gt;&gt;&gt; async_conduit = AsyncConduit(prompt=prompt_template, model=model_async)\n            &gt;&gt;&gt; responses = async_conduit.run(input_variables_list=input_vars, semaphore=asyncio.Semaphore(2))\n            &gt;&gt;&gt; for resp in responses:\n            &gt;&gt;&gt;     print(resp.content)\n        \"\"\"\n\n        async def _run_async():\n            if prompt_strings:\n                return await self._run_prompt_strings(\n                    prompt_strings, semaphore, cache, verbose, print_response\n                )\n            if input_variables_list:\n                return await self._run_input_variables(\n                    input_variables_list, semaphore, cache, verbose, print_response\n                )\n\n        responses = asyncio.run(_run_async())\n\n        assert isinstance(responses, list), \"Responses should be a list\"\n        assert all([isinstance(r, Response) for r in responses]), (\n            \"All results should be Response objects\"\n        )\n\n        return responses\n\n    async def _run_prompt_strings(\n        self,\n        prompt_strings: list[str],\n        semaphore: Optional[asyncio.Semaphore] = None,\n        cache=True,\n        verbose: Verbosity = Verbosity.PROGRESS,\n        print_response=False,\n    ) -&gt; list:\n        \"\"\"Run multiple prompt strings concurrently with enhanced progress display\"\"\"\n\n        # Create concurrent progress tracker if verbose\n        tracker = None\n        if verbose:\n            console = self.model.console or self.__class__._console\n            from conduit.progress.tracker import ConcurrentTracker\n            from conduit.progress.wrappers import create_concurrent_progress_tracker\n\n            tracker = create_concurrent_progress_tracker(console, len(prompt_strings))\n            tracker.emit_concurrent_start()\n\n        async def process_with_semaphore_and_tracking(\n            prompt_string: str,\n            parser: Parser | None,\n            semaphore: Optional[asyncio.Semaphore],\n            tracker: Optional[ConcurrentTracker],\n            cache=True,\n            verbose: Verbosity = Verbosity.SILENT,  # Always suppress individual progress during concurrent\n            print_response=False,\n        ):\n            async def do_work():\n                if semaphore:\n                    async with semaphore:\n                        return await self.model.query_async(\n                            query_input=prompt_string,\n                            response_model=self.parser.pydantic_model\n                            if self.parser\n                            else None,\n                            cache=cache,\n                            verbose=verbose,\n                            print_response=print_response,\n                        )\n                else:\n                    return await self.model.query_async(\n                        query_input=prompt_string,\n                        response_model=self.parser.pydantic_model\n                        if self.parser\n                        else None,\n                        cache=cache,\n                        verbose=verbose,\n                        print_response=print_response,\n                    )\n\n            # Wrap with concurrent tracking if available\n            if tracker:\n                from conduit.progress.wrappers import concurrent_wrapper\n\n                return await concurrent_wrapper(do_work(), tracker)\n            else:\n                return await do_work()\n\n        # Create coroutines with tracking\n        coroutines = [\n            process_with_semaphore_and_tracking(\n                prompt_string,\n                self.parser,\n                semaphore,\n                tracker,\n                cache,\n                verbose=False,  # Always suppress individual progress\n                print_response=print_response,\n            )\n            for prompt_string in prompt_strings\n        ]\n\n        # Run all operations concurrently\n        responses = await asyncio.gather(*coroutines, return_exceptions=True)\n\n        assert isinstance(responses, list), \"Responses should be a list\"\n        assert all([isinstance(r, Response) for r in responses]), (\n            \"All results should be Response objects\"\n        )\n\n        # Complete concurrent tracking\n        if tracker:\n            tracker.emit_concurrent_complete()\n\n        return responses\n\n    async def _run_input_variables(\n        self,\n        input_variables_list: list[dict],\n        semaphore: Optional[asyncio.Semaphore] = None,\n        cache=True,\n        verbose: Verbosity = Verbosity.PROGRESS,\n        print_response=False,\n    ) -&gt; list:\n        \"\"\"Run multiple input variable sets concurrently with enhanced progress display\"\"\"\n\n        if not self.prompt:\n            raise ValueError(\"No prompt assigned to AsyncConduit object\")\n\n        # Create concurrent progress tracker if verbose\n        tracker = None\n        if verbose:\n            console = self.model.console or self.__class__._console\n            from conduit.progress.tracker import ConcurrentTracker\n            from conduit.progress.wrappers import create_concurrent_progress_tracker\n\n            tracker = create_concurrent_progress_tracker(\n                console, len(input_variables_list)\n            )\n            tracker.emit_concurrent_start()\n\n        async def process_with_semaphore_and_tracking(\n            input_variables: dict,\n            parser: Parser | None,\n            semaphore: Optional[asyncio.Semaphore],\n            tracker: Optional[ConcurrentTracker],\n            cache=True,\n            verbose: Verbosity = Verbosity.SILENT,  # Always suppress individual progress during concurrent\n            print_response=False,\n        ):\n            async def do_work():\n                if semaphore:\n                    async with semaphore:\n                        return await self.model.query_async(\n                            query_input=self.prompt.render(\n                                input_variables=input_variables\n                            ),\n                            response_model=parser.pydantic_model if parser else None,\n                            cache=cache,\n                            verbose=verbose,\n                            print_response=print_response,\n                        )\n                else:\n                    return await self.model.query_async(\n                        query_input=self.prompt.render(input_variables=input_variables),\n                        response_model=parser.pydantic_model if parser else None,\n                        cache=cache,\n                        verbose=verbose,\n                        print_response=print_response,\n                    )\n\n            # Wrap with concurrent tracking if available\n            if tracker:\n                from conduit.progress.wrappers import concurrent_wrapper\n\n                return await concurrent_wrapper(do_work(), tracker)\n            else:\n                return await do_work()\n\n        # Create coroutines with tracking\n        coroutines = [\n            process_with_semaphore_and_tracking(\n                input_variables,\n                self.parser,\n                semaphore,\n                tracker,\n                cache=cache,\n                verbose=False,  # Always suppress individual progress\n                print_response=print_response,\n            )\n            for input_variables in input_variables_list\n        ]\n\n        # Run all operations concurrently\n        responses = await asyncio.gather(*coroutines, return_exceptions=True)\n\n        # Complete concurrent tracking\n        if tracker:\n            tracker.emit_concurrent_complete()\n\n        if isinstance(responses[0], ConduitError):\n            print(responses[0].info)\n            print(responses[0].detail)\n\n        assert all([isinstance(r, Response) for r in responses]), (\n            \"All results should be Response objects\"\n        )\n        assert isinstance(responses, list) and all(\n            [isinstance(r, Response) for r in responses]\n        ), \"All results should be Response objects\"\n        return responses\n</file>\n    <file path=\"conduit/__init__.py\"/>\n    <file path=\"cli/config_loader.py\">class ConfigLoader:\n    def __init__(self):\n        self.config = self.load_config()\n\n    def load_config(self) -&gt; dict:\n        from importlib.resources import files\n        import json\n\n        config_path = files(\"conduit.cli\").joinpath(\"args.json\")\n        json_string = config_path.read_text()\n        config_dict = json.loads(json_string)\n        deserialized_dict = self._deserialize(config_dict)\n        return deserialized_dict\n\n    def _deserialize(self, config_dict: dict) -&gt; dict:\n        \"\"\"\n        We need to rehydrate types from \"str\" to actual python str type, etc.\n        \"\"\"\n        # For all positional args\n        for pos_arg in config_dict[\"positional_args\"]:\n            if \"type\" in pos_arg.keys():\n                type_name = pos_arg[\"type\"]\n                pos_arg[\"type\"] = self._get_type_object(type_name)\n\n        # For all commands\n        for command in config_dict[\"commands\"]:\n            if \"type\" in command.keys():\n                type_name = command[\"type\"]\n                command[\"type\"] = self._get_type_object(type_name)\n        # For all flags\n        for flag in config_dict[\"flags\"]:\n            if \"type\" in flag.keys():\n                type_name = flag[\"type\"]\n                flag[\"type\"] = self._get_type_object(type_name)\n        return config_dict\n\n    def _get_type_object(self, type_name: str):\n        match type_name:\n            case \"str\":\n                return str\n            case \"int\":\n                return int\n            case \"float\":\n                return float\n            case \"bool\":\n                return bool\n            case _:\n                raise ValueError(f\"Unknown type: {type_name}\")\n</file>\n    <file path=\"cli/cli_class.py\">\"\"\"\nConduitCLI is our conduit library as a CLI application.\n\nCustomize the query_function to specialize for various prompts / workflows while retaining archival and other functionalities.\n\nTo customize:\n1. Define your own query function matching the QueryFunctionProtocol signature.\n2. Pass your custom function to the ConduitCLI class upon instantiation.\n\nThis allows you to tailor the behavior of ConduitCLI while leveraging its existing features.\n\"\"\"\n\nfrom rich.console import Console\nfrom argparse import ArgumentParser, Namespace\nfrom conduit.cli.config_loader import ConfigLoader\nfrom conduit.cli.handlers import HandlerMixin\nfrom conduit.cli.query_function import QueryFunctionProtocol, default_query_function\nfrom conduit.progress.verbosity import Verbosity\nfrom conduit.model.models.modelstore import ModelStore\nfrom xdg_base_dirs import xdg_data_home, xdg_config_home, xdg_cache_home\nfrom pathlib import Path\nimport sys\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Defaults\nDEFAULT_NAME = \"conduit\"\nDEFAULT_DESCRIPTION = \"Conduit: The LLM CLI\"\nDEFAULT_QUERY_FUNCTION = default_query_function\nDEFAULT_VERBOSITY = Verbosity.COMPLETE\nDEFAULT_CONSOLE = Console()\nDEFAULT_CACHE_SETTING = True\nDEFAULT_PERSISTENT_SETTING = True\nDEFAULT_PREFERRED_MODEL = \"claude\"  # But can be overridden by config\n\n\nclass ConduitCLI(HandlerMixin):\n    \"\"\"\n    Main class for the Conduit CLI application.\n    Combines argument parsing, configuration loading, and command handling.\n    Attributes:\n    - name: Name of the CLI application.\n    - description: Description of the CLI application.\n    - verbosity: Verbosity level for LLM responses.\n    - preferred_model: Default LLM model to use.\n    - console: Instance of rich.console.Console for rich text output.\n    - config: Configuration dictionary loaded from ConfigLoader.\n    - attr_mapping: Maps command-line argument names to internal attribute names.\n    - command_mapping: Maps command-line argument names to their respective handler method names.\n    - flags: Dictionary to hold parsed flag values.\n    - parser: ArgumentParser instance for parsing command-line arguments.\n    - args: Parsed arguments from the command line.\n    - stdin: Captured standard input if piped.\n    - query_function: Function to handle queries, adhering to QueryFunctionProtocol.\n    - cache: Boolean indicating whether to use caching for LLM responses.\n    Methods:\n    - setup_parser(): Sets up the argument parser based on the loaded configuration.\n    Methods from HandlerMixin:\n    - validate_handlers(): Validates that all handlers specified in the config are implemented.\n    - all handler methods (e.g., handle_history, handle_wipe, etc.) should be implemented in this class.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"conduit\",\n        description: str = DEFAULT_DESCRIPTION,\n        query_function: QueryFunctionProtocol = DEFAULT_QUERY_FUNCTION,\n        verbosity: Verbosity = DEFAULT_VERBOSITY,\n        console: Console = DEFAULT_CONSOLE,\n        cache: bool = DEFAULT_CACHE_SETTING,\n        persistent: bool = DEFAULT_PERSISTENT_SETTING,\n        system_message: str | None = None,  # If None, load from config or default to \"\"\n        preferred_model: str | None = None,\n    ):\n        # Parameters\n        self.name: str = name  # Name of the CLI application\n        self.description: str = description  # description of the CLI application\n        # Query function -- must adhere to QueryFunctionProtocol\n        self.query_function: QueryFunctionProtocol = (\n            query_function  # function to handle queries\n        )\n        assert isinstance(query_function, QueryFunctionProtocol), (\n            \"query_function must adhere to QueryFunctionProtocol\"\n        )\n        # Configs\n        self.verbosity: Verbosity = verbosity  # verbosity level for LLM responses\n        self.console: Console = console  # rich console for output\n        self.cache: bool = cache  # whether to use caching for LLM responses\n        # Get XDG paths\n        self.history_file: Path\n        self.config_dir: Path\n        self.cache_file: Path\n        self.history_file, self.config_dir, self.cache_file = (\n            self._construct_xdg_paths()\n        )\n        # Preferred models\n        self.preferred_model = preferred_model\n        self.preferred_model = self._get_preferred_model(preferred_model)\n        # System message: three options:\n        if system_message is not None:\n            self.system_message = system_message\n        else:\n            self.system_message = self._get_system_message()\n        # Persistence\n        if persistent:\n            logger.info(f\"Using persistent history at {self.history_file}\")\n            from conduit.message.messagestore import MessageStore\n            from conduit.sync import Conduit\n\n            self.history_file.parent.mkdir(parents=True, exist_ok=True)\n            message_store = MessageStore(history_file=self.history_file)\n            Conduit.message_store = message_store\n        else:\n            logger.info(\"Using in-memory history (no persistence)\")\n        # Cache\n        if cache:\n            logger.info(f\"Using cache at {self.cache_file}\")\n            from conduit.sync import Model\n            from conduit.cache.cache import ConduitCache\n\n            self.cache_file.parent.mkdir(parents=True, exist_ok=True)\n            Model.conduit_cache = ConduitCache()\n        else:\n            logger.info(\"Caching disabled\")\n        # Set up config\n        self.attr_mapping: dict = {}\n        self.command_mapping: dict = {}\n        self.parser: ArgumentParser | None = None\n        self.args: Namespace | None = None\n        self.flags: dict = {}  # This will hold all the flag values after parsing\n        self.config: dict = ConfigLoader().config\n        self._validate_handlers()  # from HandlerMixin\n\n    def run(self):\n        \"\"\"\n        Run the CLI application.\n        \"\"\"\n        self._preconfigure_logging()  # Set log level early\n        # Logging begins\n        logger.info(\"Running ConduitCLI\")\n        self.stdin: str = self._get_stdin()  # capture stdin if piped\n        # Setup parser and parse args\n        self.parser = self._setup_parser()\n        # If no args, print help and exit\n        if len(sys.argv) == 1 and not self.stdin:\n            self.parser.print_help(sys.stderr)\n            sys.exit(1)\n        # Parse args\n        self._parse_args()\n\n    def _get_preferred_model(self, preferred_model: str | None) -&gt; str:\n        \"\"\"\n        Validate preferred_model if provided, otherwise load from config or use default.\n        \"\"\"\n        model_store = ModelStore()\n        if preferred_model:\n            # Validate provided preferred_model\n            if model_store._validate_model(preferred_model):\n                return model_store._validate_model(preferred_model)\n            else:\n                logger.warning(\n                    f\"Preferred model '{preferred_model}' is not available. Falling back to config or default.\"\n                )\n        # Load from config if available\n        try:\n            preferred_model_file = xdg_config_home() / self.name / \"preferred_model\"\n            preferred_model_from_config = preferred_model_file.read_text().strip()\n            if model_store._validate_model(preferred_model_from_config):\n                logger.info(\n                    f\"Loaded preferred model '{preferred_model_from_config}' from config file.\"\n                )\n                return model_store._validate_model(preferred_model_from_config)\n            else:\n                logger.warning(\n                    f\"Preferred model '{preferred_model_from_config}' from config file is not available. Falling back to default.\"\n                )\n        except Exception as e:\n            logger.info(\n                \"No preferred model file found in .config, assuming default preferred model.\"\n            )\n        # Default preferred model\n        logger.info(f\"Using default preferred model '{DEFAULT_PREFERRED_MODEL}'\")\n        return DEFAULT_PREFERRED_MODEL\n\n    def _get_system_message(self) -&gt; str:\n        \"\"\"\n        Get the system message from config if it exists, otherwise return \"\".\n        \"\"\"\n        try:\n            system_message_file = (\n                xdg_config_home() / self.name / \"system_message.jinja2\"\n            )\n            system_message = system_message_file.read_text()\n            logger.info(f\"Loaded system message from {system_message_file}\")\n            return system_message\n        except Exception as e:\n            logger.info(\n                \"No system message file found in .config, assuming no system message.\"\n            )\n            return \"\"\n\n    def _preconfigure_logging(self):\n        \"\"\"Quick arg scan to set log level before full parsing.\"\"\"\n        root = logging.getLogger()\n\n        # If already configured (library usage), respect existing config\n        if root.handlers:\n            logger.debug(\"Logging already configured, using existing setup\")\n            return\n\n        # Determine level from CLI args or use default\n        level = logging.WARNING  # default\n\n        if \"--log\" in sys.argv:\n            idx = sys.argv.index(\"--log\")\n            if idx + 1 &lt; len(sys.argv):\n                log_flag = sys.argv[idx + 1].lower()\n                levels = {\"d\": logging.DEBUG, \"i\": logging.INFO, \"w\": logging.WARNING}\n                level = levels.get(log_flag, logging.WARNING)\n\n        # Configure once with the correct level\n        logging.basicConfig(\n            level=level,\n            format=\"%(asctime)s [%(levelname)s] %(name)s:%(lineno)d (%(funcName)s) - %(message)s\",\n            stream=sys.stderr,\n        )\n\n        # Silence noisy libraries\n        for lib in [\"markdown_it\", \"urllib3\", \"httpx\", \"httpcore\"]:\n            logging.getLogger(lib).setLevel(logging.WARNING)\n\n    def _construct_xdg_paths(self) -&gt; tuple[Path, Path, Path]:\n        \"\"\"\n        Construct XDG-compliant paths for history, config, and cache files.\n        The name (self.name) is used as the application directory for each.\n\n        Returns a tuple of Paths: (history_file, config_file, cache_file)\n\n        Note: config_file is not currently used but reserved for future use.\n        \"\"\"\n        history_file = xdg_data_home() / self.name / \"history.json\"\n        config_file = xdg_config_home() / self.name / \"config.yaml\"\n        cache_file = xdg_cache_home() / self.name / \"cache.sqlite\"\n\n        return history_file, config_file, cache_file\n\n    def _get_stdin(self) -&gt; str:\n        \"\"\"\n        Get implicit context from clipboard or other sources.\n        \"\"\"\n        context = sys.stdin.read() if not sys.stdin.isatty() else \"\"\n        return context\n\n    def _coerce_query_input(self, query_input: str | list) -&gt; str:\n        \"\"\"\n        Coerce query input to a string.\n        If input is a list, join with spaces.\n        \"\"\"\n        if isinstance(query_input, list):\n            coerced_query_input = \" \".join(query_input)\n        else:\n            coerced_query_input = query_input\n        return coerced_query_input\n\n    def _setup_parser(self):\n        \"\"\"\n        Setup the argument parser based on the configuration.\n        \"\"\"\n        logger.info(\"Setting up argument parser\")\n        parser = ArgumentParser()\n        parser.description = self.description\n        self.attr_mapping = {}\n        self.command_mapping = {}\n\n        # Handle positional args (i.e. query string if provided)\n        logger.info(\"Adding positional arguments to parser\")\n        for pos_arg in self.config.get(\"positional_args\", []):\n            dest = pos_arg.pop(\"dest\")\n            self.attr_mapping[dest] = dest\n            parser.add_argument(\n                dest,\n                **pos_arg,\n            )\n\n        # Handle flags\n        logger.info(\"Adding flags to parser\")\n        for flag in self.config[\"flags\"]:\n            abbrev = flag.pop(\"abbrev\", None)\n            name = flag.pop(\"name\")\n\n            args = [abbrev, name] if abbrev else [name]\n            arg_name = name.lstrip(\"-\").replace(\"-\", \"_\")\n            self.attr_mapping[arg_name] = arg_name  # Same as dest\n            parser.add_argument(*args, **flag)\n\n        # Handle commands\n        logger.info(\"Adding commands to parser\")\n        command_group = parser.add_mutually_exclusive_group()\n        for command in self.config[\"commands\"]:\n            handler = command.pop(\"handler\")\n            abbrev = command.pop(\"abbrev\", None)\n            name = command.pop(\"name\")\n\n            args = [abbrev, name] if abbrev else [name]\n            arg_name = name.lstrip(\"-\").replace(\"-\", \"_\")\n            self.command_mapping[arg_name] = handler\n            command_group.add_argument(*args, **command)\n\n        return parser\n\n    def _parse_args(self):\n        \"\"\"\n        Parse arguments and execute commands or prepare for query processing.\n        \"\"\"\n        logger.info(\"Parsing arguments\")\n        self.args = self.parser.parse_args()\n        logger.debug(f\"Parsed args: {self.args}\")\n\n        # Create flags dictionary\n        logger.info(\"Creating flags dictionary\")\n        self.flags = {}\n        for arg_name, attr_name in self.attr_mapping.items():\n            if hasattr(self.args, arg_name):\n                self.flags[attr_name] = getattr(self.args, arg_name)\n\n        # Coerce query input to string if necessary\n        self.flags[\"query_input\"] = self._coerce_query_input(self.flags[\"query_input\"])\n\n        # Check if any commands were specified and execute them\n        logger.info(\"Checking for commands to execute\")\n        for arg_name, handler_name in self.command_mapping.items():\n            if getattr(self.args, arg_name, False):\n                handler = getattr(self, handler_name)\n                if getattr(self.args, arg_name) not in [True, False]:\n                    handler(getattr(self.args, arg_name))\n                else:\n                    handler()\n                return\n\n        # If no commands were executed and we have query input, process it\n        logger.info(\"No commands executed; checking for query input\")\n        if self.args.query_input:\n            self.query_handler()\n\n    # Debugging methods\n    def _print_all_attrs(self, pretty: bool = True):\n        \"\"\"\n        Debugging method to print all attributes of the instance.\n        \"\"\"\n        logger.info(\"Printing all attributes of the instance\")\n        if pretty:\n            from rich.pretty import Pretty\n\n            self.console.print(Pretty(vars(self)))\n            return\n        else:\n            attrs = vars(self)\n            for attr, value in attrs.items():\n                print(f\"{attr}: {value}\")\n\n    def _get_handler_for_command(self, command_name: str):\n        \"\"\"\n        Given a command-line argument name, return the corresponding handler method.\n        \"\"\"\n        logger.info(f\"Getting handler for command: {command_name}\")\n        handler_name = self.command_mapping.get(command_name)\n        if handler_name and hasattr(self, handler_name):\n            return getattr(self, handler_name)\n        return None\n</file>\n    <file path=\"cli/handlers.py\">\"\"\"\nSome guidelines for this Mixin:\n- all methods must have the signature (self, *args, **kwargs)\n- if the config file specifies a type for a command or flag, it must be one of: str, int, float, bool, and the method must handle that type accordingly\n- the method names must match the handler names in the config file exactly, a la \"handle_history\", \"handle_wipe\", etc.\n\"\"\"\n\nfrom conduit.sync import Conduit, Verbosity\nfrom rich.markdown import Markdown\nfrom rich.console import Console\nimport logging\nimport sys\n\nlogger = logging.getLogger(__name__)\n\n# Constants\nDEFAULT_VERBOSITY = Verbosity.PROGRESS\n\n\nclass HandlerMixin:\n    def _validate_handlers(self):\n        logger.debug(\"Validating handlers...\")\n        self.config: dict  # ConduitCLI has self.config (generated from ConfigLoader)\n        for command in self.config[\"commands\"]:\n            handler_name = command.get(\"handler\")\n            if not hasattr(self, handler_name):\n                raise ValueError(\n                    f\"Handler {handler_name} not implemented in {self.__class__.__name__}\"\n                )\n            if not callable(getattr(self, handler_name)):\n                raise ValueError(\n                    f\"Handler {handler_name} is not callable in {self.__class__.__name__}\"\n                )\n\n    # Convenience methods\n    def print_markdown(self, markdown_string: str):\n        \"\"\"\n        Prints formatted markdown to the console.\n        \"\"\"\n        # Create a Markdown object\n        border = \"-\" * 100\n        markdown_string = f\"{border}\\n{markdown_string}\\n\\n{border}\"\n        md = Markdown(markdown_string)\n        self.console: Console\n        self.console.print(md)\n\n    def grab_image_from_clipboard(self) -&gt; tuple | None:\n        \"\"\"\n        Attempt to grab image from clipboard; return tuple of mime_type and base64.\n        \"\"\"\n        logger.info(\"Attempting to grab image from clipboard...\")\n        import os\n\n        if \"SSH_CLIENT\" in os.environ or \"SSH_TTY\" in os.environ:\n            self.console.print(\"Image paste not available over SSH.\", style=\"red\")\n            return\n\n        import warnings\n        from PIL import ImageGrab\n        import base64, io\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")  # Suppress PIL warnings\n            image = ImageGrab.grabclipboard()\n\n        if image:\n            buffer = io.BytesIO()\n            image.save(buffer, format=\"PNG\")  # type: ignore\n            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n            # Save for next query\n            self.console.print(\"Image captured!\", style=\"green\")\n            # Build our ImageMessage\n            image_content = img_base64\n            mime_type = \"image/png\"\n            return mime_type, image_content\n        else:\n            self.console.print(\"No image detected.\", style=\"red\")\n\n            sys.exit()\n\n    def create_image_message(\n        self, combined_query: str, mime_type: str, image_content: str\n    ) -&gt; \"ImageMessage | None\":\n        logger.info(\"Creating ImageMessage...\")\n        if not image_content or not mime_type:\n            return\n        role = \"user\"\n        text_content = combined_query\n\n        from conduit.message.imagemessage import ImageMessage\n\n        imagemessage = ImageMessage(\n            role=role,\n            text_content=text_content,\n            image_content=image_content,\n            mime_type=mime_type,\n        )\n        return imagemessage\n\n    # Handlers -- should match config file exactly (per validate_handlers)\n    def handle_history(self):\n        \"\"\"\n        View message history and exit.\n        \"\"\"\n        logger.info(\"Viewing message history...\")\n        Conduit.message_store.view_history()\n        sys.exit()\n\n    def handle_wipe(self):\n        \"\"\"\n        Clear the message history after user confirmation.\n        \"\"\"\n        logger.info(\"Wiping message history...\")\n        from rich.prompt import Confirm\n\n        confirm = Confirm.ask(\n            \"[red]Are you sure you want to wipe the message history? This action cannot be undone.[/red]\",\n            default=False,\n        )\n        if confirm:\n            Conduit.message_store.clear()\n            self.console.print(\"[green]Message history wiped.[/green]\")\n        else:\n            self.console.print(\"[yellow]Wipe cancelled.[/yellow]\")\n\n    def handle_shell(self):\n        pass\n\n    def handle_last(self):\n        \"\"\"\n        Print the last message in the message store and exit.\n        \"\"\"\n        logger.info(\"Viewing last message...\")\n        import sys\n\n        # Get last message\n        last_message = Conduit.message_store.last()\n        # If no messages, inform user\n        if not last_message:\n            self.console.print(\"[red]No messages in history.[/red]\")\n            sys.exit()\n        # Print last message\n        if self.flags[\"raw\"]:\n            print(last_message)\n        else:\n            self.print_markdown(str(last_message))\n        sys.exit()\n\n    def handle_get(self, index: int):\n        pass\n\n    def handle_config(self):\n        \"\"\"\n        Print the current configuration and exit.\n        \"\"\"\n        preferred_model = getattr(self, \"preferred_model\", \"\")\n        system_message = getattr(self, \"system_message\", \"\")[:50]\n        logger.info(\"Viewing configuration...\")\n        config_md = f\"\"\"\n# Current Configuration\n| Setting | Value |\n|---------|-------|\n| Preferred Model | {preferred_model} |\n| System Message | {system_message}... |\n| Message History | {\"Enabled\" if self.flags.get(\"chat\") else \"Disabled\"} |\n| Verbosity | {self.verbosity.name} |\n\"\"\"\n        md = Markdown(config_md)\n        self.console.print(md)\n\n    def query_handler(self):\n        \"\"\"\n        Handle a query by combining query input, context, and append, then sending to model.\n\n        Simplest usage:\n            `conduit \"What is the capital of France?\"`\n        Maximal usage:\n            `cat \"some_document.md\" | conduit -q \"Look at this doc.\" -a \" Please summarize.\"`\n        \"\"\"\n        logger.info(\"Handling query...\")\n        # Type hints since mixins confuse IDEs\n        self.flags: dict\n        self.stdin: str\n        self.verbosity: Verbosity\n        self.preferred_model: str\n\n        # Assemble the parts of the query\n        logger.debug(\"Assembling query parts...\")\n        inputs = {\n            \"query_input\": self.flags.get(\"query_input\", \"\"),\n            \"context\": f\"&lt;context&gt;{self.stdin}&lt;/context&gt;\" if self.stdin else \"\",\n            \"append\": self.flags.get(\"append\") or \"\",\n            \"system_message\": getattr(self, \"system_message\", \"\"),\n        }\n\n        # Grab our flags\n        logger.debug(\"Grabbing flags...\")\n        ## NOTE: we need to implement temperature, image, and other flags here.\n        chat = self.flags[\"chat\"]\n        raw = self.flags[\"raw\"]\n        user_defined_model = self.flags[\"model\"]\n        model_to_use = user_defined_model or self.preferred_model\n        # Start our spinner\n        with self.console.status(\n            \"[bold green]Thinking...[/bold green]\", spinner=\"dots\"\n        ):\n            # Our switch logic\n            match (chat, raw):\n                case (True, False):  # Chat (with history), pretty print\n                    logger.debug(\"Chat (with history), pretty print...\")\n                    response = self.query_function(\n                        inputs,\n                        preferred_model=model_to_use,\n                        verbose=self.verbosity,\n                        include_history=True,\n                    )\n                    self.print_markdown(str(response.content))\n                    sys.exit()\n                case (True, True):  # Chat (with history), raw print\n                    logger.debug(\"Chat (with history), raw print...\")\n                    response = self.query_function(\n                        inputs,\n                        preferred_model=model_to_use,\n                        verbose=self.verbosity,\n                        include_history=True,\n                    )\n                    print(response)\n                    sys.exit()\n                case (False, False):  # One-off request, pretty print\n                    logger.debug(\"One-off request, pretty print...\")\n                    response = self.query_function(\n                        inputs,\n                        preferred_model=model_to_use,\n                        verbose=self.verbosity,\n                        include_history=False,\n                    )\n                    self.print_markdown(str(response.content))\n                    sys.exit()\n                case (False, True):  # One-off request, raw print\n                    logger.debug(\"One-off request, raw print...\")\n                    response = self.query_function(\n                        inputs,\n                        preferred_model=preferred_model,\n                        verbose=self.verbosity,\n                        include_history=False,\n                    )\n                    print(response)\n                    sys.exit()\n</file>\n    <file path=\"cli/__init__.py\"/>\n    <file path=\"cli/query_function.py\">\"\"\"\nOur default query function -- passed into ConduitCLI as a dependency.\nDefine your own for customization.\n\"\"\"\n\nfrom conduit.sync import Prompt, Model, Conduit, Response, Verbosity\nfrom typing import Protocol, runtime_checkable\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n# First, our protocol\n@runtime_checkable\nclass QueryFunctionProtocol(Protocol):\n    \"\"\"\n    Protocol for a query function. Customized query functions should match this signature.\n    \"\"\"\n\n    def __call__(\n        self,\n        inputs: dict[str, str],\n        preferred_model: str,\n        include_history: bool,\n        verbose: Verbosity = Verbosity.PROGRESS,\n    ) -&gt; Response: ...\n\n\n# Now, our default implementation -- the beauty of LLMs with POSIX philosophy\ndef default_query_function(\n    inputs: dict[str, str],\n    preferred_model: str,\n    include_history: bool,\n    verbose: Verbosity = Verbosity.PROGRESS,\n) -&gt; Response:\n    \"\"\"\n    Default query function.\n    Note that the input dict will contain keys for all possible inputs, from the positional query, piped in context, and a potential \"append\" string.\n    The simplest query functions will likely only want query_input.\n    \"\"\"\n    logger.debug(\"Running default_query_function...\")\n    # Extract inputs from dict\n    query_input: str = inputs.get(\"query_input\", \"\")\n    context: str = (\n        \"&lt;context&gt;\\n\" + inputs.get(\"context\", \"\") + \"\\n&lt;/context&gt;\"\n        if inputs.get(\"context\")\n        else \"\"\n    )\n    append: str = inputs.get(\"append\", \"\")\n\n    # ConduitCLI's default POSIX philosophy: embrace pipes and redirection\n    combined_query = \"\\n\\n\".join([query_input, context, append])\n\n    # Inject system message if provided and message store exists\n    if Conduit.message_store:\n        system_message = inputs.get(\"system_message\", \"\")\n        if system_message:\n            Conduit.message_store.ensure_system_message(system_message)\n\n    # Our chain\n    model = Model(preferred_model)\n    prompt = Prompt(combined_query)\n    conduit = Conduit(prompt=prompt, model=model)\n    response = conduit.run(verbose=verbose, include_history=include_history)\n    assert isinstance(response, Response), \"Response is not of type Response\"\n    return response\n</file>\n    <file path=\"cli/cli_factory.py\">\"\"\"\nCustomize ConduitCLI quickly with a query function, if you don't worry about custom parameters.\nA simple query_function that ONLY takes query_input and returns a Response object is enough.\n\"\"\"\n\nfrom conduit.cli.conduit_class import (\n    ConduitCLI,\n    DEFAULT_PREFERRED_MODEL,\n    DEFAULT_VERBOSITY,\n)\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from conduit.sync import Verbosity\n\n\ndef wrap_query_function(query_function):\n    \"\"\"\n    Wrap a simple query function to match the expected signature for ConduitCLI.\n\n    Discard extra parameters, as well as extra context (i.e. contex / append).\n    \"\"\"\n\n    def wrapped(\n        inputs: dict[str, str],\n        preferred_model: str = DEFAULT_PREFERRED_MODEL,\n        verbose: \"Verbosity\" = DEFAULT_VERBOSITY,\n        nopersist: bool = False,\n    ):\n        query_input = inputs.get(\"query_input\", \"\")\n        _ = preferred_model  # Discarded\n        _ = verbose  # Discarded\n        _ = nopersist  # Discarded\n        return query_function(query_input)\n\n    return wrapped\n\n\ndef cli_factory(query_function) -&gt; ConduitCLI:\n    \"\"\"\n    Factory function to create a ConduitCLI instance with a simple query function.\n    \"\"\"\n    wrapped_function = wrap_query_function(query_function)\n    cli_instance = ConduitCLI(query_function=wrapped_function)\n    return cli_instance\n</file>\n    <file path=\"embeddings/embedding_model.py\">import json\nfrom pathlib import Path\nfrom conduit.embeddings.chroma_batch import ChromaBatch\nfrom functools import cached_property\n\n_DEVICE_CACHE = None\nEMBEDDING_MODELS_FILE = Path(__file__).parent / \"embedding_models.json\"\n\n\nclass EmbeddingModel:\n    def __init__(self, model_name: str):\n        from transformers import AutoModel, AutoTokenizer\n\n        self.model_name = model_name\n        if model_name not in self.models():\n            raise ValueError(\n                f\"Model '{model_name}' is not in the list of supported models.\"\n            )\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModel.from_pretrained(self.model_name).to(self.device())\n\n    @classmethod\n    def models(cls) -&gt; list[str]:\n        embedding_models_dict = json.loads(EMBEDDING_MODELS_FILE.read_text())\n        embedding_models: list[str] = embedding_models_dict[\"embedding_models\"]\n        return embedding_models\n\n    @classmethod\n    def device(cls) -&gt; str:\n        global _DEVICE_CACHE\n        if _DEVICE_CACHE is None:\n            import torch\n\n            _DEVICE_CACHE = (\n                \"mps\"\n                if torch.backends.mps.is_available()\n                else \"cuda\"\n                if torch.cuda.is_available()\n                else \"cpu\"\n            )\n        return _DEVICE_CACHE\n\n    def generate_embeddings(self, batch: ChromaBatch) -&gt; ChromaBatch:\n        \"\"\"\n        Generate embeddings for a batch of documents.\n        Args:\n            batch (ChromaBatch): A batch of ids and documents to generate embeddings for.\n        Returns:\n            ChromaBatch: A new batch the original ids and documents, as well as generated embeddings.\n        \"\"\"\n        import torch\n\n        inputs = self.tokenizer(\n            batch.documents, padding=True, truncation=True, return_tensors=\"pt\"\n        )\n        inputs = {k: v.to(self.device()) for k, v in inputs.items()}\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().tolist()\n\n        new_batch = ChromaBatch(\n            ids=batch.ids,\n            documents=batch.documents,\n            metadatas=batch.metadatas,\n            embeddings=embeddings,\n        )\n        return new_batch\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    model = EmbeddingModel(\"sentence-transformers/all-MiniLM-L6-v2\")\n    batch = ChromaBatch(\n        ids=[\"1\", \"2\"],\n        documents=[\"This is a test document.\", \"This is another test document.\"],\n        metadatas=[{}, {}],\n    )\n    embedded_batch = model.generate_embeddings(batch)\n    print(embedded_batch.embeddings)\n</file>\n    <file path=\"embeddings/chroma_batch.py\">from pydantic import BaseModel, Field\nfrom typing import Any\n\n\nclass ChromaBatch(BaseModel):\n    # Mandatory fields\n    ids: list[str] = Field(\n        ..., description=\"List of unique identifiers for each item in the batch.\"\n    )\n    documents: list[str] = Field(\n        ..., description=\"List of documents or text associated with each item.\"\n    )\n    # Optional fields\n    embeddings: list[list[float]] | None = Field(\n        default=None, description=\"List of embeddings corresponding to each item.\"\n    )\n    metadatas: list[dict[str, Any]] | None = Field(\n        default=None, description=\"List of metadata dictionaries for each item.\"\n    )\n</file>\n    <file path=\"parser/__init__.py\"/>\n    <file path=\"parser/parser.py\">\"\"\"\nInstructor library does most of the heavy lifting here -- great piece of software.\nParser wraps our Pydantic model as part of Conduit orchestration.\nWe don't want to serialize Parser or save it in any fashion; instead this is a bundle of class/static methods.\nSerialization (for caching, messagestore, etc.) of pydantic models is entirely done through its json schema, so we can use the same model.\n\nThree purposes for this class:\n- as a store of Pydantic models, so we can validate them later\n- as a wrapper for Pydantic models for use with Conduit\n- as a library of static methods for handling Pydantic models (include special logic for Perplexity API, and serialization / hashing)\n\"\"\"\n\nfrom typing import Union, Type\nfrom pydantic import BaseModel\nfrom conduit.logs.logging_config import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass Parser:\n    _response_models: list[\n        Type[BaseModel]\n    ] = []  # Store all pydantic classes, important for de-serialization\n\n    def __init__(self, pydantic_model: Union[Type[BaseModel], type]):\n        \"\"\"\n        Initialize the Parser with a model specification.\n        :param pydantic_model: A Pydantic BaseModel class or a list of BaseModel classes.\n        \"\"\"\n        self.original_spec = pydantic_model\n        self.pydantic_model = pydantic_model\n        # Save response models to singleton for validation\n        if isinstance(pydantic_model, type) and issubclass(pydantic_model, BaseModel):\n            if pydantic_model not in self._response_models:\n                self._response_models.append(pydantic_model)\n        elif isinstance(pydantic_model, list):\n            for model in pydantic_model:\n                if isinstance(model, type) and issubclass(model, BaseModel):\n                    if model not in self._response_models:\n                        self._response_models.append(model)\n                else:\n                    raise ValueError(\n                        \"All items in the list must be Pydantic BaseModel subclasses.\"\n                    )\n\n    def __repr__(self):\n        return f\"Parser({self.original_spec})\"\n\n    # Static methods for handling Pydantic models\n    @staticmethod\n    def to_perplexity(pydantic_model: type) -&gt; BaseModel | type:\n        \"\"\"\n        Convert the Pydantic model to a type suitable for Perplexity API.\n        For wrapper classes with single list fields, extract the inner type\n        so Instructor can handle multiple tool calls properly.\n        \"\"\"\n        # Handle wrapper models with a single list field\n        if isinstance(pydantic_model, type) and issubclass(pydantic_model, BaseModel):\n            # Pydantic v2\n            if hasattr(pydantic_model, \"model_fields\"):\n                fields = pydantic_model.model_fields\n            # Pydantic v1 fallback\n            else:\n                fields = pydantic_model.__fields__\n\n            # Check if it's a wrapper with a single list field\n            if len(fields) == 1:\n                field_name, field_info = next(iter(fields.items()))\n                field_type = (\n                    field_info.annotation\n                    if hasattr(field_info, \"annotation\")\n                    else field_info.type_\n                )\n\n                # Check if it's list[SomeBaseModel]\n                field_origin = typing.get_origin(field_type)\n                if field_origin is list:\n                    args = typing.get_args(field_type)\n                    if args and issubclass(args[0], BaseModel):\n                        # Return List[InnerModel] for Instructor\n                        return list[args[0]]\n\n        # Handle direct list[BaseModel] type annotations\n        origin = typing.get_origin(self.pydantic_model)\n        if origin is list:\n            args = typing.get_args(self.pydantic_model)\n            if args and issubclass(args[0], BaseModel):\n                return pydantic_model  # Return list[SomeModel] directly\n\n        # For non-wrapper classes, return as-is\n        return pydantic_model\n\n    @staticmethod\n    def as_string(pydantic_model: type[BaseModel]) -&gt; str:\n        \"\"\"\n        Convert the Pydantic model to a string representation.\n        This is used for logging and debugging purposes (as well as constructing our cache hash for Params).\n        \"\"\"\n        import json\n\n        schema = pydantic_model.model_json_schema()\n        # Deterministically serialize schema\n        schema_str = json.dumps(schema, sort_keys=True, separators=(\",\", \":\"))\n        return schema_str\n</file>\n    <file path=\"prompt/prompt_loader.py\">\"\"\"\nPromptLoader: Lazy-loading prompt registry.\nThis module provides a class to manage prompts, allowing for lazy loading of prompt files from a specified directory.\n\nIt supports auto-discovery of prompt files with extensions .jinja and .jinja2, and allows for custom keys to be specified.\nIt also ensures that prompt files are created if they do not exist, and caches loaded prompts for efficient access.\n\nProduction use case (auto-discovery)\n    loader = PromptLoader('/path/to/prompts')\n    prompt = loader['example_prompt']  # Loads example_prompt.jinja2 lazily\n\nDevelopment use case (scaffolding the directory + files):\n    loader = PromptLoader('/path/to/prompts', keys=['example_prompt'])\n    prompt = loader['example_prompt']  # Loads example_prompt.jinja2 lazily, creating the file if it doesn't exist\n\"\"\"\n\nfrom pathlib import Path\n\n\nclass PromptLoader:\n    \"\"\"Lazy-loading prompt registry for jinja/jinja2 template files.\"\"\"\n\n    def __init__(self, base_dir, keys=None):\n        self.base_dir = Path(base_dir)\n\n        # Ensure base_dir exists\n        self.base_dir.mkdir(parents=True, exist_ok=True)\n\n        if keys is None:\n            # Auto-discover jinja files\n            self.file_map = {}\n            for pattern in [\"*.jinja\", \"*.jinja2\"]:\n                for file_path in self.base_dir.glob(pattern):\n                    key = file_path.stem  # filename without extension\n                    self.file_map[key] = file_path.name\n        else:\n            # Generate file_map from keys (key -&gt; key.jinja2)\n            self.file_map = {}\n            for key in keys:\n                filename = f\"{key}.jinja2\"\n                self.file_map[key] = filename\n\n                # Create file if it doesn't exist\n                full_path = self.base_dir / filename\n                if not full_path.exists():\n                    full_path.touch()\n\n        self._cache = {}\n\n    @property\n    def keys(self):\n        return list(self.file_map.keys())\n\n    def __getitem__(self, key):\n        if key not in self._cache:\n            if key not in self.file_map:\n                raise KeyError(f\"Unknown prompt: {key}\")\n\n            # Handle both strings and Path objects\n            file_path = Path(self.file_map[key])\n            if not file_path.is_absolute():\n                file_path = self.base_dir / file_path\n\n            from conduit.prompt.prompt import Prompt\n\n            self._cache[key] = Prompt.from_file(file_path)\n\n        return self._cache[key]\n\n    def __contains__(self, key):\n        return key in self.file_map\n</file>\n    <file path=\"prompt/__init__.py\"/>\n    <file path=\"prompt/prompt.py\">\"\"\"\nPrompt class -- coordinates templates, input variables, and rendering.\n\"\"\"\n\nfrom jinja2 import Environment, StrictUndefined, meta\nfrom conduit.logs.logging_config import get_logger\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from pathlib import Path\n\nlogger = get_logger(__name__)\n\n# Define jinja2 environment that we will use across all prompts.\nenv = Environment(\n    undefined=StrictUndefined\n)  # set jinja2 to throw errors if a variable is undefined\n\n\nclass Prompt:\n    \"\"\" \"\n    Takes a jinja2 ready string (note: not an actual Template object; that's created by the class).\n    The three stages of prompt creation:\n    - the prompt string, which is provided to the class\n    - the jinja template created from the prompt string\n    - the rendered prompt, which is returned by the class and submitted to the LLM model.\n    \"\"\"\n\n    def __init__(self, prompt_string: str):\n        self.prompt_string = prompt_string\n        self.template = env.from_string(prompt_string)\n\n    def render(self, input_variables: dict) -&gt; str:\n        \"\"\"\n        takes a dictionary of variables\n        \"\"\"\n        rendered_prompt = self.template.render(\n            **input_variables\n        )  # this takes all named variables from the dictionary we pass to this.\n        return rendered_prompt\n\n    @property\n    def input_schema(self) -&gt; set:\n        \"\"\"\n        Returns a set of variable names from the template.\n        This can be used to validate that the input variables match the template.\n        \"\"\"\n        parsed_content = env.parse(self.prompt_string)\n        return meta.find_undeclared_variables(parsed_content)\n\n    @classmethod\n    def from_file(cls, filename: \"str | Path\") -&gt; \"Prompt\":\n        \"\"\"\n        Creates a Prompt object from a file containing the prompt string.\n        \"\"\"\n        from pathlib import Path\n\n        # Coerce to Path object\n        if not isinstance(filename, str):\n            try:\n                filename = Path(filename)\n            except TypeError as e:\n                logger.error(f\"Invalid filename type: {type(filename)}\")\n                raise e\n\n        assert isinstance(filename, Path), \"filename must be a Path object\"\n\n        # Confirm file exists\n        if not filename.exists():\n            raise FileNotFoundError(f\"Prompt file {filename} does not exist.\")\n\n        # Confirm file is .jinja2 or .jinja\n        if filename.suffix not in {\".jinja2\", \".jinja\"}:\n            raise ValueError(\n                f\"Prompt file {filename} must be a .jinja2 or .jinja file.\"\n            )\n\n        # Read the file content\n        with filename.open(\"r\", encoding=\"utf-8\") as file:\n            prompt_string = file.read()\n\n        # Create and return a Prompt object\n        return cls(prompt_string)\n\n    def __repr__(self):\n        attributes = \", \".join(\n            [f\"{k}={repr(v)[:50]}\" for k, v in self.__dict__.items()]\n        )\n        return f\"{self.__class__.__name__}({attributes})\"\n</file>\n    <file path=\"logs/logging_config.py\">import logging, sys\n\n# Global logger dictionary to ensure we return the same logger instance for each module\n_loggers = {}\n\n# In logging_config.py\n_configured = False\n\n\ndef get_logger(name=None, level=None) -&gt; logging.Logger:\n    \"\"\"\n    Returns the logger with the specified name, creating it if necessary.\n    Using this function ensures the same logger instance is returned for the same name.\n\n    Args:\n        name: The logger name (typically __name__ from the calling module)\n        level: Optional level to set for this specific logger\n\n    Returns:\n        A configured logger instance\n    \"\"\"\n    global _loggers\n\n    # Default to the root logger if no name is provided\n    if name is None:\n        name = \"ConduitLogger\"\n\n    if name in _loggers:\n        logger = _loggers[name]\n        if level is not None:\n            logger.setLevel(level)\n        return logger\n\n    # Create a new logger\n    logger = logging.getLogger(name)\n\n    # Set level if provided; otherwise inherit from parent\n    if level is not None:\n        logger.setLevel(level)\n\n    # Store in cache\n    _loggers[name] = logger\n\n    return logger\n\n\ndef configure_logging(\n    level=logging.INFO,\n    console=True,\n):\n    \"\"\"\n    Configure the root logging settings for the entire application.\n    Call this once at application startup.\n\n    Args:\n        level: The logging level (default: INFO)\n        log_file: Optional file to log to (default: None)\n        trace_mode: If True, add detailed debugging info with line numbers\n        console: If True, log to console (via stderr)\n    \"\"\"\n    # Ensure this function is only called once\n    global _configured\n    if _configured:\n        return get_logger(\"Conduit\")  # Return existing logger\n\n    _configured = True\n\n    # Create a log formatter with trace information if requested\n    log_format = (\n        \"%(asctime)s [%(levelname)s] %(name)s:%(lineno)d (%(funcName)s) - %(message)s\"\n    )\n\n    formatter = logging.Formatter(log_format)\n\n    # Set up root logger\n    root_logger = logging.getLogger()\n    root_logger.setLevel(level)\n\n    # Remove any existing handlers to avoid duplicate logs\n    for handler in root_logger.handlers[:]:\n        root_logger.removeHandler(handler)\n\n    # Add console handler if requested, using stderr instead of stdout\n    if console:\n        console_handler = logging.StreamHandler(sys.stderr)\n        console_handler.setFormatter(formatter)\n        root_logger.addHandler(console_handler)\n\n    # Create the main application logger\n    logger = get_logger(\"ConduitLogger\")\n    logger.info(\n        \"Audio pipeline logging configured with level: %s, trace_mode: True\",\n        logging.getLevelName(level),\n    )\n\n    return logger\n</file>\n    <file path=\"message/imagemessage.py\">\"\"\"\nThere are two basic message formats:\n1. OpenAI (applicable to ollama, groq, gemini, etc.)\n2. Anthropic (the one hold out)\n\nWe have a basic ImageMessage class, which is a wrapper for the OpenAI and Anthropic formats.\n\"\"\"\n\nfrom pydantic import BaseModel, Field\nfrom conduit.message.message import Message, MessageType, Role\nfrom conduit.message.convert_image import convert_image, convert_image_file\nfrom conduit.logs.logging_config import get_logger\nfrom pathlib import Path\nfrom typing import override\nimport re\n\nlogger = get_logger(__name__)\n\n# Map PIL formats to MIME types\nformat_to_mime = {\n    \".jpeg\": \"image/jpeg\",\n    \".jpg\": \"image/jpeg\",\n    \".png\": \"image/png\",\n    \".gif\": \"image/gif\",\n    \".webp\": \"image/webp\",\n}\n\n\ndef is_base64_simple(s):\n    \"\"\"\n    Simple validation for base64 strings.\n    \"\"\"\n    return bool(re.match(r\"^[A-Za-z0-9+/]*={0,2}$\", s)) and len(s) % 4 == 0\n\n\ndef extension_to_mimetype(image_file: Path) -&gt; str:\n    \"\"\"\n    Given a Path object, return the mimetype.\n    \"\"\"\n    extension = image_file.suffix.lower()\n    try:\n        mimetype = format_to_mime[extension]\n        return mimetype\n    except:\n        raise ValueError(\n            f\"Unsupported image format: {extension}. Supported formats are: {', '.join(format_to_mime.keys())}\"\n        )\n\n\n# Provider-specific classes -- these are NOT serialized at all, but constructed.\nclass AnthropicTextContent(BaseModel):\n    type: str = \"text\"\n    text: str\n\n\nclass AnthropicImageSource(BaseModel):\n    type: str = \"base64\"\n    media_type: str\n    data: str\n\n\nclass AnthropicImageContent(BaseModel):\n    type: str = \"image\"\n    source: AnthropicImageSource\n\n\nclass AnthropicImageMessage(BaseModel):\n    \"\"\"\n    ImageMessage should have a single ImageContent and a single TextContent object.\n    \"\"\"\n\n    role: Role = Field(default=\"user\", description=\"The role of the message sender.\")\n    content: list[AnthropicImageContent | AnthropicTextContent]  # type: ignore\n\n\n# OpenAI-specific message classes\nclass OpenAITextContent(BaseModel):\n    type: str = \"text\"\n    text: str = Field(description=\"The text content of the message, i.e. the prompt.\")\n\n\nclass OpenAIImageUrl(BaseModel):\n    \"\"\"Nested object for OpenAI image URL structure\"\"\"\n\n    url: str = Field(description=\"The data URL with base64 image\")\n\n\nclass OpenAIImageContent(BaseModel):\n    \"\"\"\n    OpenAI requires image_url to be an object, not a string\n    \"\"\"\n\n    type: str = \"image_url\"\n    image_url: OpenAIImageUrl = Field(description=\"The image URL object\")\n\n\nclass OpenAIImageMessage(BaseModel):\n    \"\"\"\n    ImageMessage should have a single ImageContent and a single TextContent object.\n    \"\"\"\n\n    role: Role = Field(default=\"user\", description=\"The role of the message sender.\")\n    content: list[OpenAIImageContent | OpenAITextContent]  # type: ignore\n\n\n# Our base ImageMessage class with serialization support\nclass ImageMessage(Message):\n    \"\"\"\n    ImageMessage with serialization/deserialization support.\n\n    Create with:\n    1. ImageMessage.from_image_file(image_file, text_content) - from file\n    2. ImageMessage(role=\"user\", content=[image_data, text], ...) - from processed data\n\n    You can convert it to provider formats with to_openai() and to_anthropic() methods.\n    \"\"\"\n\n    message_type: MessageType = Field(default=\"image\", exclude=True, repr=False)\n    content: list[str] = Field(\n        default_factory=list, description=\"[image_content, text_content]\"\n    )\n    text_content: str = Field(\n        description=\"The text content/prompt\", exclude=True, repr=False\n    )\n    image_content: str = Field(\n        description=\"Base64-encoded PNG image\", exclude=True, repr=False\n    )\n    mime_type: str = Field(\n        description=\"Always 'image/png' after processing\",\n        default=\"image/png\",\n        exclude=True,\n        repr=False,\n    )\n\n    @classmethod\n    def from_image_file(\n        cls, image_file: str | Path, text_content: str, role: str = \"user\"\n    ) -&gt; \"ImageMessage\":\n        \"\"\"\n        Create ImageMessage from image file.\n\n        Args:\n            image_file: Path to image file (any supported format)\n            text_content: Text prompt/question about the image\n            role: Message role (default: \"user\")\n\n        Returns:\n            ImageMessage with processed image data\n\n        Raises:\n            FileNotFoundError: If image file doesn't exist\n            ValueError: If image format is unsupported\n        \"\"\"\n        image_path = Path(image_file)\n\n        if not image_path.exists():\n            raise FileNotFoundError(f\"Image file {image_path} does not exist\")\n\n        # Validate file format\n        if image_path.suffix.lower() not in {\".png\", \".jpg\", \".jpeg\", \".gif\", \".webp\"}:\n            raise ValueError(f\"Unsupported image format: {image_path.suffix}\")\n\n        # Convert image to optimized PNG base64\n        try:\n            image_content = convert_image_file(image_path)\n        except Exception as e:\n            raise ValueError(f\"Failed to process image file {image_path}: {e}\")\n\n        # Validate the conversion worked\n        if not is_base64_simple(image_content):\n            raise ValueError(\"Image conversion produced invalid base64 data\")\n\n        return cls(\n            role=role,\n            content=[image_content, text_content],\n            text_content=text_content,\n            image_content=image_content,\n            mime_type=\"image/png\",  # Always PNG after convert_image_file()\n        )\n\n    @classmethod\n    def from_base64(\n        cls,\n        image_content: str,\n        text_content: str,\n        mime_type: str = \"image/png\",\n        role: str = \"user\",\n    ) -&gt; \"ImageMessage\":\n        \"\"\"\n        Create ImageMessage from base64 image data.\n\n        Args:\n            image_content: Base64-encoded image data\n            text_content: Text prompt/question about the image\n            mime_type: MIME type of the image (will be converted to PNG)\n            role: Message role (default: \"user\")\n\n        Returns:\n            ImageMessage with processed image data\n        \"\"\"\n        # Validate base64\n        if not is_base64_simple(image_content):\n            raise ValueError(\"Invalid base64 image data\")\n\n        # Convert to PNG if needed\n        if mime_type != \"image/png\":\n            try:\n                image_content = convert_image(image_content)\n                mime_type = \"image/png\"\n            except Exception as e:\n                raise ValueError(f\"Failed to convert image to PNG: {e}\")\n\n        return cls(\n            role=role,\n            content=[image_content, text_content],\n            text_content=text_content,\n            image_content=image_content,\n            mime_type=mime_type,\n        )\n\n    def __repr__(self):\n        return f\"ImageMessage(role={self.role}, text_content='{self.text_content[:30]}...', mime_type={self.mime_type})\"\n\n    def display(self):\n        \"\"\"\n        Display a base64-encoded image using chafa.\n        Your mileage may vary depending on the terminal and chafa version.\n        \"\"\"\n        import subprocess, base64, os\n\n        try:\n            image_data = base64.b64decode(self.image_content)\n            cmd = [\"chafa\", \"-\"]\n\n            # If in tmux or SSH, force text mode for consistency\n            if (\n                os.environ.get(\"TMUX\")\n                or os.environ.get(\"SSH_CLIENT\")\n                or os.environ.get(\"SSH_CONNECTION\")\n            ):\n                cmd.extend([\"--format\", \"symbols\", \"--symbols\", \"block\"])\n            process = subprocess.Popen(cmd, stdin=subprocess.PIPE)\n            process.communicate(input=image_data)\n        except Exception as e:\n            print(f\"Error displaying image: {e}\")\n\n    # Serialization methods\n    @override\n    def to_cache_dict(self) -&gt; dict:\n        \"\"\"\n        Convert the ImageMessage to a dictionary for caching.\n\n        Returns:\n            A dictionary representation of the ImageMessage.\n        \"\"\"\n        return {\n            \"message_type\": self.message_type,\n            \"role\": self.role,\n            \"text_content\": self.text_content,\n            \"image_content\": self.image_content,\n            \"mime_type\": self.mime_type,\n        }\n\n    @override\n    @classmethod\n    def from_cache_dict(cls, data: dict) -&gt; \"ImageMessage\":\n        \"\"\"\n        Create an ImageMessage from a cached dictionary.\n\n        Args:\n            data: Dictionary containing the cached data.\n\n        Returns:\n            An ImageMessage instance.\n\n        Raises:\n            ValidationError: If the data is invalid.\n        \"\"\"\n        return cls(\n            message_type=data[\"message_type\"],\n            role=data[\"role\"],\n            text_content=data[\"text_content\"],\n            image_content=data[\"image_content\"],\n            mime_type=data[\"mime_type\"],\n        )\n\n    # API compatibility methods. These are overridden in AudioMessage, ImageMessage, and other specialized message types (TBD).\n    @override\n    def to_openai(self) -&gt; dict:\n        \"\"\"Converts the ImageMessage to the OpenAI format.\"\"\"\n        # Create the nested URL object\n        image_url_obj = OpenAIImageUrl(\n            url=f\"data:{self.mime_type};base64,{self.image_content}\"\n        )\n\n        # Create image content with the nested object\n        image_content = OpenAIImageContent(image_url=image_url_obj)\n\n        # Create text content\n        text_content = OpenAITextContent(text=self.text_content)\n\n        openaiimagemessage = OpenAIImageMessage(\n            role=self.role,\n            content=[text_content, image_content],  # Note: text first, then image\n        )\n        return openaiimagemessage.model_dump()\n\n    @override\n    def to_anthropic(self) -&gt; dict:\n        \"\"\"Converts the ImageMessage to the Anthropic format.\"\"\"\n        image_source = AnthropicImageSource(\n            type=\"base64\", media_type=self.mime_type, data=self.image_content\n        )\n        image_content = AnthropicImageContent(source=image_source)\n        text_content = AnthropicTextContent(text=self.text_content)\n        anthropicimagemessage = AnthropicImageMessage(\n            role=self.role, content=[image_content, text_content]\n        )\n        return anthropicimagemessage.model_dump()\n\n    @override\n    def to_google(self) -&gt; dict:\n        \"\"\"\n        Defaults to OpenAI format for Google Gemini.\n        \"\"\"\n        return self.to_openai()\n\n    @override\n    def to_ollama(self) -&gt; dict:\n        \"\"\"\n        Defaults to OpenAI format for Ollama.\n        \"\"\"\n        return self.to_openai()\n\n    @override\n    def to_perplexity(self) -&gt; dict:\n        \"\"\"\n        Convert message to Perplexity API format.\n        \"\"\"\n        raise NotImplementedError(\"Perplexity API does not support image messages.\")\n</file>\n    <file path=\"message/messagestore.py\">\"\"\"\nA MessageStore object inherits from Messages and adds persistence and logging capabilities.\nWith MessageStore, you can:\n- Use all Messages methods (append, extend, indexing, etc.) directly\n- Automatically persist changes to TinyDB\n- Log conversations in human-readable format\n- All while being a drop-in replacement for Messages objects\n\nThe MessageStore IS a Messages object with superpowers.\n\"\"\"\n\nfrom conduit.message.message import Message, Role\nfrom conduit.message.textmessage import TextMessage\nfrom conduit.message.imagemessage import ImageMessage\nfrom conduit.message.audiomessage import AudioMessage\nfrom conduit.message.messages import Messages\nfrom conduit.logs.logging_config import get_logger\nfrom xdg_base_dirs import xdg_data_home, xdg_state_home\nfrom rich.console import Console\nfrom rich.rule import Rule\nfrom pydantic import BaseModel, Field\nfrom pathlib import Path\nfrom tinydb import TinyDB\nfrom typing import ClassVar, override\nfrom datetime import datetime\nimport os\n\nlogger = get_logger(__name__)\n\nDEFAULT_HISTORY_FILE = xdg_data_home() / \"conduit\" / \"history.json\"\nDEFAULT_LOG_FILE = xdg_state_home() / \"conduit\" / \"conduit.log\"\n\n\nclass MessageStore(Messages):\n    \"\"\"\n    A Messages object with automatic persistence.\n\n    \u26a0\ufe0f  MUTATION WARNING: All list operations (append, extend, etc.)\n        will automatically persist to database if history_file was provided.\n\n    Side Effects:\n        - append() \u2192 database write (if persistent=True)\n        - extend() \u2192 multiple database writes\n        - clear() \u2192 database truncation\n    \"\"\"\n\n    # Add Pydantic fields (Messages inherits from BaseModel)\n    console: Console | None = Field(default=None, exclude=True, repr=False)\n    auto_save: bool = Field(default=True, exclude=True)\n    persistent: bool = Field(default=False, exclude=True)\n    logging: bool = Field(default=False, exclude=True)\n    pruning: bool = Field(default=False, exclude=True)\n    history_file: Path | None = Field(\n        default=DEFAULT_HISTORY_FILE, exclude=True, repr=False\n    )\n    log_file: Path | None = Field(default=DEFAULT_LOG_FILE, exclude=True, repr=False)\n    db: TinyDB | None = Field(default=None, exclude=True, repr=False)\n\n    model_config: ClassVar = {\n        \"arbitrary_types_allowed\": True\n    }  # Allow Console and TinyDB types\n\n    def __init__(\n        self,\n        messages: list[Message] | Messages | None = None,\n        console: Console | None = None,\n        history_file: str | Path = \"\",\n        log_file: str | Path = \"\",\n        pruning: bool = False,\n        auto_save: bool = True,\n    ):\n        \"\"\"\n        Initialize MessageStore with optional persistence and logging.\n\n        Args:\n            messages: Initial list of messages (same as Messages class)\n            console: Rich console for formatting output\n            history_file: Path to TinyDB database file (.json extension recommended)\n            log_file: Path to human-readable log file\n            pruning: Whether to automatically prune old messages\n            auto_save: Whether to automatically save changes to database\n        \"\"\"\n        logger.info(\"Initializing MessageStore class.\")\n        # Coerce Messages object to list for parent class\n        if isinstance(messages, Messages):\n            messages = messages.messages  # Extract the list\n        # Initialize parent Messages class\n        super().__init__(messages)\n\n        # Use existing console or create a new one\n        if not console:\n            self.console = Console(width=100)\n        else:\n            self.console = console\n\n        self.auto_save = auto_save\n\n        # Config history database if requested\n        if history_file:\n            # Ensure .json extension for TinyDB\n            if not str(history_file).endswith(\".json\"):\n                history_file = str(history_file) + \".json\"\n            self.history_file = Path(history_file)\n            self.persistent = True\n            # Initialize TinyDB\n            self.db = TinyDB(self.history_file)\n            # Load existing messages from database\n            self.load()\n        else:\n            self.history_file = Path()\n            self.persistent = False\n            self.db = None\n\n        # Config log file if requested\n        if log_file:\n            self.log_file = Path(log_file)\n            with open(self.log_file, \"a\"):\n                os.utime(self.log_file, None)\n            self.logging = True\n        else:\n            self.log_file = Path()\n            self.logging = False\n\n        # Set the prune flag\n        self.pruning = pruning\n\n    def _auto_save_if_enabled(self):\n        \"\"\"Save to database if auto_save is enabled.\"\"\"\n        logger.debug(\"Auto-save check.\")\n        if self.auto_save and self.persistent:\n            self.save()\n\n    def _log_message_if_enabled(self, message: Message):\n        \"\"\"Log message if logging is enabled.\"\"\"\n        logger.debug(\"Logging check.\")\n        if self.logging:\n            self.write_to_log(message)\n\n    def ensure_system_message(self, system_message_content: str) -&gt; None:\n        \"\"\"\n        Ensure system message exists. Idempotent.\n        \"\"\"\n        if not self.system_message:\n            from conduit.message.textmessage import create_system_message\n\n            msg = create_system_message(system_message_content)[0]\n            self.insert(0, msg)\n\n    # Override Messages methods to add persistence and logging\n    @override\n    def append(self, message: Message) -&gt; None:\n        \"\"\"Add a message to the end of the list with persistence.\"\"\"\n        logger.debug(\"Appending message with persistence.\")\n        super().append(message)\n        self._log_message_if_enabled(message)\n        self._auto_save_if_enabled()\n\n    @override\n    def extend(self, messages: list[Message]) -&gt; None:\n        \"\"\"Extend the list with multiple messages with persistence.\"\"\"\n        logger.debug(\"Extending messages with persistence.\")\n        super().extend(messages)\n        for message in messages:\n            self._log_message_if_enabled(message)\n        self._auto_save_if_enabled()\n\n    @override\n    def insert(self, index: int, message: Message) -&gt; None:\n        \"\"\"Insert a message at the specified index with persistence.\"\"\"\n        logger.debug(\"Inserting message with persistence.\")\n        super().insert(index, message)\n        self._log_message_if_enabled(message)\n        self._auto_save_if_enabled()\n\n    @override\n    def remove(self, message: Message) -&gt; None:\n        \"\"\"Remove the first occurrence of a message with persistence.\"\"\"\n        logger.debug(\"Removing message with persistence.\")\n        super().remove(message)\n        self._auto_save_if_enabled()\n\n    @override\n    def pop(self, index: int = -1) -&gt; Message:\n        \"\"\"Remove and return message at index with persistence.\"\"\"\n        logger.debug(\"Popping message with persistence.\")\n        message = super().pop(index)\n        self._auto_save_if_enabled()\n        return message\n\n    @override\n    def clear(self) -&gt; None:\n        \"\"\"Remove all messages with persistence.\"\"\"\n        logger.debug(\"Clearing all messages with persistence.\")\n        super().clear()\n        self._auto_save_if_enabled()\n\n    @override\n    def __setitem__(self, key, value):\n        \"\"\"Set message(s) by index or slice with persistence.\"\"\"\n        logger.debug(\"Setting item(s) with persistence.\")\n        super().__setitem__(key, value)\n        if isinstance(value, Message):\n            self._log_message_if_enabled(value)\n        elif isinstance(value, list):\n            for msg in value:\n                if isinstance(msg, Message):\n                    self._log_message_if_enabled(msg)\n        self._auto_save_if_enabled()\n\n    @override\n    def __delitem__(self, key):\n        \"\"\"Delete message(s) by index or slice with persistence.\"\"\"\n        logger.debug(\"Deleting item(s) with persistence.\")\n        super().__delitem__(key)\n        self._auto_save_if_enabled()\n\n    @override\n    def __iadd__(self, other) -&gt; \"MessageStore\":\n        \"\"\"In-place concatenation with persistence.\"\"\"\n        logger.debug(\"In-place addition with persistence.\")\n        result = super().__iadd__(other)\n        if isinstance(other, (list, Messages)):\n            for message in other:\n                if isinstance(message, Message):\n                    self._log_message_if_enabled(message)\n        self._auto_save_if_enabled()\n        return self\n\n    # MessageStore-specific methods\n    def add_new(self, role: Role, content: str) -&gt; None:\n        \"\"\"\n        Create and add a new message (convenience method).\n        \"\"\"\n        logger.debug(\"Adding new message via convenience method.\")\n        message = TextMessage(role=role, content=content)\n        self.append(message)  # This will handle persistence and logging\n\n    def add_response(self, response: \"Response\") -&gt; None:\n        \"\"\"\n        Update the store for a successful Response.\n        This adds two messages: one for the user and one for the assistant.\n        \"\"\"\n        logger.debug(\"Adding messages from Response object.\")\n        if not isinstance(response.params.messages[-1], Message):\n            raise ValueError(\n                \"Last message in params.messages must be a Message object.\"\n            )\n        last_user_message = response.params.messages[-1]  # Last user message\n        if last_user_message.role != \"user\":\n            raise ValueError(\"Last message in params.message must be a user message.\")\n        last_assistant_message = response.message  # Last assistant message\n        if last_assistant_message.role != \"assistant\":\n            raise ValueError(\"Last message in response must be an assistant message.\")\n        # Add the messages\n        self.append(last_user_message)\n        self.append(last_assistant_message)\n\n    def write_to_log(self, item: str | BaseModel) -&gt; None:\n        \"\"\"\n        Writes a log to the log file.\n        \"\"\"\n        if not self.logging:\n            return\n\n        if isinstance(item, str):\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as file:\n                file_console = Console(file=file, force_terminal=True)\n                file_console.print(f\"[bold magenta]{item}[/bold magenta]\\n\")\n\n        elif isinstance(item, Message):\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as file:\n                file_console = Console(file=file, force_terminal=True)\n                file_console.print(Rule(title=\"Message\", style=\"bold green\"))\n                file_console.print(f\"[bold cyan]{item.role}:[/bold cyan]\")\n                try:\n                    if item.role == \"user\":\n                        file_console.print(f\"[yellow]{item.content}[/yellow]\\n\")\n                    elif item.role == \"assistant\":\n                        file_console.print(f\"[blue]{item.content}[/blue]\\n\")\n                    elif item.role == \"system\":\n                        file_console.print(f\"[green]{item.content}[/green]\\n\")\n                    else:\n                        file_console.print(f\"[white]{item.content}[/white]\\n\")\n                except Exception as e:\n                    print(f\"MessageStore error: {e}\")\n\n    def save(self):\n        \"\"\"\n        Saves the current messages to TinyDB.\n        \"\"\"\n        logger.debug(\"Saving messages to TinyDB.\")\n        if not self.persistent or not isinstance(self.db, TinyDB):\n            return\n\n        try:\n            # Clear existing messages and save all current messages\n            self.db.truncate()\n\n            # Save all current messages\n            for i, message in enumerate(self.messages):\n                if message:  # Handle None messages\n                    doc = {\n                        \"timestamp\": datetime.now().isoformat(),\n                        \"message_index\": i,\n                        \"message_data\": message.to_cache_dict(),\n                    }\n                    self.db.insert(doc)\n\n        except Exception as e:\n            logger.error(f\"Error saving history: {e}\")\n            print(f\"Error saving history: {e}\")\n\n    def load(self):\n        \"\"\"\n        Loads messages from TinyDB, replacing current messages.\n        \"\"\"\n        logger.debug(\"Loading messages from TinyDB.\")\n        if not self.persistent or self.db == None:\n            print(\"This message store is not persistent.\")\n            return\n\n        try:\n            # Get all messages from database\n            message_docs = self.db.all()\n\n            # Sort by message_index to maintain order\n            message_docs.sort(key=lambda x: x.get(\"message_index\", 0))\n\n            # Deserialize messages\n            messages_list = []\n            for doc in message_docs:\n                message_data = doc[\"message_data\"]\n                message_type = message_data.get(\"message_type\", \"Message\")\n\n                if message_type == \"ImageMessage\":\n                    messages_list.append(ImageMessage.from_cache_dict(message_data))\n                elif message_type == \"AudioMessage\":\n                    messages_list.append(AudioMessage.from_cache_dict(message_data))\n                else:\n                    messages_list.append(Message.from_cache_dict(message_data))\n\n            # Replace current messages (disable auto_save temporarily)\n            old_auto_save = self.auto_save\n            self.auto_save = False\n\n            self.clear()\n            self.extend(messages_list)\n\n            self.auto_save = old_auto_save\n\n            if self.pruning:\n                self.prune()\n\n        except Exception as e:\n            logger.error(f\"Error loading history: {e}\")\n            print(f\"Error loading history: {e}. Starting with empty history.\")\n            super().clear()  # Don't trigger auto_save for error case\n\n    def prune(self):\n        \"\"\"\n        Prunes the history to the last 20 messages.\n        \"\"\"\n        logger.debug(\"Pruning message history.\")\n        if len(self) &gt; 20:\n            # Keep last 20 messages\n            pruned_messages = list(self)[-20:]\n\n            # Disable auto_save temporarily to avoid multiple saves\n            old_auto_save = self.auto_save\n            self.auto_save = False\n\n            self.clear()\n            self.extend(pruned_messages)\n\n            self.auto_save = old_auto_save\n\n            # Save the pruned version\n            if self.persistent:\n                self.save()\n\n    def query_failed(self):\n        \"\"\"\n        Removes the last message if it's a user message (handles failed queries).\n        \"\"\"\n        logger.debug(\"Handling query failure by removing last user message if present.\")\n        if self and self.last() and self.last().role == \"user\":\n            self.pop()  # This will auto-save\n            if self.logging:\n                self.write_to_log(\"Query failed, removing last user message.\")\n\n    def view_history(self):\n        \"\"\"\n        Pretty prints the history.\n        \"\"\"\n        logger.debug(\"Viewing message history.\")\n        if len(self) == 0:\n            self.console.print(\"No history (yet).\", style=\"bold red\")\n            return\n\n        for index, message in enumerate(self):\n            if message:\n                content = str(message.content)[:50].replace(\"\\n\", \" \")\n                self.console.print(\n                    f\"[green]{index + 1}.[/green] [bold white]{message.role}:[/bold white] [yellow]{content}[/yellow]\"\n                )\n\n    def clear_logs(self):\n        \"\"\"Clears the log file.\"\"\"\n        if self.logging:\n            with open(self.log_file, \"w\") as file:\n                file.write(\"\")\n\n    def delete_database(self):\n        \"\"\"Deletes the TinyDB database file.\"\"\"\n        logger.debug(\"Deleting TinyDB database file.\")\n        if self.persistent and self.history_file.exists():\n            try:\n                if self.db:\n                    self.db.close()\n                self.history_file.unlink()\n                self.db = None\n            except Exception as e:\n                print(f\"Error deleting database: {e}\")\n\n    # Enhanced Methods\n    def get(self, index: int) -&gt; Message | None:\n        \"\"\"Gets a message by 1-based index (convenience method).\"\"\"\n        logger.debug(f\"Getting message at 1-based index {index}.\")\n        if 1 &lt;= index &lt;= len(self):\n            return self[index - 1]\n        return None\n\n    def copy(self) -&gt; \"MessageStore\":\n        \"\"\"Return a copy of the MessageStore (without persistence).\"\"\"\n        logger.debug(\"Copying MessageStore (without persistence).\")\n        return MessageStore(\n            messages=list(self.messages),\n            console=self.console,\n            # Don't copy persistence settings - new object manages its own state\n        )\n\n    def __bool__(self) -&gt; bool:\n        \"\"\"Override; while Messages should be false if empty, MessageStore should be true if it exists.\"\"\"\n        return True\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Enhanced representation showing persistence status.\"\"\"\n        persistent_info = f\"persistent={self.persistent}\"\n        return f\"MessageStore({len(self)} messages, {persistent_info})\"\n</file>\n    <file path=\"message/message.py\">\"\"\"\nMessage is the default message type recognized as industry standard (role + content).\nOur Message class is inherited from specialized types like TextMessage, AudioMessage, ImageMessage, etc.\n\nAll Messages have:\n(1) serialization methods to convert to/from dictionaries for caching and API compatibility.\n\n```\nto_cache_dict()\nfrom_cache_dict()\n```\n\n(2) methods for API compatibility with OpenAI, Anthropic, Google, and Perplexity, which are invoked by the individual provider clients.\n\n```\nto_openai()\nto_anthropic()\nto_google()\n...\n```\n\"\"\"\n\nfrom abc import abstractmethod, ABC\nfrom conduit.logs.logging_config import get_logger\nfrom pydantic import BaseModel\nfrom typing import Literal, Any\n\nlogger = get_logger(__name__)\n\n# Useful type aliases\nRole = Literal[\"user\", \"assistant\", \"system\"]\nMessageType = Literal[\"text\", \"audio\", \"image\"]\n\n\nclass Message(BaseModel, ABC):\n    \"\"\"Base message class - abstract with required Pydantic functionality\"\"\"\n\n    message_type: MessageType\n    role: Role\n    content: Any\n\n    # Serialization methods to convert the message to a dictionary\n    @abstractmethod\n    def to_cache_dict(self) -&gt; dict:\n        \"\"\"\n        Serializes the message to a dictionary for caching.\n        Must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method.\")\n\n    @classmethod\n    def from_cache_dict(cls, cache_dict: dict) -&gt; \"Message\":\n        \"\"\"Parse JSON with lazy imports to avoid circular dependencies\"\"\"\n        message_type = cache_dict[\"message_type\"]\n\n        # Import only when needed\n        if message_type == \"text\":\n            from conduit.message.textmessage import TextMessage\n\n            return TextMessage.from_cache_dict(cache_dict)\n        elif message_type == \"audio\":\n            from conduit.message.audiomessage import AudioMessage\n\n            return AudioMessage.from_cache_dict(cache_dict)\n        elif message_type == \"image\":\n            from conduit.message.imagemessage import ImageMessage\n\n            return ImageMessage.from_cache_dict(cache_dict)\n        else:\n            raise ValueError(f\"Unknown message type: {message_type}\")\n\n    # API compatibility methods. These are overridden in AudioMessage, ImageMessage, and other specialized message types (TBD).\n    def to_openai(self) -&gt; dict:\n        \"\"\"\n        Convert message to OpenAI API format.\n        \"\"\"\n        return self.to_cache_dict()\n\n    def to_anthropic(self) -&gt; dict:\n        \"\"\"\n        Convert message to Anthropic API format.\n        \"\"\"\n        return self.to_cache_dict()\n\n    def to_google(self) -&gt; dict:\n        \"\"\"\n        Convert message to Google API format.\n        \"\"\"\n        return self.to_cache_dict()\n\n    def to_ollama(self) -&gt; dict:\n        \"\"\"\n        Convert message to Ollama API format.\n        \"\"\"\n        return self.to_cache_dict()\n\n    def to_perplexity(self) -&gt; dict:\n        \"\"\"\n        Convert message to Perplexity API format.\n        \"\"\"\n        return self.to_cache_dict()\n</file>\n    <file path=\"message/convert_image.py\">from PIL import Image\nimport io, base64\nfrom pathlib import Path\n\n\ndef convert_to_png_base64(base64_data: str) -&gt; str:\n    \"\"\"\n    Convert base64 image (any format) to PNG base64.\n    \n    Args:\n        base64_data: Base64 string of image in any format\n        \n    Returns:\n        Base64 string of PNG image\n    \"\"\"\n    # Decode base64 to bytes\n    image_bytes = base64.b64decode(base64_data)\n    \n    with Image.open(io.BytesIO(image_bytes)) as img:\n        # Convert to appropriate mode for PNG\n        if img.mode in ('P', 'LA'):\n            img = img.convert('RGBA')\n        elif img.mode in ('L', '1'):\n            img = img.convert('RGB')\n        \n        # Save as PNG to buffer\n        buffer = io.BytesIO()\n        img.save(buffer, format=\"PNG\", optimize=True)\n        \n        # Convert back to base64\n        png_base64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        \n        return png_base64\n\n\ndef resize_png_base64(png_base64: str, target_size: int = 1024) -&gt; str:\n    \"\"\"\n    Resize PNG base64 to optimal size for LLM vision models.\n    \n    Args:\n        png_base64: Base64 string of PNG image\n        target_size: Maximum dimension in pixels (default 1024 for OCR)\n        \n    Returns:\n        Base64 string of resized PNG\n    \"\"\"\n    # Decode base64 to bytes\n    image_bytes = base64.b64decode(png_base64)\n    \n    with Image.open(io.BytesIO(image_bytes)) as img:\n        # Only resize if larger than target\n        if max(img.size) &gt; target_size:\n            # Calculate new size maintaining aspect ratio\n            ratio = target_size / max(img.size)\n            new_size = (int(img.width * ratio), int(img.height * ratio))\n            img = img.resize(new_size, Image.Resampling.LANCZOS)\n        \n        # Save resized PNG to buffer\n        buffer = io.BytesIO()\n        img.save(buffer, format=\"PNG\", optimize=True)\n        \n        # Convert back to base64\n        resized_base64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        \n        return resized_base64\n\n\ndef convert_image(base64_data: str, target_size: int = 1024) -&gt; str:\n    \"\"\"\n    Convert any base64 image to optimized PNG base64 for LLMs.\n    \n    Pipeline:\n    1. Convert to PNG (if not already)\n    2. Resize to LLM-friendly dimensions\n    \n    Args:\n        base64_data: Base64 string of image in any format\n        target_size: Maximum dimension in pixels (default 1024 for OCR)\n        \n    Returns:\n        Base64 string of optimized PNG\n    \"\"\"\n    # Convert to PNG format\n    png_base64 = convert_to_png_base64(base64_data)\n    \n    # Resize to target dimensions\n    final_base64 = resize_png_base64(png_base64, int(target_size))\n    \n    return final_base64\n\n\ndef convert_image_file(file_path: str | Path, target_size: int = 1024) -&gt; str:\n    \"\"\"\n    Convert image file to optimized PNG base64 for LLMs.\n    \n    Args:\n        file_path: Path to image file (png, jpg, jpeg, gif, webp only)\n        target_size: Maximum dimension in pixels (default 1024 for OCR)\n        \n    Returns:\n        Base64 string of optimized PNG\n        \n    Raises:\n        ValueError: If file format is not supported\n\n    Usage:\n        optimized_base64 = convert_image(original_base64)  # From base64\n        optimized_base64 = convert_image_file(\"photo.jpg\")  # From file path\n    \"\"\"\n    # Validate file format\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n    supported_formats = {'.png', '.jpg', '.jpeg', '.gif', '.webp'}\n    \n    if file_path.suffix.lower() not in supported_formats:\n        raise ValueError(f\"Unsupported file format: {file_path.suffix}. Supported: {', '.join(supported_formats)}\")\n    \n    # Read file to base64\n    with open(file_path, \"rb\") as f:\n        original_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n    \n    # Process through the pipeline\n    return convert_image(original_base64, target_size)\n\n</file>\n    <file path=\"message/audiomessage.py\">from conduit.message.message import Message, MessageType, Role\nfrom conduit.message.imagemessage import OpenAITextContent\nfrom conduit.logs.logging_config import get_logger\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, override\nfrom pathlib import Path\nimport base64, re\n\nlogger = get_logger(__name__)\n\n\ndef is_base64_simple(s):\n    \"\"\"\n    Simple validation for base64 strings.\n    \"\"\"\n    return bool(re.match(r\"^[A-Za-z0-9+/]*={0,2}$\", s)) and len(s) % 4 == 0\n\n\nclass OpenAIInputAudio(BaseModel):\n    \"\"\"\n    We are using Gemini through the OpenAI SDK, so we need to define the input audio format.\n    Gemini usually supports a range of audio filetypes, but when used with OpenAI SDK, it's only mp3 and wav.\n    \"\"\"\n\n    data: str = Field(description=\"The base64-encoded audio data.\")\n    format: Literal[\"mp3\", \"wav\", \"\"] = Field(\n        description=\"The format of the audio data, must be 'mp3' or 'wav'.\"\n    )\n\n\nclass OpenAIAudioContent(BaseModel):\n    \"\"\"\n    Gemini AudioContent should have a single AudioContent object.\n    NOTE: since we are using the OpenAI SDK, we use OpenAITextContent for text.\n    \"\"\"\n\n    input_audio: OpenAIInputAudio = Field(\n        description=\"The input audio data, must be a base64-encoded string with format 'mp3' or 'wav'.\"\n    )\n    type: str = Field(\n        default=\"input_audio\", description=\"The type of content, must be 'input_audio'.\"\n    )\n\n\nclass OpenAIAudioMessage(BaseModel):\n    \"\"\"\n    Gemini AudioMessage should have a single AudioContent and a single TextContent object.\n    NOTE: since we are using the OpenAI SDK, we use OpenAITextContent for text.\n    \"\"\"\n\n    role: Role = Field(default=\"user\", description=\"The role of the message sender.\")\n    content: list[OpenAIAudioContent | OpenAITextContent]  # type: ignore\n\n\nclass AudioMessage(Message):\n    \"\"\"\n    AudioMessage is a message that contains audio content.\n    It can be created from an audio file and contains both the audio content in base64 format\n    \"\"\"\n\n    message_type: MessageType = Field(default=\"audio\", exclude=True, repr=False)\n    content: list[str] = Field(default_factory=list)\n    text_content: str = Field(exclude=True, repr=False)\n    audio_content: str = Field(exclude=True, repr=False)\n    format: Literal[\"wav\", \"mp3\"] = Field(exclude=True, repr=False)\n\n    @classmethod\n    def from_audio_file(\n        cls, audio_file: str | Path, text_content: str, role: str = \"user\"\n    ) -&gt; \"AudioMessage\":\n        \"\"\"\n        Create AudioMessage from audio file.\n\n        Args:\n            audio_file (str | Path): Path to the audio file.\n            text_content (str): Text content associated with the audio.\n            role (str): Role of the message sender, default is \"user\".\n        \"\"\"\n        # Do all the file processing here\n        audio_file = Path(audio_file)\n        if not audio_file.exists():\n            raise FileNotFoundError(f\"Audio file {audio_file} does not exist.\")\n\n        audio_content = cls._convert_audio_to_base64(audio_file)\n        format = audio_file.suffix.lower()[1:]  # \"mp3\" or \"wav\"\n\n        return cls(\n            role=role,\n            content=[audio_content, text_content],\n            text_content=text_content,\n            audio_content=audio_content,\n            format=format,\n        )\n\n    @classmethod\n    def from_base64(\n        cls,\n        audio_content: str,\n        text_content: str,\n        format: str = \"mp3\",\n        role: str = \"user\",\n    ) -&gt; \"AudioMessage\":\n        \"\"\"\n        Create ImageMessage from base64 image data.\n\n        Args:\n            audio_content: Base64-encoded audio data\n            text_content: Text prompt/question about the audio (empty string for tts)\n            format: either \"mp3\" or \"wav\"\n            role: Message role (default: \"user\")\n\n        Returns:\n            AudioMessage with processed audio content.\n        \"\"\"\n        # Validate base64\n        if not is_base64_simple(audio_content):\n            raise ValueError(\"Invalid base64 image data\")\n        # TBD: optional conversion logic\n        # Construct the AudioMessage object\n        return cls(\n            text_content=text_content,\n            audio_content=audio_content,\n            format=format,\n            role=role,\n            content=[audio_content, text_content],\n        )\n\n    @classmethod\n    def _convert_audio_to_base64(cls, file_path: Path) -&gt; str:\n        \"\"\"\n        Convert the audio file to base64 string.\n        \"\"\"\n        with open(file_path, \"rb\") as f:\n            return base64.b64encode(f.read()).decode(\"utf-8\")\n\n    def __repr__(self):\n        \"\"\"\n        String representation of the AudioMessage.\n        \"\"\"\n        return f\"AudioMessage(role={self.role}, content=[{self.text_content}, {self.audio_content[:30]}...], format={self.format})\"\n\n    def play(self):\n        \"\"\"\n        Play the audio from the base64 content (no file required).\n        \"\"\"\n        from pydub import AudioSegment\n        from pydub.playback import play\n        import base64\n        import io\n\n        # Decode base64 to bytes\n        audio_bytes = base64.b64decode(self.audio_content)\n\n        # Create a file-like object from bytes\n        audio_buffer = io.BytesIO(audio_bytes)\n\n        # Load audio from the buffer\n        audio = AudioSegment.from_file(audio_buffer, format=self.format)\n\n        # Play the audio\n        play(audio)  # Serialization methods\n\n    @override\n    def to_cache_dict(self) -&gt; dict:\n        \"\"\"\n        Convert the AudioMessage to a dictionary for caching.\n        \"\"\"\n        return {\n            \"message_type\": self.message_type,\n            \"role\": self.role,\n            \"content\": self.content,\n            \"text_content\": self.text_content,\n            \"audio_content\": self.audio_content,\n            \"format\": self.format,\n        }\n\n    @override\n    @classmethod\n    def from_cache_dict(cls, cache_dict: dict) -&gt; \"AudioMessage\":\n        \"\"\"\n        Create an AudioMessage from a cached dictionary.\n        \"\"\"\n        return cls(\n            role=cache_dict[\"role\"],\n            content=cache_dict[\"content\"],\n            text_content=cache_dict[\"text_content\"],\n            audio_content=cache_dict[\"audio_content\"],\n            format=cache_dict[\"format\"],\n        )\n\n    # API compatibility methods\n    @override\n    def to_openai(self) -&gt; dict:\n        \"\"\"\n        Converts the AudioMessage to the OpenAI format.\n        \"\"\"\n        openaiinputaudio = OpenAIInputAudio(data=self.audio_content, format=self.format)\n        openaiaudiocontent = OpenAIAudioContent(input_audio=openaiinputaudio)\n        text_content = OpenAITextContent(text=self.text_content)\n        openaiaudiomessage = OpenAIAudioMessage(\n            role=self.role, content=[text_content, openaiaudiocontent]\n        )\n        return openaiaudiomessage.model_dump()\n\n    @override\n    def to_anthropic(self):\n        raise NotImplementedError(\"Anthropic API does not support audio messages.\")\n\n    @override\n    def to_google(self) -&gt; dict:\n        \"\"\"\n        Defaults to OpenAI format for Google Gemini.\n        \"\"\"\n        return self.to_openai()\n\n    @override\n    def to_ollama(self) -&gt; dict:\n        \"\"\"\n        Defaults to OpenAI format for Ollama.\n        \"\"\"\n        return self.to_openai()\n\n    @override\n    def to_perplexity(self):\n        raise NotImplementedError(\"Perplexity API does not support audio messages.\")\n</file>\n    <file path=\"message/__init__.py\">\n</file>\n    <file path=\"message/messages.py\">from collections.abc import Iterator\nfrom pydantic import BaseModel, Field, ValidationError\nfrom conduit.message.message import Message\nfrom conduit.message.textmessage import TextMessage\nfrom conduit.message.imagemessage import ImageMessage\nfrom conduit.message.audiomessage import AudioMessage\nfrom conduit.logs.logging_config import get_logger\nfrom typing import override\n\nMessageUnion = TextMessage | ImageMessage | AudioMessage\n\nlogger = get_logger(__name__)\n\n\nclass Messages(BaseModel):\n    \"\"\"\n    A Pydantic BaseModel that contains a list of Message objects.\n    Behaves like a list through dunder methods while being fully Pydantic-compatible.\n    Supports Message, ImageMessage, and AudioMessage objects (all inherit from Message).\n    class MessageStore(Messages):\n    \"\"\"\n\n    messages: list[MessageUnion | None] = Field(\n        default_factory=list,\n        description=\"List of Message objects (including ImageMessage and AudioMessage)\",\n    )\n\n    def __init__(self, messages: list[Message] = [], **kwargs):\n        \"\"\"\n        Initialize with optional list of messages.\n\n        Args:\n            messages: list of Message objects (including ImageMessage and AudioMessage) to initialize with\n        \"\"\"\n        if messages is not None:\n            super().__init__(messages=messages, **kwargs)\n        else:\n            super().__init__(**kwargs)\n        # Validate the turn order of messages\n        self._validate_turn_order()\n\n    # Validation methods\n    def _validate_turn_order(self):\n        \"\"\"\n        Validate that the message history meets the following conditions:\n        (1) only one system message\n        (2) messages alternate between user and assistant roles\n\n        This is called on init and whenever messages are added.\n        \"\"\"\n        logger.debug(f\"Validating message turn order: {self.messages}\")\n\n        def create_dialog_signature(messages: list[Message]) -&gt; str:\n            \"\"\"\n            Create a signature string for the message order.\n            \"\"\"\n            logger.debug(\"Creating dialog signature\")\n            return \"\".join(msg.role[0].lower() for msg in messages)\n\n        dialog_signature = create_dialog_signature(self.messages)\n        # Check for exactly one system message\n        if dialog_signature.count(\"s\") &gt; 1:\n            raise ValueError(\n                \"Only one 'system' message is allowed in the conversation history.\"\n            )\n        # Check for alternating user and assistant messages\n        if \"uu\" in dialog_signature or \"aa\" in dialog_signature:\n            raise ValueError(\n                f\"Messages must alternate between 'user' and 'assistant' roles. Dialog signature: {dialog_signature}\"\n            )\n        pass\n\n    # List-like interface methods\n    def append(self, message: Message) -&gt; None:\n        \"\"\"Add a message to the end of the list.\"\"\"\n        self.messages.append(message)\n        self._validate_turn_order()\n\n    def extend(self, messages: list[Message]) -&gt; None:\n        \"\"\"Extend the list with multiple messages.\"\"\"\n        self.messages.extend(messages)\n\n    def insert(self, index: int, message: Message) -&gt; None:\n        \"\"\"Insert a message at the specified index.\"\"\"\n        self.messages.insert(index, message)\n\n    def remove(self, message: Message) -&gt; None:\n        \"\"\"Remove the first occurrence of a message.\"\"\"\n        self.messages.remove(message)\n\n    def pop(self, index: int = -1) -&gt; Message:\n        \"\"\"Remove and return message at index (default last).\"\"\"\n        return self.messages.pop(index)\n\n    def clear(self) -&gt; None:\n        \"\"\"Remove all messages.\"\"\"\n        self.messages.clear()\n\n    def index(self, message: Message, start: int = 0, stop: int | None = None) -&gt; int:\n        \"\"\"Return the index of the first occurrence of message.\"\"\"\n        if stop is None:\n            return self.messages.index(message, start)\n        return self.messages.index(message, start, stop)\n\n    def count(self, message: Message) -&gt; int:\n        \"\"\"Return the number of occurrences of message.\"\"\"\n        return self.messages.count(message)\n\n    def reverse(self) -&gt; None:\n        \"\"\"Reverse the messages in place.\"\"\"\n        self.messages.reverse()\n\n    # Dunder methods for list-like behavior\n    @override\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation for debugging.\"\"\"\n        return str(self.messages)\n\n    @override\n    def __str__(self) -&gt; str:\n        \"\"\"\n        String representation showing message count and types.\n        \"\"\"\n        return self.__repr__()\n\n    @override\n    def __iter__(self) -&gt; Iterator[Message]:\n        \"\"\"Iterate over messages.\"\"\"\n        return iter(self.messages)\n\n    @override\n    def __eq__(self, other) -&gt; bool:\n        \"\"\"Check equality with another Messages object or list.\"\"\"\n        if isinstance(other, Messages):\n            return self.messages == other.messages\n        elif isinstance(other, list):\n            return self.messages == other\n        return False\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of messages.\"\"\"\n        return len(self.messages)\n\n    def __getitem__(self, key) -&gt; \"Message | Messages\":\n        \"\"\"Get message(s) by index or slice.\"\"\"\n        result = self.messages[key]\n        if isinstance(key, slice):\n            return Messages(result)\n        return result\n\n    def __setitem__(self, key, value):\n        \"\"\"Set message(s) by index or slice.\"\"\"\n        self.messages[key] = value\n\n    def __delitem__(self, key):\n        \"\"\"Delete message(s) by index or slice.\"\"\"\n        del self.messages[key]\n\n    def __reversed__(self) -&gt; Iterator[Message]:\n        \"\"\"Iterate over messages in reverse order.\"\"\"\n        return reversed(self.messages)\n\n    def __contains__(self, message: Message) -&gt; bool:\n        \"\"\"Check if message is in the list.\"\"\"\n        return message in self.messages\n\n    def __bool__(self) -&gt; bool:\n        \"\"\"Return True if there are any messages.\"\"\"\n        return bool(self.messages)\n\n    def __add__(self, other) -&gt; \"Messages\":\n        \"\"\"Concatenate with another Messages object or list.\"\"\"\n        if isinstance(other, Messages):\n            return Messages(self.messages + other.messages)\n        elif isinstance(other, list):\n            return Messages(self.messages + other)\n        return NotImplemented\n\n    def __iadd__(self, other) -&gt; \"Messages\":\n        \"\"\"In-place concatenation with another Messages object or list.\"\"\"\n        if isinstance(other, Messages):\n            self.messages.extend(other.messages)\n        elif isinstance(other, list):\n            self.messages.extend(other)\n        else:\n            return NotImplemented\n        return self\n\n    def __mul__(self, other: int) -&gt; \"Messages\":\n        \"\"\"Repeat messages n times.\"\"\"\n        if isinstance(other, int):\n            return Messages(self.messages * other)\n        return NotImplemented\n\n    def __imul__(self, other: int) -&gt; \"Messages\":\n        \"\"\"In-place repeat messages n times.\"\"\"\n        if isinstance(other, int):\n            self.messages *= other\n        else:\n            return NotImplemented\n        return self\n\n    # Conduit-specific convenience methods\n    def add_new(self, role: str, content: str) -&gt; None:\n        \"\"\"\n        Create and add a new message to the list.\n\n        Args:\n            role: Message role (user, assistant, system)\n            content: Message content\n        \"\"\"\n        if role == \"system\" and self.system_message is not None:\n            raise ValueError(\n                \"Only one 'system' message is allowed in the conversation.\"\n            )\n        else:\n            self.append(TextMessage(role=role, content=content))\n\n    def last(self) -&gt; Message | None:\n        \"\"\"\n        Get the last message in the list.\n\n        Returns:\n            Last Message object or None if empty\n        \"\"\"\n        return self.messages[-1] if self.messages else None\n\n    def get_by_role(self, role: str) -&gt; list[Message]:\n        \"\"\"\n        Get all messages with a specific role.\n\n        Args:\n            role: Role to filter by (user, assistant, system)\n\n        Returns:\n            list of messages with the specified role\n        \"\"\"\n        return [msg for msg in self.messages if msg.role == role]\n\n    def user_messages(self) -&gt; list[Message]:\n        \"\"\"Get all user messages.\"\"\"\n        return self.get_by_role(\"user\")\n\n    def assistant_messages(self) -&gt; list[Message]:\n        \"\"\"Get all assistant messages.\"\"\"\n        return self.get_by_role(\"assistant\")\n\n    def system_messages(self) -&gt; list[Message]:\n        \"\"\"Get all system messages.\"\"\"\n        return self.get_by_role(\"system\")\n\n    @property\n    def system_message(self) -&gt; Message | None:\n        \"\"\"Get the system message, if any.\"\"\"\n        systems = self.get_by_role(\"system\")\n        return systems[0] if systems else None\n\n    # Serialization methods\n    def to_cache_dict(self) -&gt; list:\n        \"\"\"\n        This differs from our usual serialization to_cache_dict method in two ways:\n        - It returns a list of dictionaries, one for each message (not a dict)\n        - It requires a provider argument, which is used to determine how to serialize the messages.\n\n        This is because audio and image messages may have different serialization requirements per provider.\n        \"\"\"\n        return [msg.to_cache_dict() for msg in self.messages]\n\n    @classmethod\n    def from_cache_dict(cls, cache_dict: list) -&gt; \"Messages\":\n        \"\"\"\n        Deserialize from a dictionary. Note: in most cases we have serialized to a list of dictionaries, though this can also handle a single dictionary (i.e. the full Messages object).\n\n        Args:\n            cache_dict: Dictionary containing cached messages\n\n        Returns:\n            Messages object\n        \"\"\"\n        if isinstance(cache_dict, list):\n            # Convert each dict to proper Message object\n            message_dicts = [Message.from_cache_dict(msg) for msg in cache_dict]\n            return cls(messages=message_dicts)\n        else:\n            # If this assertion breaks, we are deserializing a full Messages object, not a list of messages.\n            raise ValueError(\"cache_dict must be a list of message dictionaries\")\n\n    # API compatibility methods\n    def to_openai(self) -&gt; list[dict]:\n        return [msg.to_openai() for msg in self.messages]\n\n    def to_anthropic(self) -&gt; list[dict]:\n        return [msg.to_anthropic() for msg in self.messages]\n\n    def to_google(self) -&gt; list[dict]:\n        return [msg.to_google() for msg in self.messages]\n\n    def to_ollama(self) -&gt; list[dict]:\n        return [msg.to_ollama() for msg in self.messages]\n\n    def to_perplexity(self) -&gt; list[dict]:\n        return [msg.to_perplexity() for msg in self.messages]\n</file>\n    <file path=\"message/textmessage.py\">\"\"\"\nMessage is the default message type recognized as industry standard (role + content).\nOur Message class is inherited from specialized types like AudioMessage, ImageMessage, etc.\n\"\"\"\n\nfrom conduit.prompt.prompt import Prompt\nfrom conduit.logs.logging_config import get_logger\nfrom conduit.message.message import Message, MessageType\nfrom pydantic import BaseModel, Field\n\n\nlogger = get_logger(__name__)\n\n\nclass TextMessage(Message):\n    \"\"\"\n    Industry standard, more or less, for messaging with LLMs.\n    System roles can have some weirdness (like with Anthropic), but role/content is standard.\n    \"\"\"\n\n    message_type: MessageType = Field(default=\"text\", exclude=True, repr=False)\n    content: str | BaseModel | list[BaseModel]\n\n    def __str__(self):\n        \"\"\"\n        Returns the message in a human-readable format.\n        \"\"\"\n        return f\"{self.role}: {self.content}\"\n\n    def __getitem__(self, key):\n        \"\"\"\n        Allows for dictionary-style access to the object.\n        \"\"\"\n        return getattr(self, key)\n\n    # Serialization methods\n    def to_cache_dict(self) -&gt; dict:\n        \"\"\"\n        Serializes the message to a dictionary for caching.\n        \"\"\"\n        role = self.role\n        if isinstance(self.content, str):\n            content = self.content\n        elif isinstance(self.content, BaseModel):\n            content = self._serialize_pydantic(self.content)\n        elif isinstance(self.content, list):\n            content = [self._serialize_pydantic(item) for item in self.content]\n        else:\n            logger.error(f\"Unsupported content type: {type(self.content)}\")\n            raise TypeError(\n                \"Content must be a string, BaseModel, or list of BaseModels.\"\n            )\n        assert isinstance(content, (str, dict, list)), (\n            \"Content must be a string, dict, or list of dicts, \"\n        )\n        return {\n            \"message_type\": self.message_type,\n            \"role\": role,\n            \"content\": content,\n        }\n\n    @classmethod\n    def from_cache_dict(cls, cache_dict: dict):\n        \"\"\"\n        Deserializes the message from a dictionary.\n        \"\"\"\n        role = cache_dict[\"role\"]\n        content = cache_dict[\"content\"]\n        # If it's a BaseModel, we need to deserialize it\n        if isinstance(content, str):\n            content = content\n        elif isinstance(content, dict) and \"__class__\" in content:\n            if \"__class__\" in cache_dict[\"content\"]:\n                content = cls._deserialize_pydantic(cache_dict[\"content\"])\n            else:\n                logger.error(\"Content dictionary does not contain '__class__' key.\")\n                raise ValueError(\n                    \"Content dictionary must contain '__class__' key for deserialization.\"\n                )\n        # If it's a list of BaseModels, we need to deserialize each item\n        elif isinstance(content, list):\n            content = [cls._deserialize_pydantic(item) for item in content]\n        else:\n            logger.error(f\"Unsupported content type: {type(content)}\")\n            raise TypeError(\"Content must be a string, dict, or list of dicts.\")\n        return cls(role=role, content=content)\n\n    def _serialize_pydantic(self, obj: BaseModel) -&gt; dict:\n        \"\"\"\n        Serializes a Pydantic model to a dictionary.\n        \"\"\"\n        obj_dict = obj.model_dump()\n        obj_dict[\"__class__\"] = obj.__class__.__name__\n        return obj_dict\n\n    @classmethod\n    def _deserialize_pydantic(cls, obj_dict: dict) -&gt; BaseModel:\n        # First, handle PerplexityContent specifically, since it's not technically a structured response request.\n        if obj_dict.get(\"__class__\") == \"PerplexityContent\":\n            from conduit.model.clients.perplexity_content import PerplexityContent\n\n            return PerplexityContent.model_validate(obj_dict)\n\n        from conduit.parser.parser import Parser\n\n        class_name = obj_dict.pop(\"__class__\")\n\n        for response_model in Parser._response_models:\n            if response_model.__name__ == class_name:\n                try:\n                    return response_model.model_validate(obj_dict)\n                except Exception as e:\n                    logger.error(f\"Error deserializing {class_name}: {e}\")\n                    raise ValueError(\n                        f\"Error deserializing {class_name}: {e}. There maybe version conflicts with the response model.\"\n                    )\n        logger.error(f\"Unknown class name: {class_name}\")\n        raise ValueError(\n            f\"Unknown class name: {class_name}. Please check the response models in Parser._response_model.\"\n        )\n\n    # API compatibility methods -- these are inherited from Message so no need to override here.\n\n\n# Some helpful functions\ndef create_system_message(\n    system_prompt: str | Prompt, input_variables=None\n) -&gt; list[Message]:\n    \"\"\"\n    Takes a system prompt object (Prompt()) or a string, an optional input object, and returns a Message object.\n    \"\"\"\n    if isinstance(system_prompt, str):\n        system_prompt = Prompt(system_prompt)\n    if input_variables:\n        system_message = [\n            TextMessage(\n                role=\"system\",\n                content=system_prompt.render(input_variables=input_variables),\n            )\n        ]\n    else:\n        system_message = [\n            TextMessage(role=\"system\", content=system_prompt.prompt_string)\n        ]\n    return system_message\n</file>\n    <file path=\"message/videomessage.py\"/>\n    <file path=\"chat/chat.py\">\"\"\"\nExtensible chat class for CLI chat applications.\nAdd more commands by extending the commands list + defining a \"command_\" method.\n\nTODO:\n- more commands\n    - [x] dynamic registration of new command methods\n    - allow getting a message from history allow branching from somewhere in history allow pruning of history\n    - allow saving history, saving a message\n    - allow capturing screenshots for vision-based queries\n- Leviathan extension\n    - create a Chat app in Leviathan that incorporates ask, tutorialize, cookbook, etc.\n- Mentor extension\n    - system message\n    - RAG: cosmo data from postgres, similarity search from chroma, direct access of courses from mongod\n    - Curation objects: create, view, edit\n    - blacklist course (for session)\n- Learning from the Mentor implementation for more Agentic use cases\n    - prompts\n    - resources\n    - tools\n\"\"\"\n\nfrom conduit.conduit.sync_conduit import SyncConduit\nfrom conduit.model.model import Model\nfrom conduit.message.messagestore import MessageStore\nfrom conduit.message.textmessage import TextMessage\nfrom conduit.message.message import Message\nfrom conduit.message.messages import Messages\nfrom conduit.logs.logging_config import get_logger\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nfrom instructor.exceptions import InstructorRetryException\nfrom functools import partial\nfrom typing import Callable, Optional\nfrom pathlib import Path\nimport sys, inspect, readline  # Enables completion in the console\n\n\nclass Chat:\n    \"\"\"\n    Basic CLI chat implementation.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model = Model(\"claude-3-5-haiku-20241022\"),\n        messagestore: Optional[MessageStore] = SyncConduit.message_store,\n        console: Optional[Console] = SyncConduit.console,\n    ):\n        \"\"\"\n        User can inject their own messagestore, console, and model, otherwise defaults are used.\n        \"\"\"\n        self.model = model\n        if not console:\n            self.console = Console(width=120)\n        else:\n            self.console = console\n        self.messagestore = messagestore\n        self.welcome_message = \"[green]Hello! Type /exit to exit.[/green]\"\n        self.system_message: TextMessage | None = None\n        self.commands = self.get_commands()\n        self.log_file: str | Path = \"\"  # Off by default, but can be initialized.\n\n    def parse_input(self, input: str) -&gt; Callable | partial | None:\n        \"\"\"\n        Commands start with a slash. This method parses the input and returns the corresponding method.\n        If command takes a param, this returns a partial function.\n        If command is not found, it returns None (and the chat loop will handle it).\n        \"\"\"\n        commands = self.get_commands()\n\n        # Not a command; return None.\n        if not input.startswith(\"/\"):\n            return None\n\n        # Parse for the type of command; this also involves catching parameters.\n        for command in commands:\n            command_string = command.replace(\"command_\", \"\").replace(\"_\", \" \")\n            if input.startswith(\"/\" + command_string):\n                # Check if the command has parameters\n                sig = inspect.signature(getattr(self, command))\n                if sig.parameters:\n                    parametrized = True\n                else:\n                    parametrized = False\n                # Check if input has parameters\n                param = input[len(command_string) + 2 :]\n                # Conditional return\n                if param and parametrized:\n                    return partial(getattr(self, command), param)\n                elif param and not parametrized:\n                    raise ValueError(\"Command does not take parameters.\")\n                elif not param and parametrized:\n                    raise ValueError(\"Command requires parameters.\")\n                else:\n                    return getattr(self, command)\n        # Command not found\n        raise ValueError(\"Command not found.\")\n\n    def get_commands(self) -&gt; list[str]:\n        \"\"\"\n        Dynamic inventory of \"command_\" methods.\n        If you extend this with more methods, make sure they follow the \"command_\" naming convention.\n        \"\"\"\n        commands = [attr for attr in dir(self) if attr.startswith(\"command_\")]\n        return commands\n\n    # Command methods\n    def command_exit(self):\n        \"\"\"\n        Exit the chat.\n        \"\"\"\n        self.console.print(\"Goodbye!\", style=\"green\")\n        exit()\n\n    def command_help(self):\n        \"\"\"\n        Display the help message.\n        \"\"\"\n        commands = sorted(self.get_commands())\n        help_message = \"Commands:\\n\"\n        for command in commands:\n            command_name = command.replace(\"command_\", \"\").replace(\"_\", \" \")\n            command_func = getattr(self, command)\n            try:\n                command_docs = command_func.__doc__.strip()\n            except AttributeError:\n                print(f\"Command {command_name} is missing a docstring.\")\n                sys.exit()\n            help_message += (\n                f\"/[purple]{command_name}[/purple]: [green]{command_docs}[/green]\\n\"\n            )\n        self.console.print(help_message)\n\n    def command_clear_history(self):\n        \"\"\"\n        Clear the message history.\n        \"\"\"\n        if self.messagestore:\n            self.messagestore.clear()\n            self.console.print(\"Message history cleared.\", style=\"green\")\n        else:\n            self.console.print(\"No message store available.\", style=\"red\")\n\n    def command_clear(self):\n        \"\"\"\n        Clear the screen.\n        \"\"\"\n        self.console.clear()\n\n    def command_show_history(self):\n        \"\"\"\n        Display the chat history.\n        \"\"\"\n        if self.messagestore:\n            self.messagestore.view_history()\n\n    def command_show_models(self):\n        \"\"\"\n        Display available models.\n        \"\"\"\n        self.console.print(Model.models, style=\"green\")\n\n    def command_show_model(self):\n        \"\"\"\n        Display the current model.\n        \"\"\"\n        self.console.print(f\"Current model: {self.model.model}\", style=\"green\")\n\n    def command_set_model(self, param: str):\n        \"\"\"\n        Set the current model.\n        \"\"\"\n        try:\n            self.model = Model(param)\n            self.console.print(f\"Set model to {param}\", style=\"green\")\n        except ValueError:\n            self.console.print(f\"Invalid model: {param}\", style=\"red\")\n\n    def command_paste_image(self):\n        \"\"\"\n        Use this when you have an image in clipboard that you want to submit as context for LLM.\n        This gets saved as self.clipboard_image.\n        \"\"\"\n        from Conduit.message.imagemessage import ImageMessage\n        import os\n\n        if \"SSH_CLIENT\" in os.environ or \"SSH_TTY\" in os.environ:\n            self.console.print(\"Image paste not available over SSH.\", style=\"red\")\n            return\n\n        import warnings\n        from PIL import ImageGrab\n        import base64, io\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")  # Suppress PIL warnings\n            image = ImageGrab.grabclipboard()\n\n        if image:\n            buffer = io.BytesIO()\n            image.save(buffer, format=\"PNG\")\n            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n            # Save for next query\n            self.console.print(\"Image captured! What is your query?\", style=\"green\")\n            user_input = self.console.input(\n                \"[bold green]Query about image[/bold green]: \"\n            )\n            # Build our ImageMessage\n            text_content = user_input\n            image_content = img_base64\n            mime_type = \"image/png\"\n            role = \"user\"\n            imagemessage = ImageMessage(\n                role=role,\n                text_content=text_content,\n                image_content=image_content,\n                mime_type=mime_type,\n            )\n            self.messagestore.append(imagemessage)\n            response = self.query_model([self.messagestore.last()])\n            self.console.print(response)\n\n        else:\n            self.console.print(\"No image detected.\", style=\"red\")\n\n    def command_wipe_image(self):\n        \"\"\"\n        Delete image from memory.\n        \"\"\"\n        if self.clipboard_image:\n            self.clipboard_image = None\n            self.console.print(\"Image deleted.\", style=\"green\")\n        else:\n            self.console.print(\"No image to delete.\", style=\"red\")\n\n    # Main query method\n    def query_model(self, query_input: list[Message] | Messages) -&gt; str | None:\n        \"\"\"\n        Takes either a string or a list of Message objects.\n        \"\"\"\n        response = str(self.model.query(query_input, verbose=\"v\"))\n        if self.messagestore:\n            self.messagestore.add_new(role=\"assistant\", content=str(response))\n        return response\n\n    # Main chat loop\n    def chat(self):\n        self.console.clear()\n        self.console.print(self.welcome_message)\n        # If user passed a messagestore already, use that, otherwise declare a new one.\n        if self.system_message:\n            self.messagestore.append(self.system_message)\n        try:\n            while True:\n                try:\n                    user_input = self.console.input(\"[bold gold3]&gt;&gt; [/bold gold3]\")\n                    # Capture empty input\n                    if not user_input:\n                        continue\n                    # Process commands\n                    if user_input.startswith(\"/\"):\n                        command = self.parse_input(user_input)\n                        if callable(command):\n                            try:\n                                command()\n                                continue\n                            except KeyboardInterrupt:\n                                # User can cancel commands with Ctrl+C\n                                self.console.print(\"\\nCommand canceled.\", style=\"green\")\n                                continue\n                        else:\n                            self.console.print(\"Invalid command.\", style=\"red\")\n                            continue\n                    else:\n                        # Process query\n                        try:\n                            if self.messagestore:\n                                self.messagestore.add_new(\n                                    role=\"user\", content=user_input\n                                )\n                                response = self.query_model(self.messagestore.messages)\n                            else:\n                                response = self.query_model(\n                                    [TextMessage(role=\"user\", content=user_input)]\n                                )\n                            self.console.print(\n                                Markdown(str(response) + \"\\n\"), style=\"blue\"\n                            )\n                            continue\n                        except KeyboardInterrupt:\n                            # User can cancel query with Ctrl+C\n                            self.console.print(\"\\nQuery canceled.\", style=\"green\")\n                            continue\n                        except InstructorRetryException:\n                            # This exception is raised if there is some network failure from instructor.\n                            self.console.print(\n                                \"Network error. Please try again.\", style=\"red\"\n                            )\n                except ValueError as e:\n                    # If command not found, or commands throw an error, catch it and continue.\n                    self.console.print(str(e), style=\"red\")\n                    continue\n        except KeyboardInterrupt:\n            # User can exit the chat with Ctrl+C\n            self.console.print(\"\\nGoodbye!\", style=\"green\")\n</file>\n    <file path=\"chat/__init__.py\">\n</file>\n    <file path=\"chat/audio_chat.py\"/>\n    <file path=\"llm_decorator/decorator.py\">from conduit.model.model import Model\nfrom conduit.prompt.prompt import Prompt\nfrom conduit.conduit.sync_conduit import SyncConduit\nfrom conduit.logs.logging_config import get_logger\nfrom collections.abc import Callable\nimport inspect, re\n\nlogger = get_logger(__name__)\n\n# Constants\nparam_string_pattern = re.compile(\"{{(.+)}}\")\n\n\ndef _validate_template_params(func: Callable) -&gt; None:\n    \"\"\"\n    Validate that function parameters match template variables in docstring.\n\n    Raises:\n        ValueError: If parameters don't match template variables\n    \"\"\"\n    # Get function signature parameters\n    sig = inspect.signature(func)\n    func_params = set()\n\n    for param_name, param in sig.parameters.items():\n        # Skip *args and **kwargs\n        if param.kind in (param.VAR_POSITIONAL, param.VAR_KEYWORD):\n            continue\n        func_params.add(param_name)\n\n    # Extract template variables from docstring\n    docstring = func.__doc__\n    if not docstring:\n        raise ValueError(\n            f\"Function '{func.__name__}' must have a docstring containing the prompt template\"\n        )\n\n    # Find all {{variable}} patterns\n    template_vars = set(re.findall(r\"\\{\\{(\\w+)\\}\\}\", docstring))\n\n    # Compare the sets\n    missing_in_template = func_params - template_vars\n    missing_in_signature = template_vars - func_params\n\n    errors = []\n\n    if missing_in_template:\n        errors.append(\n            f\"Parameters in function signature but not in template: {missing_in_template}\"\n        )\n\n    if missing_in_signature:\n        errors.append(\n            f\"Variables in template but not in function signature: {missing_in_signature}\"\n        )\n\n    if errors:\n        raise ValueError(\n            f\"Template validation failed for function '{func.__name__}':\\n\"\n            + \"\\n\".join(f\"  - {error}\" for error in errors)\n        )\n\n\ndef _wrap_params(prompt: str) -&gt; str:\n    \"\"\"\n    Generally speaking, wrapping context in xml tags is very helpful for LLMs.\n    All prompts written as docstrings for an llm function get wrapped in XML.\n\n    Example:\n\n    {{param_name}} becomes:\n\n    &lt;param_name&gt;\n    {{param_name}}\n    &lt;/param_name&gt;\n    \"\"\"\n    return (\n        re.sub(\n            param_string_pattern,\n            lambda m: f\"&lt;{m.group(1)}&gt;\\n{{{{{m.group(1)}}}}}\\n&lt;/{m.group(1)}&gt; \",\n            prompt,\n        )\n        + \"\\n\"\n    )\n\n\ndef llm(func: Callable = None, *, model=\"haiku\") -&gt; Callable:  # Note the *\n    \"\"\"\n    Decorator to create a prompt function that can be used with an LLM.\n    Adapts basic Conduit syntax to the decorator pattern for easy composition.\n\n    How to compose:\n\n    @llm\n    def my_prompt_function(input: str):\n        \\\"\"\"\n        Your prompt template with {{input}}.\n        \\\"\"\"\n    \"\"\"\n\n    def decorator(f: Callable) -&gt; Callable:\n        _validate_template_params(f)\n\n        def wrapper(*args, **kwargs):  # Accept both positional and keyword\n            # Get function signature to bind args properly\n\n            sig = inspect.signature(f)\n            bound_args = sig.bind(*args, **kwargs)\n            bound_args.apply_defaults()\n\n            model_object = Model(model)\n            # Wrap any parameters in XML tags\n            prompt_string = _wrap_params(f.__doc__)\n            # Docstrings are tabbed / spaced, so we need to remove spaces at beginning of lines\n            prompt_string = \"\\n\".join(\n                line.lstrip() for line in prompt_string.splitlines()\n            )\n            prompt = Prompt(prompt_string)\n            conduit = SyncConduit(prompt=prompt, model=model_object)\n            response = conduit.run(input_variables=dict(bound_args.arguments))\n            return response.content  # Note: .content to get just the text\n\n        return wrapper\n\n    if func is None:\n        return decorator\n    else:\n        return decorator(func)\n</file>\n    <file path=\"llm_decorator/__init__.py\">\n</file>\n    <file path=\"compose/conduitml.py\"/>\n    <file path=\"compose/__init__.py\"/>\n    <file path=\"compose/conduit_compose.py\">#!/usr/bin/env python3\n\"\"\"\nConduitML to Mermaid DAG Converter\n\nConverts ConduitML workflow files to Mermaid flowchart diagrams showing the DAG structure.\n\nUsage:\n    python conduitml_to_mermaid.py workflow.conduitml\n    python conduitml_to_mermaid.py workflow.conduitml --output diagram.mmd\n    python conduitml_to_mermaid.py workflow.conduitml --style detailed\n\"\"\"\n\nimport yaml\nimport argparse\nimport sys\nfrom pathlib import Path\n\n\nclass ConduitMLToMermaid:\n    \"\"\"Converts ConduitML workflows to Mermaid flowchart diagrams\"\"\"\n\n    def __init__(self, style: str = \"simple\"):\n        self.style = style  # \"simple\", \"detailed\", or \"full\"\n        self.nodes: dict[str, dict] = {}\n        self.edges: list[tuple] = []\n        self.inputs: dict[str, dict] = {}\n        self.outputs: dict[str, dict] = {}\n        self.workflow_name = \"\"\n\n    def parse_conduitml(self, conduitml_content: str):\n        \"\"\"Parse ConduitML content and extract DAG structure\"\"\"\n        try:\n            data = yaml.safe_load(conduitml_content)\n            workflow = data.get(\"workflow\", {})\n\n            # Extract metadata\n            self.workflow_name = workflow.get(\"name\", \"Unnamed Workflow\")\n\n            # Extract inputs\n            self.inputs = workflow.get(\"inputs\", {})\n\n            # Extract outputs\n            self.outputs = workflow.get(\"outputs\", {})\n\n            # Extract steps and build DAG\n            steps = workflow.get(\"steps\", {})\n            self._parse_steps(steps)\n\n        except yaml.YAMLError as e:\n            raise ValueError(f\"Invalid YAML in ConduitML file: {e}\")\n        except Exception as e:\n            raise ValueError(f\"Error parsing ConduitML: {e}\")\n\n    def _parse_steps(self, steps: dict[str, dict]):\n        \"\"\"Parse workflow steps and extract dependencies\"\"\"\n        for step_name, step_config in steps.items():\n            # Clean step name (remove 'step ' prefix if present)\n            clean_name = step_name.replace(\"step \", \"\").strip()\n\n            # Store step information\n            self.nodes[clean_name] = {\n                \"name\": clean_name,\n                \"model\": step_config.get(\"model\", \"unknown\"),\n                \"description\": step_config.get(\"description\", \"\"),\n                \"has_condition\": \"condition\" in step_config,\n                \"has_parser\": \"parser\" in step_config,\n                \"parser_type\": (\n                    step_config.get(\"parser\", {}).get(\"type\", \"string\")\n                    if \"parser\" in step_config\n                    else \"string\"\n                ),\n            }\n\n            # Extract dependencies and create edges\n            depends_on = step_config.get(\"depends_on\", [])\n            for dependency in depends_on:\n                self.edges.append((dependency, clean_name))\n\n    def _sanitize_node_id(self, name: str) -&gt; str:\n        \"\"\"Convert node name to valid Mermaid ID\"\"\"\n        # Replace spaces and special characters with underscores\n        return name.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\")\n\n    def _get_node_style(self, node_info: dict) -&gt; str:\n        \"\"\"Determine node styling based on step properties\"\"\"\n        if self.style == \"simple\":\n            return \"\"\n\n        styles = []\n\n        # Color based on model type\n        model = node_info[\"model\"].lower()\n        if \"gpt\" in model or \"openai\" in model:\n            styles.append(\"fill:#e1f5fe\")  # Light blue for OpenAI\n        elif \"claude\" in model or \"anthropic\" in model:\n            styles.append(\"fill:#f3e5f5\")  # Light purple for Anthropic\n        elif \"gemini\" in model or \"google\" in model:\n            styles.append(\"fill:#e8f5e8\")  # Light green for Google\n        else:\n            styles.append(\"fill:#fff3e0\")  # Light orange for others\n\n        # Border style for special properties\n        if node_info[\"has_condition\"]:\n            styles.append(\n                \"stroke:#ff9800,stroke-width:3px,stroke-dasharray: 5 5\"\n            )  # Dashed for conditional\n        elif node_info[\"has_parser\"]:\n            styles.append(\n                \"stroke:#4caf50,stroke-width:2px\"\n            )  # Green for structured output\n\n        return (\n            f\"style {self._sanitize_node_id(node_info['name'])} {','.join(styles)}\"\n            if styles\n            else \"\"\n        )\n\n    def _format_node_label(self, node_info: dict) -&gt; str:\n        \"\"\"Format node label based on style setting\"\"\"\n        name = node_info[\"name\"]\n\n        if self.style == \"simple\":\n            return name\n        elif self.style == \"detailed\":\n            model = node_info[\"model\"]\n            # Shorten common model names\n            model_short = model.replace(\"gpt-4o-mini\", \"GPT-4o-mini\").replace(\n                \"claude-3-5-haiku\", \"Claude-Haiku\"\n            )\n            return f\"{name}&lt;br/&gt;&lt;small&gt;{model_short}&lt;/small&gt;\"\n        else:  # full\n            model = node_info[\"model\"]\n            description = (\n                node_info[\"description\"][:50] + \"...\"\n                if len(node_info[\"description\"]) &gt; 50\n                else node_info[\"description\"]\n            )\n            parser_info = (\n                f\"&lt;br/&gt;&lt;small&gt;Parser: {node_info['parser_type']}&lt;/small&gt;\"\n                if node_info[\"has_parser\"]\n                else \"\"\n            )\n            condition_info = (\n                \"&lt;br/&gt;&lt;small&gt;\u26a0\ufe0f Conditional&lt;/small&gt;\"\n                if node_info[\"has_condition\"]\n                else \"\"\n            )\n            return f\"{name}&lt;br/&gt;&lt;small&gt;{model}&lt;/small&gt;&lt;br/&gt;&lt;small&gt;{description}&lt;/small&gt;{parser_info}{condition_info}\"\n\n    def _add_input_output_nodes(self) -&gt; str:\n        \"\"\"Add input and output nodes to the diagram\"\"\"\n        lines = []\n\n        # Add input nodes\n        if self.inputs:\n            for input_name in self.inputs.keys():\n                input_id = f\"input_{self._sanitize_node_id(input_name)}\"\n                lines.append(f\"    {input_id}[\ud83d\udce5 {input_name}]\")\n                if self.style != \"simple\":\n                    lines.append(f\"    style {input_id} fill:#e3f2fd,stroke:#1976d2\")\n\n        # Add output nodes\n        if self.outputs:\n            for output_name in self.outputs.keys():\n                output_id = f\"output_{self._sanitize_node_id(output_name)}\"\n                lines.append(f\"    {output_id}[\ud83d\udce4 {output_name}]\")\n                if self.style != \"simple\":\n                    lines.append(f\"    style {output_id} fill:#f1f8e9,stroke:#689f38\")\n\n        return \"\\n\".join(lines)\n\n    def _add_input_output_edges(self) -&gt; str:\n        \"\"\"Add edges connecting inputs/outputs to workflow steps\"\"\"\n        lines = []\n\n        # Connect inputs to steps that use them (simplified - connects to steps with no dependencies)\n        if self.inputs:\n            root_steps = [\n                name\n                for name, info in self.nodes.items()\n                if not any(edge[1] == name for edge in self.edges)\n            ]\n\n            for input_name in self.inputs.keys():\n                input_id = f\"input_{self._sanitize_node_id(input_name)}\"\n                for step_name in root_steps:\n                    step_id = self._sanitize_node_id(step_name)\n                    lines.append(f\"    {input_id} --&gt; {step_id}\")\n\n        # Connect final steps to outputs (simplified - connects from steps with no dependents)\n        if self.outputs:\n            final_steps = [\n                name\n                for name, info in self.nodes.items()\n                if not any(edge[0] == name for edge in self.edges)\n            ]\n\n            for output_name in self.outputs.keys():\n                output_id = f\"output_{self._sanitize_node_id(output_name)}\"\n                for step_name in final_steps:\n                    step_id = self._sanitize_node_id(step_name)\n                    lines.append(f\"    {step_id} --&gt; {output_id}\")\n\n        return \"\\n\".join(lines)\n\n    def generate_mermaid(self) -&gt; str:\n        \"\"\"Generate Mermaid flowchart from parsed ConduitML\"\"\"\n        lines = []\n\n        # Header\n        lines.append(\"---\")\n        lines.append(f\"title: {self.workflow_name}\")\n        lines.append(\"---\")\n        lines.append(\"flowchart TD\")\n        lines.append(\"\")\n\n        # Add input/output nodes if they exist\n        io_nodes = self._add_input_output_nodes()\n        if io_nodes:\n            lines.append(\"    %% Input/Output Nodes\")\n            lines.append(io_nodes)\n            lines.append(\"\")\n\n        # Add step nodes\n        lines.append(\"    %% Workflow Steps\")\n        for node_name, node_info in self.nodes.items():\n            node_id = self._sanitize_node_id(node_name)\n            label = self._format_node_label(node_info)\n\n            # Choose node shape based on properties\n            if node_info[\"has_condition\"]:\n                lines.append(f\"    {node_id}{{{label}}}\")  # Diamond for conditional\n            elif node_info[\"has_parser\"]:\n                lines.append(f\"    {node_id}[{label}]\")  # Rectangle for structured\n            else:\n                lines.append(f\"    {node_id}({label})\")  # Rounded for simple\n\n        lines.append(\"\")\n\n        # Add step dependencies\n        if self.edges:\n            lines.append(\"    %% Dependencies\")\n            for source, target in self.edges:\n                source_id = self._sanitize_node_id(source)\n                target_id = self._sanitize_node_id(target)\n                lines.append(f\"    {source_id} --&gt; {target_id}\")\n            lines.append(\"\")\n\n        # Add input/output connections\n        io_edges = self._add_input_output_edges()\n        if io_edges:\n            lines.append(\"    %% Input/Output Connections\")\n            lines.append(io_edges)\n            lines.append(\"\")\n\n        # Add styling\n        if self.style != \"simple\":\n            lines.append(\"    %% Styling\")\n            for node_name, node_info in self.nodes.items():\n                style = self._get_node_style(node_info)\n                if style:\n                    lines.append(f\"    {style}\")\n\n        return \"\\n\".join(lines)\n\n    def validate_dag(self) -&gt; list[str]:\n        \"\"\"Validate that the workflow forms a valid DAG\"\"\"\n        errors = []\n\n        # Check for self-references\n        for source, target in self.edges:\n            if source == target:\n                errors.append(f\"Self-reference detected: {source} depends on itself\")\n\n        # Check for cycles using DFS\n        visited = set()\n        rec_stack = set()\n\n        def has_cycle(node: str) -&gt; bool:\n            visited.add(node)\n            rec_stack.add(node)\n\n            # Get all nodes this one points to\n            dependents = [target for source, target in self.edges if source == node]\n\n            for dependent in dependents:\n                if dependent not in visited:\n                    if has_cycle(dependent):\n                        return True\n                elif dependent in rec_stack:\n                    errors.append(f\"Cycle detected involving: {node} -&gt; {dependent}\")\n                    return True\n\n            rec_stack.remove(node)\n            return False\n\n        # Check all nodes for cycles\n        for node in self.nodes.keys():\n            if node not in visited:\n                has_cycle(node)\n\n        # Check for orphaned dependencies\n        all_referenced = set()\n        for source, target in self.edges:\n            all_referenced.add(source)\n            all_referenced.add(target)\n\n        for referenced in all_referenced:\n            if referenced not in self.nodes:\n                errors.append(f\"Step '{referenced}' is referenced but not defined\")\n\n        return errors\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Convert ConduitML workflow files to Mermaid DAG diagrams\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  python conduitml_to_mermaid.py workflow.conduitnml\n  python conduitml_to_mermaid.py workflow.conduitml --output diagram.mmd\n  python conduitml.py workflow.conduitml --style detailed\n  python conduitml_to_mermaid.py workflow.conduitml --style full --validate\n        \"\"\",\n    )\n\n    parser.add_argument(\"condutml_file\", help=\"Path to ConduitML workflow file\")\n\n    parser.add_argument(\n        \"--output\", \"-o\", help=\"Output file for Mermaid diagram (default: stdout)\"\n    )\n\n    parser.add_argument(\n        \"--style\",\n        \"-s\",\n        choices=[\"simple\", \"detailed\", \"full\"],\n        default=\"detailed\",\n        help=\"Diagram style: simple (names only), detailed (+ models), full (+ descriptions)\",\n    )\n\n    parser.add_argument(\n        \"--validate\",\n        \"-v\",\n        action=\"store_true\",\n        help=\"Validate DAG structure and report errors\",\n    )\n\n    args = parser.parse_args()\n\n    # Read ConduitML file\n    try:\n        conduitml_path = Path(args.conduitml_file)\n        if not conduitml_path.exists():\n            print(f\"Error: File '{args.conduitml_file}' not found\", file=sys.stderr)\n            sys.exit(1)\n\n        with open(conduitml_path, \"r\", encoding=\"utf-8\") as f:\n            conduitml_content = f.read()\n\n    except Exception as e:\n        print(f\"Error reading file: {e}\", file=sys.stderr)\n        sys.exit(1)\n\n    # Convert to Mermaid\n    try:\n        converter = ConduitMLToMermaid(style=args.style)\n        converter.parse_conduitml(conduitml_content)\n\n        # Validate if requested\n        if args.validate:\n            errors = converter.validate_dag()\n            if errors:\n                print(\"DAG Validation Errors:\", file=sys.stderr)\n                for error in errors:\n                    print(f\"  - {error}\", file=sys.stderr)\n                sys.exit(1)\n            else:\n                print(\"\u2705 DAG validation passed\", file=sys.stderr)\n\n        # Generate Mermaid diagram\n        mermaid_output = converter.generate_mermaid()\n\n        # Output to file or stdout\n        if args.output:\n            output_path = Path(args.output)\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(mermaid_output)\n            print(f\"Mermaid diagram written to: {output_path}\", file=sys.stderr)\n        else:\n            print(mermaid_output)\n\n    except Exception as e:\n        print(f\"Error converting ConduitML: {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n</file>\n    <file path=\"model/model.py\">from conduit.message.message import Message\nfrom conduit.progress.wrappers import progress_display\nfrom conduit.progress.verbosity import Verbosity\nfrom conduit.request.request import Request\nfrom conduit.result.result import ConduitResult\nfrom conduit.result.error import ConduitError\nfrom conduit.logs.logging_config import get_logger\nfrom conduit.request.outputtype import OutputType\nfrom conduit.odometer.OdometerRegistry import OdometerRegistry\nfrom pydantic import ValidationError, BaseModel\nfrom typing import Optional, TYPE_CHECKING\nfrom pathlib import Path\nfrom time import time\nimport importlib\n\n# Load only if type checking\nif TYPE_CHECKING:\n    from openai import Stream\n    from anthropic import Stream as AnthropicStream\n    from rich.console import Console\n    from pydantic import BaseModel\n    from conduit.cache.cache import ConduitCache\n    from conduit.model.model_client import ModelClient\n\n\n# Constants\ndir_path = Path(__file__).resolve().parent\nlogger = get_logger(__name__)\n\n\nclass Model:\n    # Class singletons\n    _clients = {}  # Store lazy-loaded client instances at the class level\n    conduit_cache: Optional[\"ConduitCache\"] = (\n        None  # If you want to add a cache, add it at class level as a singleton.\n    )\n    _console: Optional[\"Console\"] = (\n        None  # For rich console output, if needed. This is overridden in the Conduit class.\n    )\n    _odometer_registry = OdometerRegistry()\n\n    @classmethod\n    def from_server(cls, model: str) -&gt; \"ModelClient\":\n        \"\"\"\n        Factory method to create a ClientModel instance for interacting with a remote server.\n\n        Usage:\n        model = Model.from_server(\"gpt-oss:latest\")\n        \"\"\"\n        from conduit.model.model_client import ModelClient\n\n        return ModelClient(model)\n\n    @classmethod\n    def models(cls) -&gt; dict:\n        \"\"\"\n        Returns a dictionary of available models.\n        This is useful for introspection and debugging.\n        \"\"\"\n        from conduit.model.models.modelstore import ModelStore\n\n        return ModelStore.models()\n\n    @classmethod\n    def validate_model(cls, model: str) -&gt; bool:\n        \"\"\"\n        Validates the model name against the available models.\n        Raises ValueError if the model is not found.\n        \"\"\"\n        from conduit.model.models.modelstore import ModelStore\n\n        model_name = ModelStore._validate_model(model)\n        if model_name:\n            return True\n        else:\n            return False\n\n    # Object methods\n    def __init__(self, model: str = \"gpt-4o\", console: Optional[\"Console\"] = None):\n        from conduit.model.models.modelstore import ModelStore\n\n        self.model = ModelStore._validate_model(model)\n        self._client_type = self._get_client_type(self.model)\n        self._client = self.__class__._get_client(self._client_type)\n        self._console = console\n\n    @classmethod\n    def stats(cls):\n        \"\"\"\n        Pretty prints session statistics (from OdometerRegistry.session_odometer).\n        \"\"\"\n        cls._odometer_registry.session_odometer.stats()\n\n    @property\n    def console(self):\n        \"\"\"\n        Returns the effective console (hierarchy: instance -&gt; Model class -&gt; SyncConduit/AsyncConduit class -&gt; None)\n        \"\"\"\n        if self._console:\n            return self._console\n\n        import sys\n\n        # Check for Model._console\n        if \"conduit.model.model\" in sys.modules:\n            Model = sys.modules[\"conduit.model.model\"].Model\n            model_console = getattr(Model, \"_console\", None)\n            if model_console:\n                return model_console\n\n        # Check for SyncConduit._console\n        if \"conduit.conduit.sync_conduit\" in sys.modules:\n            SyncConduit = sys.modules[\"conduit.conduit.sync_conduit\"].SyncConduit\n            conduit_console = getattr(SyncConduit, \"_console\", None)\n            if conduit_console:\n                return conduit_console\n\n        # Check for AsyncConduit._console\n        if \"conduit.conduit.async_conduit\" in sys.modules:\n            AsyncConduit = sys.modules[\"conduit.conduit.async_conduit\"].AsyncConduit\n            async_console = getattr(AsyncConduit, \"_console\", None)\n            if async_console:\n                return async_console\n\n        return None\n\n    @console.setter\n    def console(self, console: \"Console\"):\n        \"\"\"\n        Sets the console object for rich output.\n        This is useful if you want to override the default console for a specific instance.\n        \"\"\"\n        self._console = console\n\n    def _get_client_type(self, model: str) -&gt; tuple:\n        \"\"\"\n        Setting client_type for Model object is necessary for loading the correct client in the query functions.\n        Returns a tuple with client type (which informs the module title) and the client class name (which is used to instantiate the client).\n        \"\"\"\n        from conduit.model.models.modelstore import ModelStore\n\n        model_list = ModelStore.models()\n        if model in model_list[\"openai\"]:\n            return \"openai\", \"OpenAIClientSync\"\n        elif model in model_list[\"anthropic\"]:\n            return \"anthropic\", \"AnthropicClientSync\"\n        elif model in model_list[\"google\"]:\n            return \"google\", \"GoogleClientSync\"\n        elif model in model_list[\"ollama\"]:\n            return \"ollama\", \"OllamaClientSync\"\n        elif model in model_list[\"groq\"]:\n            return \"groq\", \"GroqClientSync\"\n        elif model in model_list[\"perplexity\"]:\n            return \"perplexity\", \"PerplexityClientSync\"\n        else:\n            raise ValueError(f\"Model {model} not found in models\")\n\n    @classmethod\n    def _get_client(cls, client_type: tuple):\n        # print(f\"client type: {client_type}\")\n        if client_type[0] not in cls._clients:\n            try:\n                module = importlib.import_module(\n                    f\"conduit.model.clients.{client_type[0].lower()}_client\"\n                )\n                client_class = getattr(module, f\"{client_type[1]}\")\n                cls._clients[client_type[0]] = client_class()\n            except ImportError as e:\n                raise ImportError(f\"Failed to import {client_type} client: {str(e)}\")\n        client_object = cls._clients[client_type[0]]\n        if not client_object:\n            raise ValueError(f\"Client {client_type} not found in clients\")\n        return client_object\n\n    @progress_display\n    def query(\n        self,\n        # Standard parameters\n        query_input: str | list | Message | None = None,\n        response_model: type[\"BaseModel\"] | None = None,\n        cache=True,\n        temperature: Optional[float] = None,\n        stream: bool = False,\n        output_type: OutputType = \"text\",\n        max_tokens: Optional[int] = None,\n        # For progress reporting decorator\n        verbose: Verbosity = Verbosity.PROGRESS,\n        index: int = 0,\n        total: int = 0,\n        # If we're hand-constructing Request params, we can pass them in directly\n        request: Optional[Request] = None,\n        # Options for debugging\n        return_request: bool = False,\n        return_error: bool = False,\n    ) -&gt; \"ConduitResult | Request | Stream | AnthropicStream\":\n        try:\n            # Construct Request object if not provided (majority of cases)\n            if not request:\n                logger.info(\n                    \"Constructing Request object from query_input and other parameters.\"\n                )\n                import inspect\n\n                frame = inspect.currentframe()\n                args, _, _, values = inspect.getargvalues(frame)\n\n                query_args = {k: values[k] for k in args if k != \"self\"}\n                query_args[\"model\"] = self.model\n                cache = query_args.pop(\"cache\", False)\n                if query_input:\n                    query_args.pop(\"query_input\", None)\n                    request = Request.from_query_input(\n                        query_input=query_input, **query_args\n                    )\n                else:\n                    request = Request(**query_args)\n\n            assert isinstance(request, Request), (\n                f\"Request must be an instance of Request or None, got {type(request)}\"\n            )\n\n            # For debug, return Request if requested\n            if return_request:\n                return request\n            # For debug, return error if requested\n            if return_error:\n                from conduit.tests.fixtures import sample_error\n\n                return sample_error\n\n            # Check cache first\n            logger.info(\"Checking cache for existing results.\")\n            if cache and self.conduit_cache:\n                cached_result = self.conduit_cache.check_for_model(request)\n                if isinstance(cached_result, ConduitResult):\n                    return cached_result  # This should be a Response (part of ConduitResult)\n                elif cached_result == None:\n                    logger.info(\"No cached result found, proceeding with query.\")\n                    pass\n                elif cached_result and not isinstance(cached_result, ConduitResult):\n                    logger.error(\n                        f\"Cache returned a non-ConduitResult type: {type(cached_result)}. Ensure the cache is properly configured.\"\n                    )\n                    raise ValueError(\n                        f\"Cache returned a non-ConduitResult type: {type(cached_result)}. Ensure the cache is properly configured.\"\n                    )\n            # Execute the query\n            logger.info(\"Executing query with client.\")\n            start_time = time()\n            result, usage = self._client.query(request)\n            stop_time = time()\n            logger.info(f\"Query executed in {stop_time - start_time:.2f} seconds.\")\n\n            # Handle streaming responses\n            from conduit.model.clients.openai_client import Stream\n            from conduit.model.clients.anthropic_client import Stream as AnthropicStream\n\n            if isinstance(result, Stream) or isinstance(result, AnthropicStream):\n                if stream:\n                    logger.info(\"Returning streaming response.\")\n                    return result  # Return stream directly\n                else:\n                    logger.error(\n                        \"Streaming responses are not supported in this method. \"\n                        \"Set stream=True to receive streamed responses.\"\n                    )\n                    raise ValueError(\n                        \"Streaming responses are not supported in this method. \"\n                        \"Set stream=True to receive streamed responses.\"\n                    )\n\n            # Construct Response object\n            from conduit.result.response import Response\n            from pydantic import BaseModel\n\n            if isinstance(result, Response):\n                logger.info(\"Returning existing Response object.\")\n                response = result\n            elif isinstance(result, str) or isinstance(result, BaseModel):\n                logger.info(\n                    \"Constructing Response object from result string or BaseModel.\"\n                )\n                # Construct relevant Message type per requested output_type\n                match output_type:\n                    case \"text\":  # result is a string\n                        from conduit.message.textmessage import TextMessage\n\n                        assistant_message = TextMessage(\n                            role=\"assistant\", content=result\n                        )\n                    case \"image\":  # result is a base64 string of an image\n                        assert isinstance(result, str), (\n                            \"Image generation request should not return a BaseModel; we need base64 string.\"\n                        )\n                        from conduit.message.imagemessage import ImageMessage\n\n                        assistant_message = ImageMessage.from_base64(\n                            role=\"assistant\", text_content=\"\", image_content=result\n                        )\n                    case \"audio\":  # result is a base64 string of an audio\n                        assert isinstance(result, str), (\n                            \"Audio generation (TTS) request should not return a BaseModel; we need base64 string.\"\n                        )\n                        from conduit.message.audiomessage import AudioMessage\n\n                        assistant_message = AudioMessage.from_base64(\n                            role=\"assistant\",\n                            audio_content=result,\n                            text_content=\"\",\n                            format=\"mp3\",\n                        )\n\n                response = Response(\n                    message=assistant_message,\n                    request=request,\n                    duration=stop_time - start_time,\n                    input_tokens=usage.input_tokens,\n                    output_tokens=usage.output_tokens,\n                )\n            else:\n                logger.error(\n                    f\"Unexpected result type: {type(result)}. Expected Response or str.\"\n                )\n                raise TypeError(\n                    f\"Unexpected result type: {type(result)}. Expected Response or str.\"\n                )\n\n            # Update cache after successful query\n            logger.info(\"Updating cache with the new response.\")\n            if cache and self.conduit_cache:\n                self.conduit_cache.store_for_model(request, response)\n\n            return response  # Return Response (part of ConduitResult)\n\n        except ValidationError as e:\n            conduit_error = ConduitError.from_exception(\n                e,\n                code=\"validation_error\",\n                category=\"client\",\n                request_request=request.model_dump() if request else {},\n            )\n            logger.error(f\"Validation error: {conduit_error}\")\n            return conduit_error\n        except Exception as e:\n            conduit_error = ConduitError.from_exception(\n                e,\n                code=\"query_error\",\n                category=\"client\",\n                request_request=request.model_dump() if request else {},\n            )\n            logger.error(f\"Error during query: {conduit_error}\")\n            return conduit_error\n\n    def tokenize(self, text: str) -&gt; int:\n        \"\"\"\n        Get the token length for the given model.\n        Implementation at the client level.\n        \"\"\"\n        return self._client.tokenize(model=self.model, text=text)\n\n    def pretty(self, user_input):\n        pretty = user_input.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n        return pretty[:60] + \"...\" if len(pretty) &gt; 60 else pretty\n\n    def __repr__(self):\n        attributes = \", \".join(\n            [f\"{k}={repr(v)[:50]}\" for k, v in self.__dict__.items()]\n        )\n        return f\"{self.__class__.__name__}({attributes})\"\n</file>\n    <file path=\"model/model_async.py\">from conduit.model.model import Model\nfrom conduit.request.request import Request\nfrom conduit.progress.wrappers import progress_display\nfrom conduit.progress.verbosity import Verbosity\nfrom conduit.message.textmessage import TextMessage\nfrom conduit.result.result import ConduitResult\nfrom conduit.result.response import Response\nfrom conduit.cache.cache import ConduitCache\nfrom conduit.logs.logging_config import get_logger\nfrom typing import Optional\nfrom time import time\nfrom pydantic import ValidationError, BaseModel\nimport importlib\n\nlogger = get_logger(__name__)\n\n\nclass ModelAsync(Model):\n    _async_clients = {}  # Separate from Model._clients\n    conduit_cache: Optional[ConduitCache] = None\n\n    def _get_client_type(self, model: str) -&gt; tuple:\n        \"\"\"\n        Overrides the parent method to return the async version of each client type.\n        \"\"\"\n        from conduit.model.models.modelstore import ModelStore\n\n        model_list = ModelStore.models()\n        if model in model_list[\"openai\"]:\n            return \"openai\", \"OpenAIClientAsync\"\n        elif model in model_list[\"anthropic\"]:\n            return \"anthropic\", \"AnthropicClientAsync\"\n        elif model in model_list[\"ollama\"]:\n            return \"ollama\", \"OllamaClientAsync\"\n        elif model in model_list[\"google\"]:\n            return \"google\", \"GoogleClientAsync\"\n        elif model in model_list[\"perplexity\"]:\n            return \"perplexity\", \"PerplexityClientAsync\"\n        else:\n            raise ValueError(f\"Model {model} not found in models\")\n\n    @classmethod\n    def _get_client(cls, client_type: tuple):\n        # print(f\"client type: {client_type}\")\n        if client_type[0] not in cls._async_clients:\n            try:\n                module = importlib.import_module(\n                    f\"conduit.model.clients.{client_type[0].lower()}_client\"\n                )\n                client_class = getattr(module, f\"{client_type[1]}\")\n                cls._async_clients[client_type[0]] = client_class()\n            except ImportError as e:\n                raise ImportError(f\"Failed to import {client_type} client: {str(e)}\")\n        client_object = cls._async_clients[client_type[0]]\n        if not client_object:\n            raise ValueError(f\"Client {client_type} not found in clients\")\n        return client_object\n\n    @progress_display\n    async def query_async(\n        self,\n        # Standard params\n        query_input: str | list,\n        verbose: Verbosity = Verbosity.PROGRESS,\n        response_model: type[BaseModel] | None = None,\n        raw=False,\n        cache=False,\n        print_response=False,\n        # If hand-rolling Request params, you can just pass the object directly.\n        request: Optional[Request] = None,\n        # For debug: return Request, or an example Error\n        return_request: bool = False,\n        return_error: bool = False,\n    ) -&gt; ConduitResult:\n        try:\n            if request == None:\n                import inspect\n\n                frame = inspect.currentframe()\n                args, _, _, values = inspect.getargvalues(frame)\n\n                query_args = {k: values[k] for k in args if k != \"self\"}\n                query_args[\"model\"] = self.model\n                if query_input:\n                    query_args.pop(\"query_input\", None)\n                    request = Request.from_query_input(\n                        query_input=query_input, **query_args\n                    )\n                else:\n                    request = Request(**query_args)\n\n            assert request and isinstance(request, Request), (\n                f\"request should be a Request object, not {type(request)}\"\n            )\n\n            # For debug, return Request if requested\n            if return_request:\n                return request\n            # For debug, return error if requested\n            if return_error:\n                from conduit.tests.fixtures import sample_error\n\n                return sample_error\n\n            # Check cache first\n            if cache and self.conduit_cache:\n                cached_result = self.conduit_cache.check_for_model(request)\n                if cached_result is not None:\n                    return cached_result  # This should be a Response\n\n            # Execute the query\n            start_time = time()\n            result, usage = await self._client.query(request)\n            stop_time = time()\n\n            # Create Response object\n            if isinstance(result, Response):\n                response = result\n            elif isinstance(result, str):\n                assistant_message = TextMessage(role=\"assistant\", content=result)\n\n                response = Response(\n                    message=assistant_message,\n                    request=request,\n                    duration=stop_time - start_time,\n                    input_tokens=usage.input_tokens,\n                    output_tokens=usage.output_tokens,\n                )\n            else:\n                # Handle other result types (BaseModel, etc.)\n                assistant_message = TextMessage(role=\"assistant\", content=result)\n\n                response = Response(\n                    message=assistant_message,\n                    request=request,\n                    duration=stop_time - start_time,\n                    input_tokens=usage.input_tokens,\n                    output_tokens=usage.output_tokens,\n                )\n\n            # Update cache after successful query\n            if cache and self.conduit_cache:\n                self.conduit_cache.store_for_model(request, response)\n\n            return response  # Always return Response (part of ConduitResult)\n\n        except ValidationError as e:\n            from conduit.result.error import ConduitError\n\n            return ConduitError.from_exception(\n                e,\n                code=\"validation_error\",\n                category=\"client\",\n                request_request=request.model_dump() if request else {},\n            )\n        except Exception as e:\n            from conduit.result.error import ConduitError\n\n            return ConduitError.from_exception(\n                e,\n                code=\"async_query_error\",\n                category=\"client\",\n                request_request=request.model_dump() if request else {},\n            )\n</file>\n    <file path=\"model/model_client.py\">\"\"\"\nClientModel - A server-based Model implementation that maintains protocol compatibility with Model class.\n\nUnlike ModelAsync, this does NOT inherit from Model, but implements the same interface.\n\"\"\"\n\nfrom conduit.progress.wrappers import progress_display\nfrom conduit.progress.verbosity import Verbosity\nfrom conduit.request.request import Request\nfrom conduit.result.result import ConduitResult\nfrom conduit.result.error import ConduitError\nfrom conduit.logs.logging_config import get_logger\nfrom conduit.request.outputtype import OutputType\nfrom conduit.message.message import Message\nfrom pydantic import ValidationError, BaseModel\nfrom typing import Optional, TYPE_CHECKING\nfrom time import time\n\n# Load only if type checking\nif TYPE_CHECKING:\n    from rich.console import Console\n\nlogger = get_logger(__name__)\n\n\nclass ModelClient:\n    \"\"\"\n    Server-based model client that implements the same protocol as Model class.\n    Works seamlessly with Conduit and other components.\n    \"\"\"\n\n    def __init__(\n        self, model: str = \"gpt-oss:latest\", console: Optional[\"Console\"] = None\n    ):\n        \"\"\"\n        Initialize ClientModel with server connection.\n\n        Args:\n            model: Model name (must be available on server)\n            server_url: Optional server URL (defaults to SiphonClient default)\n            console: Optional Rich console for progress display\n        \"\"\"\n        self.model = model\n        self._console = console\n        self._client = self._initialize_client()\n        self._validate_server_model()\n\n    def _initialize_client(self):\n        \"\"\"Initialize SiphonClient connection\"\"\"\n        from SiphonServer.client.siphonclient import SiphonClient\n\n        client = SiphonClient()\n\n        # Test connection\n        try:\n            status = client.get_status()\n            if status.get(\"status\") != \"healthy\":\n                raise ConnectionError(\"Server is not healthy\")\n            return client\n        except Exception as e:\n            raise ConnectionError(f\"Failed to connect to Siphon server: {e}\")\n\n    def _validate_server_model(self):\n        \"\"\"Validate that the model is available on the server\"\"\"\n        try:\n            status = self._client.get_status()\n            available_models = status.get(\"models_available\", [])\n\n            if self.model not in available_models:\n                raise ValueError(\n                    f\"Model '{self.model}' not available on server. \"\n                    f\"Available models: {available_models}\"\n                )\n        except Exception as e:\n            raise ValueError(f\"Failed to validate model on server: {e}\")\n\n    @property\n    def console(self):\n        \"\"\"\n        Returns the effective console (matches Model's console hierarchy)\n        \"\"\"\n        if self._console:\n            return self._console\n\n        import sys\n\n        # Check for Model._console\n        if \"conduit.model.model\" in sys.modules:\n            Model = sys.modules[\"conduit.model.model\"].Model\n            model_console = getattr(Model, \"_console\", None)\n            if model_console:\n                return model_console\n\n        # Check for SyncConduit._console\n        if \"conduit.conduit.sync_conduit\" in sys.modules:\n            conduit = sys.modules[\"conduit.conduit.sync_conduit\"].SyncConduit\n            conduit_console = getattr(conduit, \"_console\", None)\n            if conduit_console:\n                return conduit_console\n\n        return None\n\n    @console.setter\n    def console(self, console: \"Console\"):\n        \"\"\"Sets the console object for rich output\"\"\"\n        self._console = console\n\n    @progress_display\n    def query(\n        self,\n        # Standard parameters (match Model.query signature)\n        query_input: str | list | Message | None = None,\n        response_model: type[\"BaseModel\"] | None = None,\n        cache=True,\n        temperature: Optional[float] = None,\n        stream: bool = False,\n        output_type: OutputType = \"text\",\n        max_tokens: Optional[int] = None,\n        # For progress reporting decorator\n        verbose: Verbosity = Verbosity.PROGRESS,\n        index: int = 0,\n        total: int = 0,\n        # If we're hand-constructing Request params\n        request: Optional[Request] = None,\n        # Options for debugging\n        return_request: bool = False,\n        return_error: bool = False,\n    ) -&gt; ConduitResult:\n        \"\"\"\n        Query the server model - matches Model.query() signature exactly\n        \"\"\"\n        try:\n            # Construct Request object if not provided\n            if not request:\n                logger.info(\"Constructing Request object for server client\")\n                import inspect\n\n                frame = inspect.currentframe()\n                args, _, _, values = inspect.getargvalues(frame)\n\n                query_args = {k: values[k] for k in args if k != \"self\"}\n                query_args[\"model\"] = self.model\n                cache = query_args.pop(\"cache\", False)\n\n                if query_input:\n                    query_args.pop(\"query_input\", None)\n                    request = Request.from_query_input(\n                        query_input=query_input, **query_args\n                    )\n                else:\n                    request = Request(**query_args)\n\n            # For debug, return Request if requested\n            if return_request:\n                return request\n\n            # For debug, return error if requested\n            if return_error:\n                from conduit.tests.fixtures import sample_error\n\n                return sample_error\n\n            # Server doesn't support streaming\n            if stream:\n                logger.warning(\n                    \"Server does not support streaming, ignoring stream=True\"\n                )\n\n            # Execute the query via server\n            logger.info(\"Sending query to Conduit server\")\n            start_time = time()\n\n            # Use SiphonClient's query_sync method\n            response = self._client.query_sync(request)\n\n            stop_time = time()\n            logger.info(\n                f\"Server query completed in {stop_time - start_time:.2f} seconds\"\n            )\n\n            # The server should return a Response object\n            from conduit.result.response import Response\n\n            if isinstance(response, Response):\n                return response\n            else:\n                raise TypeError(f\"Expected Response from server, got {type(response)}\")\n\n        except ValidationError as e:\n            conduit_error = ConduitError.from_exception(\n                e,\n                code=\"validation_error\",\n                category=\"client\",\n                request_request=request.model_dump() if request else {},\n            )\n            logger.error(f\"Validation error: {conduit_error}\")\n            return conduit_error\n        except Exception as e:\n            conduit_error = ConduitError.from_exception(\n                e,\n                code=\"server_query_error\",\n                category=\"client\",\n                request_request=request.model_dump() if request else {},\n            )\n            logger.error(f\"Error during server query: {conduit_error}\")\n            return conduit_error\n\n    def tokenize(self, text: str) -&gt; int:\n        \"\"\"\n        Get token count - fallback to basic estimation since server may not support this\n        \"\"\"\n        try:\n            # Try to use server tokenization if available\n            # This would need to be implemented on the server side\n            return len(text.split()) * 1.3  # Rough estimation\n        except Exception:\n            # Fallback to word-based estimation\n            return int(len(text.split()) * 1.3)\n\n    def __repr__(self):\n        \"\"\"String representation\"\"\"\n        return f\"ClientModel(model='{self.model}', server_url='{self.server_url}')\"\n</file>\n    <file path=\"model/__init__.py\"/>\n    <file path=\"model/clients/perplexity_content.py\">from pydantic import BaseModel\nfrom typing import override\n\n\nclass PerplexityCitation(BaseModel):\n    title: str\n    url: str\n    date: str | None\n\n\nclass PerplexityContent(BaseModel):\n    text: str\n    citations: list[PerplexityCitation]\n\n    @override\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Format the content and citations for display, in markdown format.\n        \"\"\"\n        if not self.citations:\n            return self.text\n        citations_strs = []\n        for citation in self.citations:\n            citation_str = f\"[{citation.title}]({citation.url})\"\n            if citation.date:\n                citation_str += f\" ({citation.date})\"\n            citations_strs.append(citation_str)\n        citations_str = \"\\n\".join(f\"- {c}\" for c in citations_strs)\n        return f\"{self.text}\\n\\n## Sources:\\n{citations_str}\"\n</file>\n    <file path=\"model/clients/ollama_client.py\">\"\"\"\nClient subclass for Ollama models.\nThis doesn't require an API key since these are locally hosted models.\nWe can use openai api calls to the ollama server, but we use the instructor library to handle the API calls.\nThis has special logic for updating the models.json file, since the available Ollama models will depend on what we have pulled.\nWe define preferred defaults for context sizes in a separate json file.\n\"\"\"\n\nfrom conduit.model.clients.client import Client, Usage\nfrom conduit.request.request import Request\nfrom conduit.logs.logging_config import get_logger\nfrom pydantic import BaseModel\nfrom openai import OpenAI, AsyncOpenAI, Stream\nfrom xdg_base_dirs import xdg_state_home, xdg_config_home\nfrom pathlib import Path\nfrom collections import defaultdict\nimport instructor, ollama, json\n\nlogger = get_logger(__name__)\n\nDIR_PATH = Path(__file__).resolve().parent\nOLLAMA_MODELS_PATH = xdg_state_home() / \"conduit\" / \"ollama_models.json\"\nOLLAMA_CONTEXT_SIZES_PATH = xdg_config_home() / \"conduit\" / \"ollama_context_sizes.json\"\n\n\nclass OllamaClient(Client):\n    \"\"\"\n    This is a base class; we have two subclasses: OpenAIClientSync and OpenAIClientAsync.\n    Don't import this.\n    \"\"\"\n\n    # Load Ollama context sizes from the JSON file\n    try:\n        Path(OLLAMA_CONTEXT_SIZES_PATH).parent.mkdir(parents=True, exist_ok=True)\n        with open(OLLAMA_CONTEXT_SIZES_PATH) as f:\n            _ollama_context_data = json.load(f)\n\n        # Use defaultdict to set default context size to 4096 if not specified\n        _ollama_context_sizes = defaultdict(lambda: 32768)\n        _ollama_context_sizes.update(_ollama_context_data)\n    except Exception:\n        logger.warning(\n            f\"Could not load Ollama context sizes from {OLLAMA_CONTEXT_SIZES_PATH}. Using default of 32768.\"\n        )\n        _ollama_context_sizes = defaultdict(lambda: 32768)\n\n    def __init__(self):\n        self._client = self._initialize_client()\n        self.update_ollama_models()  # This allows us to keep the model file up to date.\n\n    def _initialize_client(self):\n        \"\"\"\n        Logic for this is unique to each client (sync / async).\n        \"\"\"\n        pass\n\n    def _get_api_key(self):\n        \"\"\"\n        Best thing about Ollama; no API key needed.\n        \"\"\"\n        return \"\"\n\n    def update_ollama_models(self):\n        \"\"\"\n        Updates the list of Ollama models.\n        We run is every time ollama is initialized.\n        \"\"\"\n        # Ensure the directory exists\n        Path(OLLAMA_MODELS_PATH).parent.mkdir(parents=True, exist_ok=True)\n        # Lazy load ollama module\n        ollama_models = [m[\"model\"] for m in ollama.list()[\"models\"]]\n        ollama_model_dict = {\"ollama\": ollama_models}\n        with open(OLLAMA_MODELS_PATH, \"w\") as f:\n            json.dump(ollama_model_dict, f)\n\n    def tokenize(self, model: str, text: str) -&gt; int:\n        \"\"\"\n        Count tokens using Ollama's generate API via the official library.\n        This actually runs a text generation, but only only for one token to minimize compute, since we only want the count of input tokens.\n        \"\"\"\n        response = ollama.generate(\n            model=model,\n            prompt=text,\n            options={\"num_predict\": 1},  # Set to minimal generation\n        )\n        return int(response.get(\"prompt_eval_count\", 0))\n\n\nclass OllamaClientSync(OllamaClient):\n    def _initialize_client(self):\n        client = instructor.from_openai(\n            OpenAI(\n                base_url=\"http://localhost:11434/v1\",\n                api_key=\"ollama\",  # required, but unused\n            ),\n            mode=instructor.Mode.JSON,\n        )\n        return client\n\n    def query(\n        self,\n        request: Request,\n    ) -&gt; tuple:\n        structured_response = None\n        if request.response_model is not None:\n            # We want the raw response from OpenAI, so we use `create_with_completion`\n            structured_response, result = (\n                self._client.chat.completions.create_with_completion(\n                    **request.to_openai()\n                )\n            )\n        else:\n            # Use the standard completion method\n            result = self._client.chat.completions.create(**request.to_openai())\n        # Capture usage\n        usage = Usage(\n            input_tokens=result.usage.prompt_tokens,\n            output_tokens=result.usage.completion_tokens,\n        )\n        if structured_response is not None:\n            # If we have a structured response, return it along with usage\n            return structured_response, usage\n        # Try to retrieve the text first\n        try:\n            return result.choices[0].message.content, usage\n        except AttributeError:\n            # If the result is not in the expected format, return the raw result\n            pass\n        if isinstance(result, BaseModel):\n            return result, usage\n        if isinstance(result, Stream):\n            # Handle streaming response if needed\n            return result, usage\n\n\nclass OllamaClientAsync(OllamaClient):\n    def _initialize_client(self):\n        \"\"\"\n        This is just ollama's async client.\n        \"\"\"\n        ollama_async_client = instructor.from_openai(\n            AsyncOpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n            mode=instructor.Mode.JSON,\n        )\n        return ollama_async_client\n\n    async def query(\n        self,\n        request: Request,\n    ) -&gt; tuple:\n        structured_response = None\n        if request.response_model is not None:\n            # We want the raw response from Ollama, so we use `create_with_completion`\n            (\n                structured_response,\n                result,\n            ) = await self._client.chat.completions.create_with_completion(\n                **request.to_openai()\n            )\n        else:\n            # Use the standard completion method\n            result = await self._client.chat.completions.create(**request.to_openai())\n        # Capture usage\n        usage = Usage(\n            input_tokens=result.usage.prompt_tokens,\n            output_tokens=result.usage.completion_tokens,\n        )\n        if structured_response is not None:\n            # If we have a structured response, return it along with usage\n            return structured_response, usage\n        # Try to retrieve the text first\n        try:\n            return result.choices[0].message.content, usage\n        except AttributeError:\n            # If the result is not in the expected format, return the raw result\n            pass\n        if isinstance(result, BaseModel):\n            return result, usage\n</file>\n    <file path=\"model/clients/anthropic_client.py\">\"\"\"\nClient subclass for Anthropic models.\nTBD: implement streaming support.\n\"\"\"\n\nfrom conduit.model.clients.client import Client, Usage\nfrom conduit.request.request import Request\nfrom conduit.model.clients.load_env import load_env\nfrom anthropic import Anthropic, AsyncAnthropic, Stream\nfrom pydantic import BaseModel\nimport instructor, os\n\n\nclass AnthropicClient(Client):\n    def __init__(self):\n        self._client = self._initialize_client()\n\n    def _initialize_client(self):\n        \"\"\"\n        We use the Instructor library by default, as this offers a great interface for doing function calling and working with pydantic objects.\n        \"\"\"\n        anthropic_client = Anthropic(api_key=self._get_api_key())\n        return instructor.from_anthropic(anthropic_client)\n\n    def _get_api_key(self):\n        load_env(\"ANTHROPIC_API_KEY\")\n        if os.getenv(\"ANTHROPIC_API_KEY\") is None:\n            raise ValueError(\"No ANTHROPIC_API_KEY found in environment variables\")\n        else:\n            return os.getenv(\"ANTHROPIC_API_KEY\")\n\n    def tokenize(self, model: str, text: str) -&gt; int:\n        \"\"\"\n        Get token count per official Anthropic api endpoint.\n        \"\"\"\n        # Convert text to message format\n        anthropic_client = Anthropic(api_key=self._get_api_key())\n        messages = [{\"role\": \"user\", \"content\": text}]\n        token_count = anthropic_client.messages.count_tokens(\n            model=model,\n            messages=messages,\n        )\n        return token_count.input_tokens\n\n\nclass AnthropicClientSync(AnthropicClient):\n    def _initialize_client(self):\n        \"\"\"\n        We use the Instructor library by default, as this offers a great interface for doing function calling and working with pydantic objects.\n        \"\"\"\n        anthropic_client = Anthropic(api_key=self._get_api_key())\n        return instructor.from_anthropic(anthropic_client)\n\n    def query(\n        self,\n        request: Request,\n    ) -&gt; tuple:\n        structured_response = None\n        if request.response_model is not None:\n            structured_response, result = (\n                self._client.chat.completions.create_with_completion(\n                    **request.to_anthropic(),\n                )\n            )\n        else:\n            result = self._client.chat.completions.create(**request.to_anthropic())\n        # Capture usage\n        usage = Usage(\n            input_tokens=result.usage.input_tokens,\n            output_tokens=result.usage.output_tokens,\n        )\n        if structured_response is not None:\n            # If we have a structured response, return it along with usage\n            return structured_response, usage\n        # First try to get text content from the result\n        try:\n            result = result.content[0].text\n            return result, usage\n        except AttributeError:\n            pass\n        if isinstance(result, BaseModel):\n            return result, usage\n        elif isinstance(result, Stream):\n            # Handle streaming response if needed\n            return result, usage\n        else:\n            raise ValueError(\n                f\"Unexpected result type from Anthropic API: {type(result)}\"\n            )\n\n\nclass AnthropicClientAsync(AnthropicClient):\n    def _initialize_client(self):\n        \"\"\"\n        We use the Instructor library by default, as this offers a great interface for doing function calling and working with pydantic objects.\n        \"\"\"\n        anthropic_async_client = AsyncAnthropic(api_key=self._get_api_key())\n        return instructor.from_anthropic(anthropic_async_client)\n\n    async def query(\n        self,\n        request: Request,\n    ) -&gt; tuple:\n        structured_response = None\n        if request.response_model is not None:\n            (\n                structured_response,\n                result,\n            ) = await self._client.chat.completions.create_with_completion(\n                **request.to_anthropic(),\n            )\n        else:\n            result = await self._client.chat.completions.create(\n                **request.to_anthropic()\n            )\n        # Capture usage\n        usage = Usage(\n            input_tokens=result.usage.input_tokens,\n            output_tokens=result.usage.output_tokens,\n        )\n        if structured_response is not None:\n            # If we have a structured response, return it along with usage\n            return structured_response, usage\n        # First try to get text content from the result\n        try:\n            result = result.content[0].text\n            return result, usage\n        except AttributeError:\n            pass\n        if isinstance(result, BaseModel):\n            return result, usage\n        elif isinstance(result, Stream):\n            # Handle streaming response if needed\n            return result, usage\n        else:\n            raise ValueError(\n                f\"Unexpected result type from Anthropic API: {type(result)}\"\n            )\n</file>\n    <file path=\"model/clients/openai_client.py\">from conduit.model.clients.client import Client, Usage\nfrom conduit.model.clients.load_env import load_env\nfrom conduit.request.request import Request\nfrom conduit.logs.logging_config import get_logger\nfrom openai import OpenAI, AsyncOpenAI, Stream\nfrom pydantic import BaseModel\nimport instructor, tiktoken\n\n\nlogger = get_logger(__name__)\n\n\nclass OpenAIClient(Client):\n    \"\"\"\n    This is a base class; we have two subclasses: OpenAIClientSync and OpenAIClientAsync.\n    Don't import this.\n    \"\"\"\n\n    def __init__(self):\n        self._client = self._initialize_client()\n\n    def _get_api_key(self) -&gt; str:\n        api_key = load_env(\"OPENAI_API_KEY\")\n        return api_key\n\n    def tokenize(self, model: str, text: str) -&gt; int:\n        \"\"\"\n        Return the token count for a string, per model's tokenization function.\n        \"\"\"\n        try:\n            encoding = tiktoken.encoding_for_model(model)\n        except KeyError:\n            encoding = tiktoken.get_encoding(\"cl100k_base\")\n        token_count = len(encoding.encode(text))\n        return token_count\n\n\nclass OpenAIClientSync(OpenAIClient):\n    def _initialize_client(self) -&gt; object:\n        \"\"\"\n        We use the Instructor library by default, as this offers a great interface for doing function calling and working with pydantic objects.\n        \"\"\"\n        openai_client = OpenAI(api_key=self._get_api_key())\n        return instructor.from_openai(openai_client)\n\n    def query(\n        self,\n        request: Request,\n    ) -&gt; tuple:\n        match request.output_type:\n            case \"text\":\n                return self._generate_text(request)\n            case \"image\":\n                return self._generate_image(request)\n            case \"audio\":\n                return self._generate_audio(request)\n            case _:\n                raise ValueError(f\"Unsupported output type: {request.output_type}\")\n\n    def _generate_text(self, request: Request) -&gt; tuple:\n        structured_response = None\n        if request.response_model is not None:\n            # We want the raw response from OpenAI, so we use `create_with_completion`\n            structured_response, result = (\n                self._client.chat.completions.create_with_completion(\n                    **request.to_openai()\n                )\n            )\n        else:\n            # Use the standard completion method\n            result = self._client.chat.completions.create(**request.to_openai())\n        # Capture usage\n        usage = Usage(\n            input_tokens=result.usage.prompt_tokens,\n            output_tokens=result.usage.completion_tokens,\n        )\n        if structured_response is not None:\n            # If we have a structured response, return it along with usage\n            return structured_response, usage\n        # First try to get text content from the result\n        try:\n            result = result.choices[0].message.content\n            return result, usage\n        except AttributeError:\n            # If the result is a BaseModel or Stream, handle accordingly\n            pass\n        if isinstance(result, BaseModel):\n            return result, usage\n        elif isinstance(result, Stream):\n            # Handle streaming response if needed\n            return result, usage\n\n    def _generate_image(self, request: Request) -&gt; tuple:\n        response = self._client.images.generate(\n            model=request.model,\n            prompt=request.messages[-1].content,\n            n=1,\n            size=\"1024x1024\",\n            response_format=\"b64_json\",\n        )\n        result = response.data[0].b64_json\n        assert isinstance(result, str)\n        usage = Usage(input_tokens=0, output_tokens=0)\n        return result, usage\n\n    def _generate_audio(self, request: Request) -&gt; tuple:\n        raise NotImplementedError\n\n\nclass OpenAIClientAsync(OpenAIClient):\n    def _initialize_client(self):\n        \"\"\"\n        We use the Instructor library by default, as this offers a great interface for doing function calling and working with pydantic objects.\n        \"\"\"\n        openai_async_client = AsyncOpenAI(api_key=self._get_api_key())\n        return instructor.from_openai(openai_async_client)\n\n    async def query(\n        self,\n        request: Request,\n    ) -&gt; tuple:\n        structured_response = None\n        if request.response_model is not None:\n            # We want the raw response from OpenAI, so we use `create_with_completion`\n            (\n                structured_response,\n                result,\n            ) = await self._client.chat.completions.create_with_completion(\n                **request.to_openai()\n            )\n        else:\n            # Use the standard completion method\n            result = await self._client.chat.completions.create(**request.to_openai())\n        # Capture usage\n        usage = Usage(\n            input_tokens=result.usage.prompt_tokens,\n            output_tokens=result.usage.completion_tokens,\n        )\n        if structured_response is not None:\n            # If we have a structured response, return it along with usage\n            return structured_response, usage\n        try:\n            result = result.choices[0].message.content\n            return result, usage\n        except AttributeError:\n            # If the result is a BaseModel or Stream, handle accordingly\n            pass\n</file>\n    <file path=\"model/clients/load_env.py\">from dotenv import load_dotenv\nimport os\n\n\ndef load_env(api_key: str) -&gt; str:\n    \"\"\"\n    Load any .env files that exist, then load api_key from environment.\n    If user has no api_key, return an error.\n    \"\"\"\n    # Load .env file it exists\n    load_dotenv()\n    # Get API key with env var taking precedence\n    found_api_key = os.getenv(api_key)\n    if not found_api_key:\n        raise ValueError(f\"{api_key} not found in environment or .env file.\")\n    else:\n        return found_api_key\n</file>\n    <file path=\"model/clients/perplexity_client.py\">\"\"\"\nFor Perplexity models.\nNOTE: these use standard OpenAI SDK, the only difference in processing is that the response object has an extra 'citations' field.\nYou want both the 'content' and 'citations' fields from the response object.\nPerplexity inputs the footnotes within the content.\nFor this reason, we define a Pydantic class as our framework is fine with BaseModels as a response. You can still access it as a string and get the content if needed. Citations can be access by choice.\n\"\"\"\n\nfrom conduit.model.clients.client import Client, Usage\nfrom conduit.model.clients.load_env import load_env\nfrom conduit.model.clients.perplexity_content import (\n    PerplexityContent,\n    PerplexityCitation,\n)\nfrom conduit.request.request import Request\nfrom openai import OpenAI, AsyncOpenAI\nfrom openai.types.chat.chat_completion import ChatCompletion\nfrom pydantic import BaseModel\nimport instructor, tiktoken\n\n\nclass PerplexityClient(Client):\n    \"\"\"\n    This is a base class; we have two subclasses: OpenAIClientSync and OpenAIClientAsync.\n    Don't import this.\n    \"\"\"\n\n    def __init__(self):\n        self._client = self._initialize_client()\n\n    def _initialize_client(self):\n        \"\"\"\n        Logic for this is unique to each client (sync / async).\n        \"\"\"\n        pass\n\n    def _get_api_key(self):\n        api_key = load_env(\"PERPLEXITY_API_KEY\")\n        return api_key\n\n    def tokenize(self, model: str, text: str) -&gt; int:\n        \"\"\"\n        Return the token count for a string, per model's tokenization function.\n        cl100k_base is good enough for Perplexity, per Perplexity documentation.\n        \"\"\"\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n        token_count = len(encoding.encode(text))\n        return token_count\n\n\nclass PerplexityClientSync(PerplexityClient):\n    def __init__(self):\n        self._client = self._initialize_client()\n\n    def _initialize_client(self):\n        # Keep both raw and instructor clients\n        self._raw_client = OpenAI(\n            api_key=self._get_api_key(), base_url=\"https://api.perplexity.ai\"\n        )\n        return instructor.from_perplexity(self._raw_client)\n\n    def query(self, request: Request) -&gt; tuple:\n        structured_response = None\n        if request.response_model is not None:\n            # Use instructor for structured responses\n            structured_response, result = (\n                self._client.chat.completions.create_with_completion(\n                    **request.to_perplexity()\n                )\n            )\n            # Handle structured response...\n        else:\n            # Use raw client for unstructured responses\n            perplexity_params = request.to_perplexity()\n            perplexity_params.pop(\"response_model\", None)  # Remove None response_model\n            result = self._raw_client.chat.completions.create(**perplexity_params)\n        # Capture usage\n        usage = Usage(\n            input_tokens=result.usage.prompt_tokens,\n            output_tokens=result.usage.completion_tokens,\n        )\n        if structured_response is not None:\n            # If we have a structured response, return it along with usage\n            return structured_response, usage\n        if isinstance(result, ChatCompletion):\n            # Construct a PerplexityContent object from the response\n            citations = result.search_results\n            assert isinstance(citations, list) and all(\n                [isinstance(citation, dict) for citation in citations]\n            ), \"Citations should be a list of dicts\"\n            citations = [PerplexityCitation(**citation) for citation in citations]\n            content = PerplexityContent(\n                text=result.choices[0].message.content, citations=citations\n            )\n            return content, usage\n        if isinstance(result, BaseModel):\n            return result, usage\n        else:\n            raise ValueError(\"Unexpected result type: {}\".format(type(result)))\n\n\nclass PerplexityClientAsync(PerplexityClient):\n    def _initialize_client(self):\n        \"\"\"\n        We use the Instructor library by default, as this offers a great interface for doing function calling and working with pydantic objects.\n        \"\"\"\n        perplexity_client = AsyncOpenAI(\n            api_key=self._get_api_key(), base_url=\"https://api.perplexity.ai\"\n        )\n        return instructor.from_perplexity(perplexity_client)\n\n    async def query(\n        self,\n        request: Request,\n    ) -&gt; tuple:\n        result = await self._client.chat.completions.create(**request.to_perplexity())\n        # Capture usage\n        usage = Usage(\n            input_tokens=result.usage.prompt_tokens,\n            output_tokens=result.usage.completion_tokens,\n        )\n        if isinstance(result, ChatCompletion):\n            # Construct a PerplexityContent object from the response\n            citations = result.search_results\n            assert isinstance(citations, list) and all(\n                [isinstance(citation, dict) for citation in citations]\n            ), \"Citations should be a list of dicts\"\n            citations = [PerplexityCitation(**citation) for citation in citations]\n            content = PerplexityContent(\n                text=result.choices[0].message.content, citations=citations\n            )\n            return content, usage\n        if isinstance(result, BaseModel):\n            return result, usage\n        else:\n            raise ValueError(\"Unexpected result type: {}\".format(type(result)))\n</file>\n    <file path=\"model/clients/__init__.py\"/>\n    <file path=\"model/clients/google_client.py\">\"\"\"\nFor Google Gemini models.\n\"\"\"\n\nfrom conduit.model.clients.client import Client, Usage\nfrom conduit.model.clients.load_env import load_env\nfrom conduit.request.request import Request\nfrom openai import OpenAI, AsyncOpenAI, Stream\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass GoogleClient(Client):\n    \"\"\"\n    This is a base class; we have two subclasses: GoogleClientSync and GoogleClientAsync.\n    Don't import this.\n    \"\"\"\n\n    def __init__(self):\n        self._client = self._initialize_client()\n\n    def _get_api_key(self):\n        api_key = load_env(\"GOOGLE_API_KEY\")\n        return api_key\n\n    def tokenize(self, model: str, text: str) -&gt; int:\n        \"\"\"\n        Get the token count per official tokenizer (through API).\n        \"\"\"\n        # Example using google-generativeai SDK for estimation\n        import google.generativeai as genai\n\n        client = genai.GenerativeModel(model_name=model)\n        response = client.count_tokens(text)\n        token_count = response.total_tokens\n        return token_count\n\n\nclass GoogleClientSync(GoogleClient):\n    def _initialize_client(self):\n        \"\"\"\n        We use the Instructor library by default, as this offers a great interface for doing function calling and working with pydantic objects.\n        \"\"\"\n        client = instructor.from_openai(\n            OpenAI(\n                api_key=self._get_api_key(),\n                base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n            ),\n            mode=instructor.Mode.JSON,\n        )\n        return client\n\n    def query(\n        self,\n        request: Request,\n    ) -&gt; tuple:\n        match request.output_type:\n            case \"text\":\n                return self._generate_text(request)\n            case \"image\":\n                return self._generate_image(request)\n            case \"audio\":\n                return self._generate_audio(request)\n            case _:\n                raise ValueError(f\"Unsupported output type: {request.output_type}\")\n\n    def _generate_text(self, request: Request) -&gt; tuple:\n        structured_response = None\n        if request.response_model is not None:\n            # We want the raw response from Google, so we use `create_with_completion`\n            structured_response, result = (\n                self._client.chat.completions.create_with_completion(\n                    **request.to_openai()\n                )\n            )\n        else:\n            # Use the standard completion method\n            result = self._client.chat.completions.create(**request.to_openai())\n        # Capture usage\n        usage = Usage(\n            input_tokens=result.usage.prompt_tokens,\n            output_tokens=result.usage.completion_tokens,\n        )\n        if structured_response is not None:\n            # If we have a structured response, return it along with usage\n            return structured_response, usage\n        # First try to get text content from the result\n        try:\n            result = result.choices[0].message.content\n            return result, usage\n        except AttributeError:\n            pass\n        if isinstance(result, BaseModel):\n            return result, usage\n        elif isinstance(result, Stream):\n            # Handle streaming response if needed\n            return result, usage\n\n    def _generate_image(self, request: Request) -&gt; tuple:\n        response = self._client.images.generate(\n            model=request.model,\n            prompt=request.messages[-1].content,\n            response_format=\"b64_json\",\n            n=1,\n        )\n        result = response.data[0].b64_json\n        assert isinstance(result, str)\n        usage = Usage(input_tokens=0, output_tokens=0)\n        return result, usage\n\n    def _generate_audio(self, request: Request) -&gt; tuple:\n        raise NotImplementedError\n\n\nclass GoogleClientAsync(GoogleClient):\n    def _initialize_client(self):\n        \"\"\"\n        We use the Instructor library by default, as this offers a great interface for doing function calling and working with pydantic objects.\n        \"\"\"\n        client = instructor.from_openai(\n            AsyncOpenAI(\n                api_key=self._get_api_key(),\n                base_url=\"https://generativelanguage.googleapis.com/v1beta/\",\n            ),\n            mode=instructor.Mode.JSON,\n        )\n        return client\n\n    async def query(\n        self,\n        request: Request,\n    ) -&gt; tuple:\n        structured_response = None\n        if request.response_model is not None:\n            # We want the raw response from OpenAI, so we use `create_with_completion`\n            (\n                structured_response,\n                result,\n            ) = await self._client.chat.completions.create_with_completion(\n                **request.to_openai()\n            )\n        else:\n            # Use the standard completion method\n            result = await self._client.chat.completions.create(**request.to_openai())\n        # Capture usage\n        usage = Usage(\n            input_tokens=result.usage.prompt_tokens,\n            output_tokens=result.usage.completion_tokens,\n        )\n        if structured_response is not None:\n            # If we have a structured response, return it along with usage\n            return structured_response, usage\n        # First try to get text content from the result\n        try:\n            result = result.choices[0].message.content\n            return result, usage\n        except AttributeError:\n            pass\n        if isinstance(result, BaseModel):\n            return result, usage\n        elif isinstance(result, Stream):\n            # Handle streaming response if needed\n            return result, usage\n</file>\n    <file path=\"model/clients/client.py\">\"\"\"\nBase class for clients; openai, anthropic, etc. inherit from this class.\nABC abstract methods are like pydantic validators for classes. They ensure that the child classes implement the methods defined in the parent class.\nIf a client subclass doesn't implement _get_api_key, for example, the code will raise an error when trying to instantiate the subclass.\nThis guarantees all client subclasses have the methods below.\nTODO: implement a class SDK as a protocol for all the library clients (openai, ollama, groq, etc.). This would define an object that has a chat function, fex.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom conduit.request.request import Request\nfrom pydantic import BaseModel\n\n\nclass Usage(BaseModel):\n    \"\"\"\n    Simple data class for usage statistics, standardized across providers.\n    \"\"\"\n\n    input_tokens: int\n    output_tokens: int\n\n\nclass Client(ABC):\n    @abstractmethod\n    def __init__(self):\n        \"\"\"\n        Typically we should see this here:\n        self._client = self._initialize_client()\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _initialize_client(self) -&gt; object:\n        \"\"\"\n        This method should initialize the client object, this is idiosyncratic to each SDK.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _get_api_key(self) -&gt; str:\n        \"\"\"\n        API keys are accessed via dotenv.\n        This should be in an .env file in the root directory.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def query(self, request: Request) -&gt; tuple:\n        \"\"\"\n        All client subclasses must have a query function that can take:\n        - a Request object, which contains all the parameters needed for the query\n\n        And returns\n        - A tuple of\n            - either a string (i.e. text generation) or a Pydantic model (function calling)\n            - a Usage object containing input and output token counts\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def tokenize(self, model: str, text: str) -&gt; int:\n        \"\"\"\n        Get the token count for a text, per a given model's tokenization function.\n        \"\"\"\n        pass\n\n    def __repr__(self):\n        \"\"\"\n        Standard repr.\n        \"\"\"\n        attributes = \", \".join(\n            [f\"{k}={repr(v)[:50]}\" for k, v in self.__dict__.items()]\n        )\n        return f\"{self.__class__.__name__}({attributes})\"\n</file>\n    <file path=\"model/models/providerstore.py\">\"\"\"\nRepository class for managing provider specifications.\nThis module provides functions to create, read, update, and delete provider specifications.\n\"\"\"\n\nfrom conduit.model.models.providerspec import ProviderSpec\nfrom pathlib import Path\n\ndir_path = Path(__file__).parent\nproviders_file = dir_path / \"providers.jsonl\"\n\n\nclass ProviderStore:\n    @classmethod\n    def create_provider(cls, provider: ProviderSpec) -&gt; None:\n        \"\"\"\n        Create a new provider specification and save it to the JSONL file.\n\n        Args:\n            provider (ProviderSpec): The provider specification to create.\n        \"\"\"\n        with open(providers_file, \"a\") as f:\n            f.write(provider.model_dump_json() + \"\\n\")\n\n    @classmethod\n    def get_all_providers(cls) -&gt; list[ProviderSpec]:\n        \"\"\"\n        Read all provider specifications from the JSONL file.\n\n        Returns:\n            list[ProviderSpec]: A list of provider specifications.\n        \"\"\"\n        providers = []\n        with open(providers_file, \"r\") as f:\n            for line in f:\n                providers.append(ProviderSpec.model_validate_json(line.strip()))\n        return providers\n\n    @classmethod\n    def update_provider(cls, provider: ProviderSpec) -&gt; None:\n        \"\"\"\n        Update an existing provider specification in the JSONL file.\n\n        Args:\n            provider (ProviderSpec): The provider specification to update.\n        \"\"\"\n        providers = cls.get_all_providers()\n        updated = False\n        with open(providers_file, \"w\") as f:\n            for p in providers:\n                if p.provider == provider.provider:\n                    f.write(provider.model_dump_json() + \"\\n\")\n                    updated = True\n                else:\n                    f.write(p.model_dump_json() + \"\\n\")\n        if not updated:\n            raise ValueError(f\"Provider {provider.provider} not found for update.\")\n\n    @classmethod\n    def delete_provider(cls, provider_name: str) -&gt; None:\n        \"\"\"\n        Delete a provider specification from the JSONL file.\n\n        Args:\n            provider_name (str): The name of the provider to delete.\n        \"\"\"\n        providers = cls.get_all_providers()\n        with open(providers_file, \"w\") as f:\n            deleted = False\n            for p in providers:\n                if p.provider != provider_name:\n                    f.write(p.model_dump_json() + \"\\n\")\n                else:\n                    deleted = True\n            if not deleted:\n                raise ValueError(f\"Provider {provider_name} not found for deletion.\")\n\n    @classmethod\n    def get_provider(cls, provider_name: str) -&gt; ProviderSpec:\n        \"\"\"\n        Get a specific provider specification by name.\n\n        Args:\n            provider_name (str): The name of the provider to retrieve.\n\n        Returns:\n            ProviderSpec: The provider specification if found.\n\n        Raises:\n            ValueError: If the provider is not found.\n        \"\"\"\n        providers = cls.get_all_providers()\n        for p in providers:\n            if p.provider == provider_name:\n                return p\n        raise ValueError(f\"Provider {provider_name} not found.\")\n</file>\n    <file path=\"model/models/research_models.py\">from conduit.model.models.modelspec import ModelSpecList, ModelSpec\nfrom conduit.model.models.modelspecs_CRUD import (\n    create_modelspecs_from_scratch,\n    add_modelspec,\n    get_all_modelspecs,\n    in_db,\n)\nfrom conduit.model.model import Model\nfrom conduit.conduit.sync_conduit import SyncConduit\nfrom conduit.prompt.prompt import Prompt\nfrom conduit.parser.parser import Parser\nfrom rich.console import Console\n\n\nlist_prompt_str = \"\"\"\nYou are an assistant who will help me identify the capabilities of a list of LLMs.\n\n&lt;provider&gt;\n{{provider}}\n&lt;/provider&gt;\n\n&lt;model_list&gt;\n{{model_list}}\n&lt;/model_list&gt;\n\nYou will be identifying, for each model, whether it supports the following functionalities:\n\n- **text_completion**: The ability to generate, continue, or complete text based on prompts and context\n- **image_analysis**: The capability to analyze, understand, and describe visual content in images\n- **image_gen**: The ability to create or generate new images from text descriptions or prompts\n- **audio_analysis**: The capability to process, transcribe, or analyze audio content and speech\n- **audio_gen**: The ability to synthesize speech or generate audio content from text or other inputs\n- **video_analysis**: The capability to analyze, understand, and extract information from video content\n- **video_gen**: The ability to create or generate video content from text descriptions or other inputs\n- **reasoning**: The capability to perform logical reasoning, problem-solving, and complex analytical tasks\n\nFor each model, please return a ModelCapabilities object with boolean values for each capability, indicating whether the model supports (True) or does not support (False) each functionality.\n\nYou should also generate factual description of each model (50-80 words), including:\n- The model's architecture type and parameter size (if known)\n- Specific capabilities it supports (multimodal inputs, function calling, etc.)\n- Context window size and key technical specifications\n- Release date or version information\n- Primary intended use cases or design focus\n- Any notable technical features or limitations\n\nAvoid promotional language or subjective quality assessments. Focus on objective, verifiable information about what the model can do and its technical characteristics.\n\nReturn a different object for each model -- since there are {{length}} models in our list, return {{length}} ModelCapabilities objects.\n\"\"\".strip()\n\nindividual_prompt_str = \"\"\"\nYou are an assistant who will help me identify the capabilities of a specific LLM.\n\nHere's the provider:\n\n&lt;provider&gt;\n{{provider}}\n&lt;/provider&gt;\n\nAnd here's the specific model:\n\n&lt;model&gt;\n{{model}}\n&lt;/model&gt;\n\nYou will be identifying, for the above model, whether it supports the following functionalities:\n- **text_completion**: The ability to generate, continue, or complete text based on prompts and context\n- **image_analysis**: The capability to analyze, understand, and describe visual content in images\n- **image_gen**: The ability to create or generate new images from text descriptions or prompts\n- **audio_analysis**: The capability to process, transcribe, or analyze audio content and speech\n- **audio_gen**: The ability to synthesize speech or generate audio content from text or other inputs\n- **video_analysis**: The capability to analyze, understand, and extract information from video content\n- **video_gen**: The ability to create or generate video content from text descriptions or other inputs\n- **reasoning**: The capability to perform logical reasoning, problem-solving, and complex analytical tasks\n\nReturn a ModelCapabilities object with boolean values for each capability, indicating whether the model supports (True) or does not support (False) each functionality.\n\nYou should also generate a factual description of the model (50-80 words), including:\n- The model's architecture type and parameter size (if known)\n- Specific capabilities it supports (multimodal inputs, function calling, etc.)\n- Context window size and key technical specifications\n- Release date or version information\n- Primary intended use cases or design focus\n- Any notable technical features or limitations\n\nAvoid promotional language or subjective quality assessments. Focus on objective, verifiable information about what the model can do and its technical characteristics.\n\"\"\".strip()\n\n\ndef get_capabilities_by_provider(\n    provider: str, model_list: list[str]\n) -&gt; list[ModelSpec]:\n    console = Console()\n    SyncConduit._console = console  # Set the console for Conduit to use\n    length = len(model_list)\n    model = Model(\"sonar-pro\")\n    prompt = Prompt(prompt_str)\n    parser = Parser(ModelSpecList)\n    sync_conduit = SyncConduit(model=model, prompt=prompt, parser=parser)\n    response = sync_conduit.run(\n        input_variables={\n            \"provider\": provider,\n            \"model_list\": model_list,\n            \"length\": length,\n        }\n    )\n    return response.content.specs\n\n\ndef get_all_capabilities() -&gt; list[ModelSpec]:\n    \"\"\"\n    Get capabilities for all models across all providers. This shouldn't need to be run often, since it replaces the entire database of model specs.\n    \"\"\"\n    all_models = Model.models()\n    all_specs = []\n    for index, (provider, models) in enumerate(all_models.items()):\n        print(\n            f\"Processing {index + 1}/{len(all_models)}: {provider} with {len(models)} models\"\n        )\n        specs = get_capabilities_by_provider(provider=provider, model_list=models)\n        all_specs.extend(specs)\n    return all_specs\n\n\ndef create_from_scratch() -&gt; None:\n    \"\"\"\n    Create a new empty database for ModelSpecs and populate it with capabilities from all providers.\n    This will overwrite any existing data in modelspecs.json.\n    \"\"\"\n    all_specs = get_all_capabilities()\n    create_modelspecs_from_scratch(all_specs)\n    print(f\"Populated ModelSpecs database with {len(all_specs)} entries.\")\n    # Test retrieval of all specs\n    retrieved_specs = get_all_modelspecs()\n    assert len(retrieved_specs) == len(all_specs), (\n        \"Retrieved specs do not match created specs.\"\n    )\n\n\ndef get_capabilities_by_model(provider: str, model: str) -&gt; ModelSpec:\n    \"\"\"\n    Get capabilities for a specific model.\n    \"\"\"\n    console = Console()\n    SyncConduit._console = console  # Set the console for Conduit to use\n    model_obj = Model(\"sonar-pro\")\n    prompt = Prompt(individual_prompt_str)\n    parser = Parser(ModelSpec)\n    sync_conduit = SyncConduit(model=model_obj, prompt=prompt, parser=parser)\n    response = sync_conduit.run(input_variables={\"provider\": provider, \"model\": model})\n    return response.content\n\n\ndef create_modelspec(model: str) -&gt; None:\n    \"\"\"\n    Create a new ModelSpec in the database.\n    \"\"\"\n    from conduit.model.models.modelstore import ModelStore\n\n    provider = ModelStore.identify_provider(model)\n    model_spec = get_capabilities_by_model(provider, model)\n    if isinstance(model_spec, ModelSpec):\n        model_spec.model = model\n    else:\n        raise ValueError(\n            f\"Expected ModelSpec, got {type(model_spec)} for model {model}\"\n        )\n    if not in_db(model_spec):\n        add_modelspec(model_spec)\n        print(f\"Added ModelSpec for {model_spec.model} to the database.\")\n    else:\n        print(f\"ModelSpec for {model_spec.model} already exists in the database.\")\n\n\nif __name__ == \"__main__\":\n    modelspec = get_capabilities_by_model(\"ollama\", \"qwq:latest\")\n</file>\n    <file path=\"model/models/modelspec.py\">from pydantic import BaseModel, Field\nfrom conduit.model.models.provider import Provider\nfrom typing import Optional\n\n\nclass ModelSpec(BaseModel):\n    model: str = Field(\n        ...,\n        description=\"The name of the model, e.g. gpt-4o, claude-2, gemini-1.5-flash, etc.\",\n    )\n    description: Optional[str] = Field(\n        ..., description=\"A brief description of the model's capabilities and features.\"\n    )\n\n    # Provider-specific capabilities\n    provider: Provider = Field(\n        ...,\n        description=\"The provider of the model, e.g. openai, ollama, perplexity, google, anthropic, etc.\",\n    )\n    temperature_range: list[float] = Field(\n        ...,\n        description=\"The supported temperature range for controlling randomness in model outputs, where lower values produce more deterministic responses and higher values increase creativity. OpenAI's standard range is 0.0 to 2.0, e.g. [0.0, 2.0]\",\n    )\n    context_window: int = Field(\n        ...,\n        description=\"The maximum number of tokens the model can process in a single request, including both input and output tokens. OpenAI refers to this as 'max_tokens' in their API documentation. This represents Ollama's 'num_ctx' parameter, e.g. 128000\",\n    )\n\n    # Model characteristics\n    parameter_count: Optional[str] = Field(\n        None, description=\"Parameter count (e.g., '7b', '70b', '405b')\"\n    )\n    context_window: int = Field(..., description=\"Maximum context window in tokens\")\n    knowledge_cutoff: Optional[str] = Field(\n        default=None,\n        description=\"Date when the model's knowledge was last updated; None if unknown or continuously updated\",\n    )\n    text_completion: bool = Field(\n        ...,\n        description=\"The ability to generate, continue, or complete text based on prompts and context, e.g. gpt-4o\",\n    )\n    image_analysis: bool = Field(\n        ...,\n        description=\"The capability to analyze, understand, and describe visual content in images, e.g. claude-sonnet-4-20250514\",\n    )\n    image_gen: bool = Field(\n        ...,\n        description=\"The ability to create or generate new images from text descriptions or prompts, e.g. gemini-2.0-flash-001\",\n    )\n    audio_analysis: bool = Field(\n        ...,\n        description=\"The capability to process, transcribe, or analyze audio content and speech, e.g. gpt-4o-audio-preview\",\n    )\n    audio_gen: bool = Field(\n        ...,\n        description=\"The ability to synthesize speech or generate audio content from text or other inputs, e.g. gpt-4o-audio-preview\",\n    )\n    video_analysis: bool = Field(\n        ...,\n        description=\"The capability to analyze, understand, and extract information from video content, e.g. llama3.2-vision:11b\",\n    )\n    video_gen: bool = Field(\n        ...,\n        description=\"The ability to create or generate video content from text descriptions or other inputs, e.g. gemini-2.5-pro-preview-05-06\",\n    )\n    reasoning: bool = Field(\n        ...,\n        description=\"The capability to perform logical reasoning, problem-solving, and complex analytical tasks, e.g. o1-preview\",\n    )\n\n    def __str__(self):\n        return f\"{self.model} ({self.provider})\"\n\n    def __repr__(self):\n        return f\"ModelSpec(model={self.model}, provider={self.provider})\"\n\n    @property\n    def card(self):\n        \"\"\"\n        Pretty print the model capabilities.\n        \"\"\"\n        from rich.console import Console\n        from rich.markdown import Markdown\n        from rich.table import Table\n\n        console = Console()\n        table = Table(\n            title=f\"[bold bright_yellow]Model Capabilities for {self.provider + '/' + self.model}[/bold bright_yellow]\"\n        )\n        table.add_column(\"Capability\", justify=\"left\", style=\"cyan\")\n        table.add_column(\"Supported\", justify=\"center\", style=\"green\")\n        table.add_column(\"Description\", justify=\"left\", style=\"white\")\n        if self.temperature_range:\n            table.add_row(\n                \"Temperature Range\",\n                f\"{self.temperature_range[0]} to {self.temperature_range[1]}\",\n                \"The range of temperature values supported by the model\",\n            )\n        else:\n            table.add_row(\n                \"Temperature Range\",\n                \"Unknown\",\n                \"The range of temperature values supported by the model\",\n            )\n        table.add_row(\n            \"Context Window\",\n            str(self.context_window),\n            \"The maximum number of tokens the model can process in a single request\",\n        )\n        table.add_row(\n            \"Parameter Count\",\n            self.parameter_count or \"Unknown\",\n            \"The number of parameters in the model\",\n        )\n        table.add_row(\n            \"Knowledge Cutoff\",\n            self.knowledge_cutoff or \"Unknown\",\n            \"The date when the model's knowledge was last updated\",\n        )\n        table.add_row(\n            \"Text Completion\",\n            str(self.text_completion),\n            \"The ability to generate, continue, or complete text based on prompts and context\",\n        )\n        table.add_row(\n            \"Image Analysis\",\n            str(self.image_analysis),\n            \"The capability to analyze, understand, and describe visual content in images\",\n        )\n        table.add_row(\n            \"Image Generation\",\n            str(self.image_gen),\n            \"The ability to create or generate new images from text descriptions or prompts\",\n        )\n        table.add_row(\n            \"Audio Analysis\",\n            str(self.audio_analysis),\n            \"The capability to process, transcribe, or analyze audio content and speech\",\n        )\n        table.add_row(\n            \"Audio Generation\",\n            str(self.audio_gen),\n            \"The ability to synthesize speech or generate audio content from text or other inputs\",\n        )\n        table.add_row(\n            \"Video Analysis\",\n            str(self.video_analysis),\n            \"The capability to analyze, understand, and extract information from video content\",\n        )\n        table.add_row(\n            \"Video Generation\",\n            str(self.video_gen),\n            \"The ability to create or generate video content from text descriptions or other inputs\",\n        )\n        table.add_row(\n            \"Reasoning\",\n            str(self.reasoning),\n            \"The capability to perform logical reasoning, problem-solving, and complex analytical tasks\",\n        )\n        description = f\"\\n[bold green]Description:[/bold green] [yellow]{self.description or 'No description available'}[/yellow]\\n\"\n\n        # Add a bar for each capability\n        console.print(\"\\n\")\n        console.print(table)\n        console.print(description)\n\n    # Validation methods\n    def is_image_analysis_supported(self) -&gt; bool:\n        \"\"\"Check if image analysis is supported.\"\"\"\n        return self.image_analysis\n\n    def is_image_generation_supported(self) -&gt; bool:\n        \"\"\"Check if image generation is supported.\"\"\"\n        return self.image_gen\n\n    def is_audio_analysis_supported(self) -&gt; bool:\n        \"\"\"Check if audio analysis is supported.\"\"\"\n        return self.audio_analysis\n\n    def is_audio_generation_supported(self) -&gt; bool:\n        \"\"\"Check if audio generation is supported.\"\"\"\n        return self.audio_gen\n\n    def is_video_analysis_supported(self) -&gt; bool:\n        \"\"\"Check if video analysis is supported.\"\"\"\n        return self.video_analysis\n\n    def is_video_generation_supported(self) -&gt; bool:\n        \"\"\"Check if video generation is supported.\"\"\"\n        return self.video_gen\n\n    def is_text_completion_supported(self) -&gt; bool:\n        \"\"\"Check if text completion is supported.\"\"\"\n        return self.text_completion\n\n    def is_reasoning_supported(self) -&gt; bool:\n        \"\"\"Check if reasoning capabilities are supported.\"\"\"\n        return self.reasoning\n\n\nclass ModelSpecList(BaseModel):\n    \"\"\"\n    A list of model capabilities.\n    This is entirely for requests (to Perplexity).\n    \"\"\"\n\n    specs: list[ModelSpec]\n\n\nif __name__ == \"__main__\":\n    example = ModelSpec(\n        model=\"example-model\",\n        description=\"An example model for demonstration purposes.\",\n        provider=\"openai\",\n        temperature_range=[0.0, 2.0],\n        context_window=4096,\n        parameter_count=\"7b\",\n        knowledge_cutoff=\"2023-10-01\",\n        text_completion=True,\n        image_analysis=False,\n        image_gen=True,\n        audio_analysis=False,\n        audio_gen=True,\n        video_analysis=False,\n        video_gen=False,\n        reasoning=True,\n    )\n    example.card\n</file>\n    <file path=\"model/models/modelspecs_CRUD.py\">\"\"\"\nCRUD functions for TinyDB; handles serializing / deserializing our ModelCapabilities objects.\n\"\"\"\n\nfrom conduit.model.models.modelspec import ModelSpec\nfrom tinydb import TinyDB, Query\nfrom pathlib import Path\n\ndir_path = Path(__file__).parent\nmodelspecs_file = dir_path / \"modelspecs.json\"\ndb = TinyDB(modelspecs_file)\n\n\ndef create_modelspecs_from_scratch(modelspecs=list[ModelSpec]) -&gt; None:\n    \"\"\"\n    Create a new empty database for ModelSpecs.\n    This will overwrite any existing data in modelspecs.json.\n    \"\"\"\n    global db\n    db.close()  # Close the existing database if it exists\n    if modelspecs_file.exists():\n        modelspecs_file.unlink()  # Delete the existing file\n    db = TinyDB(modelspecs_file)  # Reopen the database\n    print(f\"Created new ModelSpecs database at {modelspecs_file}\")\n    # Now populate the database with any initial data if needed\n    if modelspecs:\n        for spec in modelspecs:\n            add_modelspec(spec)\n\n\ndef add_modelspec(model_spec: ModelSpec) -&gt; None:\n    \"\"\"\n    Add a ModelSpec to the database.\n    \"\"\"\n    db.insert(model_spec.model_dump())\n\n\ndef create_modelspec(model_spec: ModelSpec) -&gt; None:\n    t\n\n\ndef get_all_modelspecs() -&gt; list[ModelSpec]:\n    \"\"\"\n    Retrieve all ModelSpecs from the database.\n    \"\"\"\n    return [ModelSpec(**item) for item in db.all()]\n\n\ndef get_modelspec_by_name(model: str) -&gt; ModelSpec:\n    \"\"\"\n    Retrieve a ModelSpec by its name.\n    \"\"\"\n    ModelQuery = Query()\n    item = db.search(ModelQuery.model == model)\n    if item:\n        return ModelSpec(**item[0])\n    else:\n        raise ValueError(f\"ModelSpec with name '{model}' not found.\")\n\n\ndef get_all_model_names() -&gt; list[str]:\n    \"\"\"\n    Retrieve all model names from the database.\n    \"\"\"\n    ModelQuery = Query()\n    return [item[\"model\"] for item in db.all()]\n\n\ndef update_modelspec(model: str, updated_spec: ModelSpec) -&gt; None:\n    \"\"\"\n    Update a ModelSpec in the database.\n    \"\"\"\n    ModelQuery = Query()\n    db.update(updated_spec.model_dump(), ModelQuery.model == model)\n\n\ndef delete_modelspec(model: str) -&gt; None:\n    \"\"\"\n    Delete a ModelSpec from the database.\n    \"\"\"\n    ModelQuery = Query()\n    db.remove(ModelQuery.model == model)\n\n\ndef in_db(model_spec: ModelSpec) -&gt; bool:\n    \"\"\"\n    Check if a ModelSpec is already in the database.\n    \"\"\"\n    ModelQuery = Query()\n    item = db.search(ModelQuery.model == model_spec.model)\n    return bool(item)\n</file>\n    <file path=\"model/models/provider.py\">from typing import Literal\n\nProvider = Literal[\"openai\", \"ollama\", \"anthropic\", \"google\", \"perplexity\"]\n\n</file>\n    <file path=\"model/models/validate_capabilities.py\">\"\"\"\nA set of test scripts that will validate whether a ModelCapabilities is valid or not.\nFor example, if a model spec says image generation is possible, generate an image.\n\"\"\"\n\n\n# Test functions\ndef generate_text_completion(model):\n    \"\"\"Test if the model can generate text completions.\"\"\"\n    raise NotImplementedError(\"Text completion generation is not implemented yet.\")\n\n\ndef analyze_image(model):\n    \"\"\"Test if the model can analyze images.\"\"\"\n    raise NotImplementedError(\"Image analysis is not implemented yet.\")\n\n\ndef generate_image(model):\n    \"\"\"Test if the model can generate images.\"\"\"\n    raise NotImplementedError(\"Image generation is not implemented yet.\")\n\n\ndef analyze_audio(model):\n    \"\"\"Test if the model can analyze audio.\"\"\"\n    raise NotImplementedError(\"Audio analysis is not implemented yet.\")\n\n\ndef generate_audio(model):\n    \"\"\"Test if the model can generate audio.\"\"\"\n    raise NotImplementedError(\"Audio generation is not implemented yet.\")\n\n\ndef analyze_video(model):\n    \"\"\"Test if the model can analyze video.\"\"\"\n    raise NotImplementedError(\"Video analysis is not implemented yet.\")\n\n\ndef generate_video(model):\n    \"\"\"Test if the model can generate video.\"\"\"\n    raise NotImplementedError(\"Video generation is not implemented yet.\")\n</file>\n    <file path=\"model/models/__init__.py\"/>\n    <file path=\"model/models/providerspec.py\">from pydantic import BaseModel, Field\nfrom typing import Literal\nfrom pathlib import Path\n\ndir_path = Path(__file__).parent\nproviders_file = dir_path / \"providers.jsonl\"\n\nclass ProviderSpec(BaseModel):\n    \"\"\"\n    A specification for a provider's capabilities.\n    This is entirely for requests (to Perplexity).\n    \"\"\"\n    provider: str = Field(..., description=\"The name of the provider\")\n    temperature_range: list[float] | Literal[\"Not supported\"] = Field(\n        ..., description=\"The range of temperature settings available for the provider\"\n    )\n\ndef populate_providers_file():\n    \"\"\"\n    Populate the providers.jsonl file with provider specifications.\n    \"\"\"\n    if providers_file.exists():\n        providers_file.unlink()  # Remove existing file if it exists\n\n    providers_file.touch()  # Create a new empty file\n\n    # Define provider specifications\n    openai = ProviderSpec(\n        provider=\"openai\",\n        temperature_range=[0.0, 2.0]\n    )\n\n    anthropic = ProviderSpec(\n        provider=\"anthropic\",\n        temperature_range=[0.0, 1.0]\n        )\n\n    google = ProviderSpec(\n        provider=\"google\",\n        temperature_range=[0.0, 1.0]\n    )\n\n    groq = ProviderSpec(\n        provider=\"groq\",\n        temperature_range=[0.0, 1.0]\n    )\n\n    perplexity = ProviderSpec(\n        provider=\"perplexity\",\n        temperature_range=\"Not supported\"\n    )\n\n    ollama = ProviderSpec(\n        provider=\"ollama\",\n        temperature_range=[0.0, 1.0]\n    )\n\n    providers = [openai, anthropic, google, groq, perplexity, ollama]\n\n    providers_file.write_text(\n        \"\\n\".join(provider.model_dump_json() for provider in providers)\n    )\n    print(f\"Populated {providers_file} with provider specifications.\")\n\nif __name__ == \"__main__\":\n    populate_providers_file()\n</file>\n    <file path=\"model/models/modelstore.py\">\"\"\"\nOur list of API llm models is in models.json, in our repo, as is the list of aliases.\nThe list of ollama models is host-specific, and therefore in our state directory.\nOllama context sizes is user-configurable, and therefore in our config directory.\n\"\"\"\n\nfrom conduit.model.models.providerstore import ProviderStore\nfrom conduit.model.models.provider import Provider\nfrom conduit.model.models.modelspec import ModelSpec\nfrom conduit.model.models.modelspecs_CRUD import (\n    get_modelspec_by_name,\n    get_all_modelspecs,\n    delete_modelspec,\n    get_all_model_names,\n)\nfrom conduit.model.models.research_models import create_modelspec\nfrom conduit.logs.logging_config import get_logger\nfrom xdg_base_dirs import xdg_state_home, xdg_config_home\nfrom pathlib import Path\nimport json, itertools\n\nlogger = get_logger(__name__)\n\n# Our data stores\nDIR_PATH = Path(__file__).parent\nMODELS_PATH = DIR_PATH / \"models.json\"\nOLLAMA_MODELS_PATH = xdg_state_home() / \"conduit\" / \"ollama_models.json\"\nOLLAMA_CONTEXT_SIZES_PATH = xdg_config_home() / \"conduit\" / \"ollama_context_sizes.json\"\nALIASES_PATH = DIR_PATH / \"aliases.json\"\n\n\nclass ModelStore:\n    \"\"\"\n    Class to manage model information for the Conduit library.\n    Provides methods to retrieve supported models, aliases, and validate model names.\n    \"\"\"\n\n    @classmethod\n    def models(cls):\n        \"\"\"\n        Definitive list of models supported by Conduit library, as well as the local list of ollama models.\n        \"\"\"\n        with open(MODELS_PATH) as f:\n            models_json = json.load(f)\n        if OLLAMA_MODELS_PATH.exists():\n            with open(OLLAMA_MODELS_PATH) as f:\n                ollama_models = json.load(f)\n            models_json[\"ollama\"] = ollama_models[\"ollama\"]\n        return models_json\n\n    @classmethod\n    def list_models(cls) -&gt; list[str]:\n        \"\"\"List of all models supported by Conduit library.\"\"\"\n        models = cls.models()\n        return list(itertools.chain.from_iterable(models.values()))\n\n    @classmethod\n    def list_model_types(cls) -&gt; list[str]:\n        \"\"\"List of model types supported by Conduit library.\"\"\"\n        return [\n            \"image_analysis\",\n            \"image_gen\",\n            \"audio_analysis\",\n            \"audio_gen\",\n            \"video_analysis\",\n            \"video_gen\",\n            \"reasoning\",\n            \"text_completion\",\n        ]\n\n    @classmethod\n    def list_providers(cls) -&gt; list[str]:\n        \"\"\"Definitive list of providers supported by Conduit library.\"\"\"\n        providers = ProviderStore.get_all_providers()\n        return [provider.provider for provider in providers]\n\n    @classmethod\n    def identify_provider(cls, model: str) -&gt; Provider:\n        \"\"\"\n        Identify the provider for a given model.\n        Returns the Provider object if found, raises ValueError otherwise.\n        \"\"\"\n        models = cls.models()\n        for provider, model_list in models.items():\n            if model in model_list:\n                return provider\n        raise ValueError(f\"Provider not found for model: {model}\")\n\n    @classmethod\n    def aliases(cls):\n        \"\"\"Definitive list of model aliases supported by Conduit library.\"\"\"\n        with open(ALIASES_PATH) as f:\n            return json.load(f)\n\n    @classmethod\n    def is_supported(cls, model: str) -&gt; bool:\n        \"\"\"\n        Check if the model is supported by the Conduit library.\n        Returns True if the model is supported, False otherwise.\n        \"\"\"\n        in_aliases = model in cls.aliases().keys()\n        in_models = model in list(itertools.chain.from_iterable(cls.models().values()))\n        return in_aliases or in_models\n\n    @classmethod\n    def _validate_model(cls, model: str) -&gt; str:\n        \"\"\"\n        Validate the model name against the supported models and aliases.\n        Converts aliases to their corresponding model names if necessary.\n        \"\"\"\n        # Load aliases\n        aliases = cls.aliases()\n        # Assign models based on aliases\n        if model in cls.aliases().keys():\n            model = aliases[model]\n        elif cls.is_supported(model):\n            model = model\n        else:\n            ValueError(\n                f\"WARNING: Model not found locally: {model}. This may cause errors.\"\n            )\n        return model\n\n    @classmethod\n    def get_num_ctx(cls, ollama_model: str) -&gt; int:\n        \"\"\"\n        Get the preferred num_ctx for a given ollama model.\n        \"\"\"\n        default_value = 32768\n        if OLLAMA_CONTEXT_SIZES_PATH.exists():\n            with open(OLLAMA_CONTEXT_SIZES_PATH) as f:\n                context_sizes = json.load(f)\n            if ollama_model in context_sizes:\n                return context_sizes[ollama_model]\n            else:\n                logger.warning(\n                    f\"Model {ollama_model} not found in context sizes file. Using default value of {default_value}.\"\n                )\n                return default_value  # Default context size if not specified -- may throw an error for smaller models\n        else:\n            raise FileNotFoundError(\n                f\"Context sizes file not found: {ollama_context_sizes_path}\"\n            )\n\n    @classmethod\n    def display(cls):\n        from rich.console import Console\n        from rich.columns import Columns\n        from rich.text import Text\n\n        console = Console()\n        models = cls.models()\n\n        # Calculate total items and split points\n        all_items = []\n        for provider, model_list in models.items():\n            all_items.append((provider, None))  # Provider header\n            for model in model_list:\n                all_items.append((provider, model))\n\n        # Split into three roughly equal columns\n        total_items = len(all_items)\n        col1_end = total_items // 3\n        col2_end = 2 * total_items // 3\n\n        # Create three columns\n        left_column = Text()\n        middle_column = Text()\n        right_column = Text()\n\n        for i, (provider, model) in enumerate(all_items):\n            if i &lt; col1_end:\n                target_column = left_column\n            elif i &lt; col2_end:\n                target_column = middle_column\n            else:\n                target_column = right_column\n\n            if model is None:  # Provider header\n                target_column.append(f\"{provider.upper()}\\n\", style=\"bold cyan\")\n            else:  # Model name\n                target_column.append(f\"  {model}\\n\", style=\"white\")\n\n        console.print(\n            Columns([left_column, middle_column, right_column], equal=True, expand=True)\n        )\n\n    # Consistency\n    @classmethod\n    def update(cls):\n        \"\"\"\n        Compare model list from models.json, and make sure there are corresponding ModelSpec objects in database.\n        Delete objects that don't have their model name in models.json; and create new ModelSpec objects if they are not in db.\n        \"\"\"\n        if not cls._is_consistent():\n            print(\n                \"Model specifications are not consistent with models.json. Updating...\"\n            )\n            logger.info(\n                \"Model specifications are not consistent with models.json. Updating...\"\n            )\n            cls._update_models()\n        else:\n            print(\n                \"Model specifications are consistent with models.json. No update needed.\"\n            )\n            logger.info(\n                \"Model specifications are consistent with models.json. No update needed.\"\n            )\n\n    @classmethod\n    def _update_models(cls):\n        # Get all ModelSpec objects from the database\n        modelspec_db_names = set(get_all_model_names())\n        # Create a set of model names from the models.json file\n        models = cls.models()\n        models_json_names = set(itertools.chain.from_iterable(models.values()))\n        # Find models that are in models.json but not in the database\n        models_not_in_modelspec_db = models_json_names - modelspec_db_names\n        # Find models that are in the database but not in models.json\n        models_not_in_model_list = modelspec_db_names - models_json_names\n        logger.info(\n            f\"Found {len(models_not_in_modelspec_db)} models not in the database.\"\n        )\n        logger.info(f\"Found {len(models_not_in_model_list)} models not in models.json.\")\n        # Delete all ModelSpec objects that are not in models.json\n        logger.info(\n            f\"Deleting {len(models_not_in_model_list)} models not in models.json.\"\n        )\n        [delete_modelspec(model) for model in models_not_in_model_list]\n        # Create all Modelspec objects that are in models.json but not in the database\n        logger.info(\n            f\"Creating {len(models_not_in_modelspec_db)} models not in the database.\"\n        )\n        [create_modelspec(model) for model in models_not_in_modelspec_db]\n        if cls._is_consistent():\n            logger.info(\n                \"Model specifications are now consistent with models.json. Update complete.\"\n            )\n            return\n        else:\n            raise ValueError(\n                \"Model specifications are not consistent with models.json, after running .update().\"\n            )\n\n    @classmethod\n    def _is_consistent(cls) -&gt; bool:\n        \"\"\"\n        Check if the model specifications in the database are consistent with the models.json file.\n        Returns True if consistent, False otherwise.\n        \"\"\"\n        # Get list of models from models.json\n        models = cls.models()\n\n        # Get all ModelSpec objects from the database\n        model_specs = get_all_modelspecs()\n\n        # Create a set of model names from the models.json file\n        model_names = set(itertools.chain.from_iterable(models.values()))\n\n        consistent = True\n        # Check if all ModelSpec names are in the models.json file\n        for model_spec in model_specs:\n            if model_spec.model not in model_names:\n                consistent = False\n        # Check if all models in models.json have a corresponding ModelSpec object\n        for _, model_list in models.items():\n            for model in model_list:\n                if not any(model_spec.model == model for model_spec in model_specs):\n                    consistent = False\n\n        return consistent\n\n    # Getters\n    @classmethod\n    def get_model(cls, model: str) -&gt; ModelSpec:\n        \"\"\"\n        Get the model name, validating against aliases and supported models.\n        \"\"\"\n        model = cls._validate_model(model)\n        try:\n            return get_modelspec_by_name(model)\n        except ValueError:\n            raise ValueError(f\"Model '{model}' not found in the database.\")\n\n    @classmethod\n    def get_all_models(cls) -&gt; list[ModelSpec]:\n        \"\"\"\n        Get all models as ModelSpec objects.\n        \"\"\"\n        return get_all_modelspecs()\n\n    ## Get subsets of models by provider\n    @classmethod\n    def by_provider(cls, provider: Provider) -&gt; list[ModelSpec]:\n        \"\"\"\n        Get a list of models for a specific provider.\n        \"\"\"\n        return [\n            modelspec\n            for modelspec in cls.get_all_models()\n            if modelspec.provider == provider\n        ]\n\n    ## Get subsets of models by type\n    @classmethod\n    def by_type(cls, model_type: str) -&gt; list[ModelSpec]:\n        \"\"\"\n        Get a list of models by type.\n        Raises ValueError if the model type is not valid.\n        \"\"\"\n        if model_type not in cls.list_model_types():\n            raise ValueError(\n                f\"Invalid model type: {model_type}. Must be one of: {', '.join(cls.list_model_types())}.\"\n            )\n        match model_type:\n            case \"image_analysis\":\n                return cls.image_analysis_models()\n            case \"image_gen\":\n                return cls.image_gen_models()\n            case \"audio_analysis\":\n                return cls.audio_analysis_models()\n            case \"audio_gen\":\n                return cls.audio_gen_models()\n            case \"video_analysis\":\n                return cls.video_analysis_models()\n            case \"video_gen\":\n                return cls.video_gen_models()\n            case \"reasoning\":\n                return cls.reasoning_models()\n            case \"text_completion\":\n                return cls.text_completion_models()\n            case _:\n                raise ValueError(\n                    f\"Invalid model type: {model_type}. Must be one of: {', '.join(cls.list_model_types())}.\"\n                )\n\n    ## Get lists of models by capability\n    @classmethod\n    def image_analysis_models(cls) -&gt; list[ModelSpec]:\n        \"\"\"\n        Get a list of models that support image analysis.\n        \"\"\"\n        return [\n            modelspec for modelspec in cls.get_all_models() if modelspec.image_analysis\n        ]\n\n    @classmethod\n    def image_gen_models(cls) -&gt; list[ModelSpec]:\n        \"\"\"\n        Get a list of models that support image generation.\n        \"\"\"\n        return [modelspec for modelspec in cls.get_all_models() if modelspec.image_gen]\n\n    @classmethod\n    def audio_analysis_models(cls) -&gt; list[ModelSpec]:\n        \"\"\"\n        Get a list of models that support audio analysis.\n        \"\"\"\n        return [\n            modelspec for modelspec in cls.get_all_models() if modelspec.audio_analysis\n        ]\n\n    @classmethod\n    def audio_gen_models(cls) -&gt; list[ModelSpec]:\n        \"\"\"\n        Get a list of models that support audio generation.\n        \"\"\"\n        return [modelspec for modelspec in cls.get_all_models() if modelspec.audio_gen]\n\n    @classmethod\n    def video_analysis_models(cls) -&gt; list[ModelSpec]:\n        \"\"\"\n        Get a list of models that support video analysis.\n        \"\"\"\n        return [\n            modelspec for modelspec in cls.get_all_models() if modelspec.video_analysis\n        ]\n\n    @classmethod\n    def video_gen_models(cls) -&gt; list[ModelSpec]:\n        \"\"\"\n        Get a list of models that support video generation.\n        \"\"\"\n        return [modelspec for modelspec in cls.get_all_models() if modelspec.video_gen]\n\n    @classmethod\n    def reasoning_models(cls) -&gt; list[ModelSpec]:\n        \"\"\"\n        Get a list of models that support reasoning.\n        \"\"\"\n        return [modelspec for modelspec in cls.get_all_models() if modelspec.reasoning]\n\n    @classmethod\n    def text_completion_models(cls) -&gt; list[ModelSpec]:\n        \"\"\"\n        Get a list of models that support text completion.\n        \"\"\"\n        return [\n            modelspec for modelspec in cls.get_all_models() if modelspec.text_completion\n        ]\n</file>\n    <file path=\"odometer/TokenEvent.py\">from pydantic import BaseModel, Field\nfrom conduit.model.models.provider import Provider\n\n\nclass TokenEvent(BaseModel):\n    provider: Provider = Field(\n        ...,\n        description=\"Model provider (Anthropic, OpenAI, Google, Ollama, Perplexity, etc.).\",\n    )\n    model: str = Field(\n        ..., description=\"Specific model by name (gpt-4o, claude-3.5-sonnet, etc.).\"\n    )\n    input_tokens: int = Field(\n        ..., description=\"Prompt tokens as defined and provided in API response.\"\n    )\n    output_tokens: int = Field(\n        ..., description=\"Output tokens as defined and provided in API response.\"\n    )\n    timestamp: int = Field(\n        ..., description=\"Unix epoch timestamp.\"\n    )  # Unix epoch timestamp\n    host: str = Field(\n        ..., description=\"Simple host detection for multi-machine tracking.\"\n    )\n</file>\n    <file path=\"odometer/SessionOdometer.py\">from conduit.odometer.Odometer import Odometer\n\n\nclass SessionOdometer(Odometer):\n    \"\"\"\n    Attaches to Model as a singleton (._session_odomoter).\n    Always loads by default.\n    \"\"\"\n</file>\n    <file path=\"odometer/ConversationOdometer.py\">from conduit.odometer.Odometer import Odometer\nfrom pydantic import Field\n\n\nclass ConversationOdometer(Odometer):\n    \"\"\"\n    Attaches to ModelStore on instance level. (self.conversation_odometer)\n    Routed to by TokenEvent.host.\n    \"\"\"\n\n    conversation_id: str = Field(\n        ..., description=\"Unique identifier for the conversation.\"\n    )\n    # model_name: str = Field(\n    #     ...,\n    #     description=\"Name of the model used in this conversation. Assumes one model for entire MessageStore, since how else can you track context window?\",\n    # )\n    # provider: str = Field(\n    #     ...,\n    #     description=\"Provider of the model used in this conversation (OpenAI, Anthropic, etc.).\",\n    # )\n    # context_window: int = Field(\n    #     ..., description=\"Context window size of the model used in this conversation.\"\n    # )\n    # start_time: datetime = Field(\n    #     default_factory=datetime.now, description=\"Start time of the conversation.\"\n    # )\n    #\n    # # Methods:\n    # def context_utilization(self) -&gt; float:\n    #     # Calculate percentage of context window used\n    #     pass\n    #\n    # def should_warn_context_limit(self) -&gt; bool:\n    #     # Check if approaching context limits\n    #     pass\n</file>\n    <file path=\"odometer/PersistentOdometer.py\">from conduit.odometer.Odometer import Odometer\nfrom conduit.odometer.SessionOdometer import SessionOdometer\nfrom conduit.odometer.database.PersistenceBackend import PersistenceBackend\nfrom datetime import date, timedelta\nfrom pydantic import Field\n\n\nclass PersistentOdometer(Odometer):\n    \"\"\"\n    Handles long-term storage of odometer data.\n    \"\"\"\n\n    model_config = {\"arbitrary_types_allowed\": True}\n\n    # Add this field declaration\n    backend: PersistenceBackend | None = Field(default=None, exclude=True)\n\n    def __init__(self, backend: PersistenceBackend | None = None, **kwargs):\n        super().__init__(**kwargs)\n        if backend is None:\n            # Your existing backend creation logic\n            try:\n                from conduit.odometer.database.pgres.PostgresBackend import (\n                    PostgresBackend,\n                )\n\n                self.backend = PostgresBackend()\n            except Exception as e:\n                print(f\"Real error: {e}\")\n                raise\n                # from conduit.odometer.database.sqlite.SqliteBackend import SQLiteBackend\n                #\n                # self.backend = SQLiteBackend(\"odometer.db\")\n        else:\n            self.backend = backend\n\n    def sync_session_data(self, session_odometer: SessionOdometer):\n        \"\"\"\n        Syncs the session odometer data to the persistent storage.\n        \"\"\"\n        if session_odometer.events:\n            self.backend.store_events(session_odometer.events)\n            # Update local aggregates\n            for event in session_odometer.events:\n                self.record(event)\n\n    def load_historical_data(self, days: int = 30):\n        \"\"\"Load recent historical data for analytics\"\"\"\n        start_date = date.today() - timedelta(days=days)\n        events = self.backend.get_events(start_date=start_date)\n\n        # Rebuild aggregates from events\n        self.clear_aggregates()\n        for event in events:\n            self.record(event)\n</file>\n    <file path=\"odometer/__init__.py\"/>\n    <file path=\"odometer/OdometerRegistry.py\">from conduit.odometer.SessionOdometer import SessionOdometer\nfrom conduit.odometer.ConversationOdometer import ConversationOdometer\nfrom conduit.odometer.PersistentOdometer import PersistentOdometer\nfrom conduit.odometer.TokenEvent import TokenEvent\nimport atexit, signal, sys\n\n\nclass OdometerRegistry:\n    \"\"\"\n    A registry for odometers that allows for adding, removing, and retrieving odometers.\n    Singleton which attaches to Model class as a class variable (._odometer_registry).\n\n    Four purposes for this class:\n    1. Centralized management of odometers for session, conversation, and persistent data.\n    2. Handles the registration of new conversation odometers.\n    3. Distributes TokenEvents to the appropriate odometers.\n    4. Ensures that data is saved on program exit or interruption.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the odometer registry with an empty dictionary.\n        \"\"\"\n        self.session_odometer = SessionOdometer()\n        self.conversation_odometers: dict[str, ConversationOdometer] = {}\n        self.persistent_odometer = PersistentOdometer()\n        # Register the persistent odometer to save on exit\n        atexit.register(self._save_on_exit)\n        # Register signal handlers for interrupts\n        signal.signal(signal.SIGINT, self._signal_handler)  # Ctrl+C\n        signal.signal(signal.SIGTERM, self._signal_handler)  # Termination\n\n    def register_conversation_odometer(self, conversation_id: str):\n        \"\"\"\n        Register a new ConversationOdometer for the given conversation ID.\n        If an odometer already exists for the conversation, it will not be replaced.\n        \"\"\"\n        if conversation_id not in self.conversation_odometers:\n            self.conversation_odometers[conversation_id] = ConversationOdometer(\n                conversation_id=conversation_id\n            )\n\n    def emit_token_event(self, event: TokenEvent):\n        \"\"\"\n        Main entry point for the TokenEvents sent by Response.__init__().\n        Distributes the event to the appropriate odometers.\n        \"\"\"\n        self.session_odometer.record(event)\n        self.persistent_odometer.record(event)\n        if event.host in self.conversation_odometers:\n            self.conversation_odometers[event.host].record(event)\n        else:\n            # If no conversation odometer exists for the host, create one\n            self.register_conversation_odometer(event.host)\n            self.conversation_odometers[event.host].record(event)\n\n    def _signal_handler(self, signum, frame):\n        \"\"\"\n        Handle interrupt signals\n        \"\"\"\n        self._save_on_exit()\n        sys.exit(0)\n\n    def _save_on_exit(self):\n        \"\"\"\n        Called on normal program exit.\n        \"\"\"\n        try:\n            self.persistent_odometer.sync_session_data(self.session_odometer)\n            # Also sync any active conversation odometers\n            for conv_odo in self.conversation_odometers.values():\n                self.persistent_odometer.sync_session_data(self.session_odometer)\n        except Exception as e:\n            # Log error but don't crash on exit\n            print(f\"Warning: Failed to save odometer data on exit: {e}\")\n</file>\n    <file path=\"odometer/Odometer.py\">\"\"\"\nsession odometer: in memory sqlite3 (conn = sqlite3.connect(\":memory:\")\nconversation odomoter: saved in a sqlite3 file (similar naming convention to history store)\npersistent odometer: saved in postgres\n\"\"\"\n\nfrom conduit.odometer.TokenEvent import TokenEvent\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\n\n\nclass Odometer(BaseModel):\n    # Raw event storage\n    events: list[TokenEvent] = Field(default_factory=list)\n\n    # Aggregated totals\n    total_input_tokens: int = Field(default=0)\n    total_output_tokens: int = Field(default=0)\n    total_tokens: int = Field(default=0)  # computed property or field\n\n    # Provider-level aggregation\n    provider_totals: dict[str, dict[str, int]] = Field(default_factory=dict)\n    # Structure: {\"openai\": {\"input\": 1000, \"output\": 500, \"total\": 1500}}\n\n    # Model-level aggregation\n    model_totals: dict[str, dict[str, int]] = Field(default_factory=dict)\n    # Structure: {\"gpt-4o\": {\"input\": 800, \"output\": 300, \"total\": 1100}}\n\n    # Time-based aggregation\n    daily_totals: dict[str, dict[str, int]] = Field(default_factory=dict)\n    # Structure: {\"2025-01-15\": {\"input\": 500, \"output\": 200, \"total\": 700}}\n\n    # Session metadata\n    session_start: datetime = Field(default_factory=datetime.now)\n    last_updated: datetime = Field(default_factory=datetime.now)\n\n    # Host tracking (for multi-machine scenarios)\n    hosts: set[str] = Field(default_factory=set)\n\n    def record(self, token_event: TokenEvent):\n        \"\"\"\n        Add a TokenEvent to the odometer and update all aggregates.\n        provider: str\n        model: str\n        input_tokens: int\n        output_tokens: int\n        timestamp: int\n        host: str\n        \"\"\"\n        self.events.append(token_event)\n        self.total_input_tokens += token_event.input_tokens\n        self.total_output_tokens += token_event.output_tokens\n        self.total_tokens += token_event.input_tokens + token_event.output_tokens\n\n        # Update provider totals\n        if token_event.provider not in self.provider_totals:\n            self.provider_totals[token_event.provider] = {\n                \"input\": 0,\n                \"output\": 0,\n                \"total\": 0,\n            }\n        self.provider_totals[token_event.provider][\"input\"] += token_event.input_tokens\n        self.provider_totals[token_event.provider][\"output\"] += (\n            token_event.output_tokens\n        )\n        self.provider_totals[token_event.provider][\"total\"] += (\n            token_event.input_tokens + token_event.output_tokens\n        )\n\n        # Update model totals\n        if token_event.model not in self.model_totals:\n            self.model_totals[token_event.model] = {\"input\": 0, \"output\": 0, \"total\": 0}\n        self.model_totals[token_event.model][\"input\"] += token_event.input_tokens\n        self.model_totals[token_event.model][\"output\"] += token_event.output_tokens\n        self.model_totals[token_event.model][\"total\"] += (\n            token_event.input_tokens + token_event.output_tokens\n        )\n\n        # Update daily totals\n        date_str = datetime.fromtimestamp(token_event.timestamp).strftime(\"%Y-%m-%d\")\n        if date_str not in self.daily_totals:\n            self.daily_totals[date_str] = {\"input\": 0, \"output\": 0, \"total\": 0}\n        self.daily_totals[date_str][\"input\"] += token_event.input_tokens\n        self.daily_totals[date_str][\"output\"] += token_event.output_tokens\n        self.daily_totals[date_str][\"total\"] += (\n            token_event.input_tokens + token_event.output_tokens\n        )\n\n        # Update hosts set\n        self.hosts.add(token_event.host)\n\n    # Query methods\n    def get_provider_breakdown(self) -&gt; dict[str, int]:\n        # Return provider totals\n        pass\n\n    def get_model_breakdown(self) -&gt; dict[str, int]:\n        # Return model totals\n        pass\n\n    def get_daily_usage(self, date: str) -&gt; dict[str, int]:\n        # Return usage for specific day\n        pass\n\n    def get_recent_activity(self, hours: int = 24) -&gt; list[TokenEvent]:\n        # Filter events by timestamp\n        pass\n\n    def stats(self):\n        \"\"\"\n        Pretty-print the stats from the odometer.\n        \"\"\"\n        from rich.console import Console\n        from rich.table import Table\n        from rich.text import Text\n\n        console = Console()\n        table = Table(title=\"Odometer Stats\")\n        table.add_column(\"Category\", justify=\"left\", style=\"cyan\")\n        table.add_column(\"Input Tokens\", justify=\"right\", style=\"green\")\n        table.add_column(\"Output Tokens\", justify=\"right\", style=\"yellow\")\n        table.add_column(\"Total Tokens\", justify=\"right\", style=\"magenta\")\n        table.add_row(\n            \"Total\",\n            str(self.total_input_tokens),\n            str(self.total_output_tokens),\n            str(self.total_tokens),\n        )\n        for provider, totals in self.provider_totals.items():\n            table.add_row(\n                Text(provider, style=\"blue\"),\n                str(totals[\"input\"]),\n                str(totals[\"output\"]),\n                str(totals[\"total\"]),\n            )\n        for model, totals in self.model_totals.items():\n            table.add_row(\n                Text(model, style=\"blue\"),\n                str(totals[\"input\"]),\n                str(totals[\"output\"]),\n                str(totals[\"total\"]),\n            )\n        for date, totals in self.daily_totals.items():\n            table.add_row(\n                Text(date, style=\"blue\"),\n                str(totals[\"input\"]),\n                str(totals[\"output\"]),\n                str(totals[\"total\"]),\n            )\n        console.print(table)\n</file>\n    <file path=\"odometer/database/PersistenceBackend.py\">from conduit.odometer.TokenEvent import TokenEvent\nfrom abc import ABC, abstractmethod\nfrom datetime import date\n\n\nclass PersistenceBackend(ABC):\n    \"\"\"Abstract interface for odometer persistence\"\"\"\n\n    @abstractmethod\n    def store_events(self, events: list[TokenEvent]) -&gt; None:\n        \"\"\"Store raw token events\"\"\"\n        pass\n\n    @abstractmethod\n    def get_events(\n        self,\n        start_date: date | None = None,\n        end_date: date | None = None,\n        provider: str | None = None,\n        model: str | None = None,\n        host: str | None = None,\n    ) -&gt; list[TokenEvent]:\n        \"\"\"Query token events with filters\"\"\"\n        pass\n\n    @abstractmethod\n    def get_aggregates(\n        self,\n        group_by: str,  # \"provider\", \"model\", \"host\", \"date\"\n        start_date: date | None = None,\n        end_date: date | None = None,\n    ) -&gt; dict[str, dict[str, int]]:\n        \"\"\"Get aggregated statistics\"\"\"\n        pass\n</file>\n    <file path=\"odometer/database/pgres/wipe_database.py\">from dbclients import get_postgres_client\nfrom conduit.logs.logging_config import get_logger\n\nlogger = get_logger(__name__)\n\nget_db_connection = get_postgres_client(\"context_db\", dbname=\"chain\")\n\n\ndef wipe_token_events_database(confirm: bool = False) -&gt; bool:\n    \"\"\"\n    Delete all token events from the database.\n\n    Args:\n        confirm: Must be True to actually perform the deletion (safety check)\n\n    Returns:\n        bool: True if successful, False otherwise\n    \"\"\"\n    if not confirm:\n        logger.warning(\n            \"wipe_token_events_database called without confirm=True. No action taken.\"\n        )\n        return False\n\n    try:\n        with get_db_connection() as conn:\n            with conn.cursor() as cursor:\n                # Get count before deletion for logging\n                cursor.execute(\"SELECT COUNT(*) FROM token_events\")\n                count_before = cursor.fetchone()[0]\n\n                # Delete all records\n                cursor.execute(\"DELETE FROM token_events\")\n                deleted_count = cursor.rowcount\n\n                # Reset the sequence for the ID column\n                cursor.execute(\"ALTER SEQUENCE token_events_id_seq RESTART WITH 1\")\n\n                conn.commit()\n                logger.warning(\n                    f\"Database wiped: deleted {deleted_count} token events (was {count_before} total)\"\n                )\n                return True\n\n    except Exception as e:\n        logger.error(f\"Failed to wipe database: {e}\")\n        return False\n\n\ndef main():\n    \"\"\"\n    Main function to execute the wipe operation.\n    \"\"\"\n    confirm = (\n        input(\"Are you sure you want to wipe the token events database? (yes/no): \")\n        .strip()\n        .lower()\n    )\n    if confirm == \"yes\":\n        success = wipe_token_events_database(confirm=True)\n        if success:\n            print(\"Database wiped successfully.\")\n        else:\n            print(\"Failed to wipe the database.\")\n    else:\n        print(\"Operation cancelled. No changes made.\")\n\n\nif __name__ == \"__main__\":\n    main()\n</file>\n    <file path=\"odometer/database/pgres/PostgresBackend.py\">from conduit.odometer.database.PersistenceBackend import PersistenceBackend\nfrom conduit.odometer.TokenEvent import TokenEvent\nfrom dbclients import get_postgres_client\nfrom datetime import date, datetime\nfrom typing import override\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nget_db_connection = get_postgres_client(\"context_db\", dbname=\"chain\")\n\n\nclass PostgresBackend(PersistenceBackend):\n    def __init__(self, connection_string: str | None = None):\n        self.conn_string: str | None = connection_string\n        self._initialized: bool = False\n        self._ensure_schema()\n\n    def _ensure_schema(self):\n        \"\"\"Create tables if they don't exist\"\"\"\n        if self._initialized:\n            return\n\n        create_table_sql = \"\"\"\n        CREATE TABLE IF NOT EXISTS token_events (\n            id SERIAL PRIMARY KEY,\n            provider VARCHAR(50) NOT NULL,\n            model VARCHAR(200) NOT NULL,\n            input_tokens INTEGER NOT NULL,\n            output_tokens INTEGER NOT NULL,\n            timestamp BIGINT NOT NULL,\n            host VARCHAR(100) NOT NULL,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        );\n        \n        -- Create indexes if they don't exist\n        CREATE INDEX IF NOT EXISTS idx_token_events_timestamp ON token_events(timestamp);\n        CREATE INDEX IF NOT EXISTS idx_token_events_provider ON token_events(provider);\n        CREATE INDEX IF NOT EXISTS idx_token_events_model ON token_events(model);\n        CREATE INDEX IF NOT EXISTS idx_token_events_host ON token_events(host);\n        CREATE INDEX IF NOT EXISTS idx_token_events_provider_model ON token_events(provider, model);\n        \"\"\"\n\n        try:\n            with get_db_connection() as conn:\n                with conn.cursor() as cursor:\n                    cursor.execute(create_table_sql)\n                    conn.commit()\n                    logger.info(\"Database schema initialized successfully\")\n                    self._initialized = True\n        except Exception as e:\n            logger.error(f\"Failed to initialize database schema: {e}\")\n            raise ValueError(f\"Database initialization failed: {e}\")\n\n    @override\n    def store_events(self, events: list[TokenEvent]) -&gt; None:\n        \"\"\"Store raw token events using bulk insert\"\"\"\n        if not events:\n            return\n\n        insert_sql = \"\"\"\n        INSERT INTO token_events (provider, model, input_tokens, output_tokens, timestamp, host)\n        VALUES (%s, %s, %s, %s, %s, %s)\n        \"\"\"\n\n        try:\n            with get_db_connection() as conn:\n                with conn.cursor() as cursor:\n                    # Prepare data for bulk insert\n                    event_data = [\n                        (\n                            event.provider,\n                            event.model,\n                            event.input_tokens,\n                            event.output_tokens,\n                            event.timestamp,\n                            event.host,\n                        )\n                        for event in events\n                    ]\n\n                    # Execute bulk insert\n                    cursor.executemany(insert_sql, event_data)\n                    conn.commit()\n                    logger.info(f\"Stored {len(events)} token events\")\n\n        except Exception as e:\n            logger.error(f\"Failed to store events: {e}\")\n            raise\n\n    @override\n    def get_events(\n        self,\n        start_date: date | None = None,\n        end_date: date | None = None,\n        provider: str | None = None,\n        model: str | None = None,\n        host: str | None = None,\n    ) -&gt; list[TokenEvent]:\n        \"\"\"Query token events with filters\"\"\"\n\n        # Build dynamic query\n        where_conditions = []\n        params = []\n\n        if start_date:\n            start_timestamp = int(\n                datetime.combine(start_date, datetime.min.time()).timestamp()\n            )\n            where_conditions.append(\"timestamp &gt;= %s\")\n            params.append(start_timestamp)\n\n        if end_date:\n            end_timestamp = int(\n                datetime.combine(end_date, datetime.max.time()).timestamp()\n            )\n            where_conditions.append(\"timestamp &lt;= %s\")\n            params.append(end_timestamp)\n\n        if provider:\n            where_conditions.append(\"provider = %s\")\n            params.append(provider)\n\n        if model:\n            where_conditions.append(\"model = %s\")\n            params.append(model)\n\n        if host:\n            where_conditions.append(\"host = %s\")\n            params.append(host)\n\n        # Build the query\n        base_query = \"SELECT provider, model, input_tokens, output_tokens, timestamp, host FROM token_events\"\n        if where_conditions:\n            query = f\"{base_query} WHERE {' AND '.join(where_conditions)}\"\n        else:\n            query = base_query\n\n        query += \" ORDER BY timestamp DESC\"\n\n        try:\n            with get_db_connection() as conn:\n                with conn.cursor() as cursor:\n                    cursor.execute(query, params)\n                    rows = cursor.fetchall()\n\n                    # Convert rows to TokenEvent objects\n                    events = []\n                    for row in rows:\n                        events.append(\n                            TokenEvent(\n                                provider=row[0],\n                                model=row[1],\n                                input_tokens=row[2],\n                                output_tokens=row[3],\n                                timestamp=row[4],\n                                host=row[5],\n                            )\n                        )\n\n                    logger.info(f\"Retrieved {len(events)} token events\")\n                    return events\n\n        except Exception as e:\n            logger.error(f\"Failed to retrieve events: {e}\")\n            raise\n\n    @override\n    def get_aggregates(\n        self,\n        group_by: str,  # \"provider\", \"model\", \"host\", \"date\"\n        start_date: date | None = None,\n        end_date: date | None = None,\n    ) -&gt; dict[str, dict[str, int]]:\n        \"\"\"Get aggregated statistics\"\"\"\n\n        # Validate group_by parameter\n        valid_groups = [\"provider\", \"model\", \"host\", \"date\"]\n        if group_by not in valid_groups:\n            raise ValueError(f\"group_by must be one of {valid_groups}\")\n\n        # Build date conditions\n        where_conditions = []\n        params = []\n\n        if start_date:\n            start_timestamp = int(\n                datetime.combine(start_date, datetime.min.time()).timestamp()\n            )\n            where_conditions.append(\"timestamp &gt;= %s\")\n            params.append(start_timestamp)\n\n        if end_date:\n            end_timestamp = int(\n                datetime.combine(end_date, datetime.max.time()).timestamp()\n            )\n            where_conditions.append(\"timestamp &lt;= %s\")\n            params.append(end_timestamp)\n\n        # Build group by clause\n        if group_by == \"date\":\n            group_clause = \"DATE(to_timestamp(timestamp))\"\n            select_clause = f\"{group_clause} as group_key\"\n        else:\n            group_clause = group_by\n            select_clause = f\"{group_by} as group_key\"\n\n        # Build the query\n        base_query = f\"\"\"\n        SELECT \n            {select_clause},\n            SUM(input_tokens) as total_input,\n            SUM(output_tokens) as total_output,\n            SUM(input_tokens + output_tokens) as total_tokens,\n            COUNT(*) as event_count\n        FROM token_events\n        \"\"\"\n\n        if where_conditions:\n            query = f\"{base_query} WHERE {' AND '.join(where_conditions)}\"\n        else:\n            query = base_query\n\n        query += f\" GROUP BY {group_clause} ORDER BY total_tokens DESC\"\n\n        try:\n            with get_db_connection() as conn:\n                with conn.cursor() as cursor:\n                    cursor.execute(query, params)\n                    rows = cursor.fetchall()\n\n                    # Convert to expected format\n                    result = {}\n                    for row in rows:\n                        group_key = str(row[0])  # Convert date objects to string\n                        result[group_key] = {\n                            \"input\": row[1],\n                            \"output\": row[2],\n                            \"total\": row[3],\n                            \"events\": row[4],\n                        }\n\n                    logger.info(\n                        f\"Retrieved aggregates for {len(result)} groups by {group_by}\"\n                    )\n                    return result\n\n        except Exception as e:\n            logger.error(f\"Failed to retrieve aggregates: {e}\")\n            raise\n\n    def health_check(self) -&gt; bool:\n        \"\"\"Test if backend is accessible and healthy\"\"\"\n        try:\n            with get_db_connection() as conn:\n                with conn.cursor() as cursor:\n                    cursor.execute(\"SELECT 1\")\n                    return cursor.fetchone()[0] == 1\n        except Exception as e:\n            logger.error(f\"Health check failed: {e}\")\n            return False\n\n    def get_total_events(self) -&gt; int:\n        \"\"\"Get total number of events stored\"\"\"\n        try:\n            with get_db_connection() as conn:\n                with conn.cursor() as cursor:\n                    cursor.execute(\"SELECT COUNT(*) FROM token_events\")\n                    return cursor.fetchone()[0]\n        except Exception as e:\n            logger.error(f\"Failed to get total events: {e}\")\n            return 0\n\n    def get_overall_stats(self) -&gt; dict:\n        \"\"\"Get overall statistics without fetching individual events\"\"\"\n        query = \"\"\"\n        SELECT \n            COUNT(*) as requests,\n            SUM(input_tokens) as total_input,\n            SUM(output_tokens) as total_output,\n            COUNT(DISTINCT provider) as unique_providers,\n            COUNT(DISTINCT model) as unique_models\n        FROM token_events\n        \"\"\"\n\n        try:\n            with get_db_connection() as conn:\n                with conn.cursor() as cursor:\n                    cursor.execute(query)\n                    row = cursor.fetchone()\n                    return {\n                        \"requests\": row[0],\n                        \"input\": row[1],\n                        \"output\": row[2],\n                        \"total_tokens\": row[1] + row[2],\n                        \"providers\": row[3],\n                        \"models\": row[4],\n                    }\n        except Exception as e:\n            logger.error(f\"Failed to get overall stats: {e}\")\n            raise\n</file>\n    <file path=\"odometer/database/pgres/snapshot.py\">#!/usr/bin/env python3\n\"\"\"\nOdometer Snapshot - Shows usage statistics from the persistent odometer\n\nUsage:\n    python snapshot.py\n\"\"\"\n\nfrom conduit.odometer.database.pgres.PostgresBackend import PostgresBackend\nfrom datetime import date\nfrom rich.console import Console\nfrom rich.table import Table\nimport sys\n\n\ndef format_large_number(num):\n    \"\"\"Format large numbers with commas\"\"\"\n    return f\"{num:,}\"\n\n\ndef print_usage_stats(console, backend):\n    \"\"\"Print usage statistics as a single line\"\"\"\n    stats = backend.get_overall_stats()\n\n    if not stats:\n        console.print(\"[red]No usage data found[/red]\")\n        return\n\n    console.print(\"[bold gold3]Usage Statistics[/bold gold3]\")\n    console.print(\n        f\"[cyan]Requests:[/cyan] {format_large_number(stats['requests'])}    \"\n        f\"[cyan]Tokens:[/cyan] {format_large_number(stats['total_tokens'])}    \"\n        f\"[cyan]Input:[/cyan] [green]{format_large_number(stats['input'])}[/green]    \"\n        f\"[cyan]Output:[/cyan] [yellow]{format_large_number(stats['output'])}[/yellow]    \"\n        f\"[cyan]Providers:[/cyan] {stats['providers']}    \"\n        f\"[cyan]Models:[/cyan] {stats['models']}\"\n    )\n\n\ndef print_provider_table(console, backend):\n    \"\"\"Print clean provider table\"\"\"\n    provider_stats = backend.get_aggregates(\"provider\")\n\n    if not provider_stats:\n        return\n\n    console.print(f\"\\n[bold gold3]Usage by Provider[/bold gold3]\")\n\n    table = Table(show_header=True, header_style=\"bold\", box=None, padding=(0, 1))\n    table.add_column(\"Provider\", style=\"cyan\")\n    table.add_column(\"Requests\", justify=\"right\")\n    table.add_column(\"Input\", justify=\"right\", style=\"green\")\n    table.add_column(\"Output\", justify=\"right\", style=\"yellow\")\n    table.add_column(\"Total\", justify=\"right\", style=\"bold\")\n\n    sorted_providers = sorted(\n        provider_stats.items(), key=lambda x: x[1][\"total\"], reverse=True\n    )\n\n    for provider, stats in sorted_providers:\n        table.add_row(\n            provider,\n            format_large_number(stats[\"events\"]),\n            format_large_number(stats[\"input\"]),\n            format_large_number(stats[\"output\"]),\n            format_large_number(stats[\"total\"]),\n        )\n\n    console.print(table)\n\n\ndef print_models_table(console, model_stats, limit=10, title=\"Top 10 Models\"):\n    \"\"\"Print clean top models table\"\"\"\n    if not model_stats:\n        return\n\n    console.print(f\"\\n[bold gold3]{title}[/bold gold3]\")\n\n    table = Table(show_header=True, header_style=\"bold\", box=None, padding=(0, 1))\n    table.add_column(\"Rank\", justify=\"center\", width=4)\n    table.add_column(\"Model\", style=\"cyan\")\n    table.add_column(\"Requests\", justify=\"right\")\n    table.add_column(\"Total Tokens\", justify=\"right\", style=\"bold\")\n\n    sorted_models = sorted(\n        model_stats.items(), key=lambda x: x[1][\"total\"], reverse=True\n    )[:limit]\n\n    for i, (model, stats) in enumerate(sorted_models, 1):\n        display_model = model[:35] + \"...\" if len(model) &gt; 38 else model\n        rank_text = \"\ud83e\udd47\" if i == 1 else \"\ud83e\udd48\" if i == 2 else \"\ud83e\udd49\" if i == 3 else str(i)\n\n        table.add_row(\n            rank_text,\n            display_model,\n            format_large_number(stats[\"events\"]),\n            format_large_number(stats[\"total\"]),\n        )\n\n    console.print(table)\n\n\ndef main():\n    \"\"\"Main function to generate the snapshot\"\"\"\n    console = Console()\n\n    try:\n        with console.status(\"Connecting...\", spinner=\"dots\"):\n            backend = PostgresBackend()\n\n            if not backend.health_check():\n                console.print(\"\u274c [red]Database connection failed[/red]\")\n                sys.exit(1)\n\n        print_usage_stats(console, backend)\n        print_provider_table(console, backend)\n\n        # All-time models\n        all_time_stats = backend.get_aggregates(\"model\")\n        print_models_table(console, all_time_stats)\n\n        # Today's models\n        today = date.today()\n        daily_stats = backend.get_aggregates(\"model\", start_date=today, end_date=today)\n        if daily_stats:\n            print_models_table(console, daily_stats, title=\"Top 10 Models (Today)\")\n        else:\n            console.print(f\"\\n[bold gold3]No usage today[/bold gold3]\")\n\n    except KeyboardInterrupt:\n        console.print(\"\\n[yellow]Cancelled[/yellow]\")\n        sys.exit(0)\n    except Exception as e:\n        console.print(f\"\u274c [red]Error: {e}[/red]\")\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n</file>\n    <file path=\"odometer/database/sqlite/SqliteBackend.py\">from conduit.odometer.database.PersistenceBackend import PersistenceBackend\nfrom conduit.odometer.TokenEvent import TokenEvent\nfrom typing import override\nfrom datetime import date\nfrom pathlib import Path\n\ndefault_db_path = Path(__file__).parent / \"odometer.db\"\n\n\nclass SQLiteBackend(PersistenceBackend):\n    def __init__(self, db_path: str | Path = default_db_path):\n        self.db_path: str | Path = db_path\n        raise NotImplementedError(\n            \"SQLite backend is not yet implemented. Please use PostgreSQL backend.\"\n        )\n\n    @override\n    def store_events(self, events: list[TokenEvent]) -&gt; None:\n        # Local SQLite storage\n        pass\n\n    @override\n    def get_events(\n        self,\n        start_date: date | None = None,\n        end_date: date | None = None,\n        provider: str | None = None,\n        model: str | None = None,\n        host: str | None = None,\n    ) -&gt; list[TokenEvent]:\n        # Query with filters\n        return []\n\n    @override\n    def get_aggregates(\n        self,\n        group_by: str,  # \"provider\", \"model\", \"host\", \"date\"\n        start_date: date | None = None,\n        end_date: date | None = None,\n    ) -&gt; dict[str, dict[str, int]]:\n        # Aggregation queries\n        return {}\n</file>\n    <file path=\"request/outputtype.py\">\"\"\"\nPrimitives for various types of llm outputs.\n- completion: get a TextMessage (default)\n- imagegen: get an ImageMessage\n- tts: get an AudioMessage\n\"\"\"\n\nfrom typing import Literal\n\nOutputType = Literal[\"text\", \"image\", \"audio\"]\n</file>\n    <file path=\"request/__init__.py\"/>\n    <file path=\"request/clientparams.py\">from pydantic import BaseModel, Field\nfrom conduit.model.models.provider import Provider\nfrom typing import Optional, ClassVar\n\n\nclass ClientParams(BaseModel):\n    \"\"\"\n    Parameters that are specific to a client.\n    \"\"\"\n\n    provider: ClassVar[Provider]\n\n\nclass OpenAIParams(ClientParams):\n    \"\"\"\n    Parameters specific to OpenAI API spec-using clients.\n    NOTE: This is a generic OpenAI client, not the official OpenAI API.\n    \"\"\"\n\n    # Class var\n    temperature_range: ClassVar[tuple[float, float]] = (0.0, 2.0)\n    provider: ClassVar[Provider] = \"openai\"\n\n    # Core parameters\n    max_tokens: Optional[int] = None\n    frequency_penalty: Optional[float] = None\n    presence_penalty: Optional[float] = None\n    stop: Optional[list[str]] = None\n\n\nclass GoogleParams(OpenAIParams):\n    \"\"\"\n    Parameters specific to Gemini clients.\n    Inherits from OpenAIParams to maintain compatibility with OpenAI API spec.\n    \"\"\"\n\n    # Class var\n    temperature_range: ClassVar[tuple[float, float]] = (0.0, 1.0)\n    provider: ClassVar[Provider] = \"google\"\n\n\nclass OllamaParams(OpenAIParams):\n    \"\"\"\n    Parameters specific to Ollama clients.\n    Inherits from OpenAIParams to maintain compatibility with OpenAI API spec.\n    Ollama has a special implementation of client_params, see the special handling within Params.to_openai_spec().\n    \"\"\"\n\n    # Class var\n    temperature_range: ClassVar[tuple[float, float]] = (0.0, 1.0)\n    provider: ClassVar[Provider] = \"ollama\"\n\n    # Core parameters -- note for Instructor we will need to embed these in \"extra_body\":{\"options\": {}}\n    num_ctx: Optional[int] = None\n    temperature: Optional[float] = None\n    top_k: Optional[int] = None\n    top_p: Optional[float] = None\n    repeat_penalty: Optional[float] = None\n    stop: Optional[list[str]] = None\n\n\nclass AnthropicParams(ClientParams):\n    \"\"\"\n    Parameters specific to Anthropic clients.\n    \"\"\"\n\n    # Class var\n    temperature_range: ClassVar[tuple[float, float]] = (0.0, 1.0)\n    provider: ClassVar[Provider] = \"anthropic\"\n\n    # Core parameters\n    top_k: Optional[int] = None\n    top_p: Optional[float] = None\n    stop_sequences: Optional[list[str]] = None\n\n    # Excluded from serialization in API calls\n    model: str = Field(\n        default=\"\",\n        description=\"The model identifier to use for inference.\",\n        exclude=True,\n    )\n\n\nclass PerplexityParams(OpenAIParams):\n    \"\"\"\n    Parameters specific to Perplexity clients.\n    Inherits from OpenAIParams to maintain compatibility with OpenAI API spec.\n    \"\"\"\n\n    # Class var\n    temperature_range: ClassVar[tuple[float, float]] = (0.0, 2.0)\n    provider: ClassVar[Provider] = \"perplexity\"\n\n\n# All client parameters types, for validation\nClientParamsModels = [\n    OpenAIParams,\n    OllamaParams,\n    AnthropicParams,\n    GoogleParams,\n    PerplexityParams,\n]\n</file>\n    <file path=\"request/request.py\">from pydantic import BaseModel, Field, ValidationError\nfrom typing import Optional\nfrom conduit.message.message import Message\nfrom conduit.message.textmessage import TextMessage\nfrom conduit.message.messages import Messages\nfrom conduit.parser.parser import Parser\nfrom conduit.progress.verbosity import Verbosity\nfrom conduit.progress.display_mixins import (\n    RichDisplayParamsMixin,\n    PlainDisplayParamsMixin,\n)\nfrom conduit.request.clientparams import (\n    ClientParamsModels,\n    OpenAIParams,\n    OllamaParams,\n    AnthropicParams,\n    GoogleParams,\n    PerplexityParams,\n)\nfrom conduit.request.outputtype import OutputType\nfrom conduit.model.models.provider import Provider\n\n\nclass Request(BaseModel, RichDisplayParamsMixin, PlainDisplayParamsMixin):\n    \"\"\"\n    Parameters that are constructed by Model and are sent to Clients.\n    Note: we mixin our DisplayParamsMixin classes to provide rich and plain display methods.\n    \"\"\"\n\n    # Core parameters\n    output_type: OutputType = Field(\n        default=\"text\", description=\"Desired output: 'text', 'image', 'audio'\"\n    )\n    model: str = Field(..., description=\"The model identifier to use for inference.\")\n    messages: Messages | list[Message] = Field(\n        default_factory=list,\n        description=\"List of messages to send to the model. Can include text, images, audio, etc.\",\n    )\n\n    # Optional parameters\n    temperature: Optional[float] = Field(\n        default=None,\n        description=\"Temperature for sampling. If None, defaults to provider-specific value.\",\n    )\n    stream: bool = False\n    verbose: Verbosity = Field(\n        default=Verbosity.PROGRESS,\n        exclude=True,\n        description=\"Verbosity level for logging and progress display.\",\n    )\n    response_model: type[BaseModel] | dict | None = Field(\n        default=None,\n        description=\"Pydantic model to convert messages to a specific format. If dict, this is a schema for the model for serialization / caching purposes.\",\n    )\n    max_tokens: Optional[int] = Field(\n        default=None,\n        description=\"Maximum number of tokens to generate. If None, it is not passed to the client (except for Anthropic, which requires it and has a default).\",\n    )\n    # Post model init parameters\n    provider: Optional[Provider] = Field(\n        default=None,\n        description=\"Provider of the model, populated post init. Not intended for direct use.\",\n    )\n    # Client parameters (embedded in dict for now)\n    client_params: Optional[dict] = Field(\n        default=None,\n        description=\"Client-specific parameters. Validated against the provider-approved params, through our ClientParams.\",\n    )\n\n    def model_post_init(self, __context) -&gt; None:\n        \"\"\"\n        Post-initialization method to validate and set parameters.\n        Validates:\n        - Model is supported by at least one provider\n        - Temperature is within range for the provider\n        - Client parameters match the provider\n        Sets:\n        - Provider based on the model\n        - Client parameters based on the provider\n        - Messages are converted to a consistent format\n        Finally, validates that we have our required parameters.\n        \"\"\"\n        self._validate_model()  # Raise error if model is not supported\n        self._set_provider()  # Set provider based on model\n        self._validate_temperature()  # Raise error if temperature is out of range for provider\n        self._set_client_params()  # Set client_params based on provider\n        # Convert messages to Messages object if they are a list\n        self.messages = (\n            Messages(self.messages)\n            if isinstance(self.messages, list)\n            else self.messages\n        )\n        # Validate the whole lot\n        if self.messages is None or len(self.messages) == 0:\n            raise ValueError(\"Messages cannot be empty. Likely a code error.\")\n        if self.provider == \"\" or self.provider is None:\n            raise ValueError(\n                \"Provider must be set based on the model. Likely a code error.\"\n            )\n\n    # Constructor methods\n    @classmethod\n    def from_query_input(\n        cls, query_input: str | Message | list[Message], **kwargs\n    ) -&gt; \"Request\":\n        \"\"\"\n        Create a Request from various input types.\n\n        Args:\n            query_input: Can be:\n                - str: Creates a TextMessage with this content\n                - Message: Uses the Message directly (AudioMessage, ImageMessage, etc.)\n                - list[Message]: Uses the list of messages directly\n        \"\"\"\n        messages = kwargs.pop(\"messages\", [])\n\n        # Handle different input types\n        if isinstance(query_input, str):\n            # Original behavior - create TextMessage from string\n            user_message = TextMessage(role=\"user\", content=query_input)\n            modified_messages = messages + [user_message]\n\n        elif isinstance(query_input, Message):\n            # \u2705 NEW: Handle Message objects directly (AudioMessage, ImageMessage, etc.)\n            modified_messages = messages + [query_input]\n\n        elif isinstance(query_input, list) and all(\n            isinstance(msg, Message) for msg in query_input\n        ):\n            # \u2705 NEW: Handle list of Message objects\n            modified_messages = messages + query_input\n\n        else:\n            raise ValueError(\n                f\"query_input must be str, Message, or list[Message], got {type(query_input)}\"\n            )\n\n        kwargs.update({\"messages\": modified_messages})\n        return cls(**kwargs)\n\n    # Validation methods\n    def _set_provider(self):\n        from conduit.model.models.modelstore import ModelStore\n\n        for provider in ModelStore.models().keys():\n            if self.model in ModelStore.models()[provider]:\n                self.provider = provider\n                return\n        if not self.provider:\n            raise ValueError(f\"Model '{self.model}' is not supported by any provider.\")\n\n    def _set_client_params(self):\n        \"\"\"\n        User should add extra client params as a dict called client_params.\n        \"\"\"\n        # If client_params is already set, we assume it's a dict and validate it.\n        if self.client_params is None:\n            return\n        # If client_params is a dict, we need to validate it against the provider's client params.\n        for client_param_type in ClientParamsModels:\n            if self.provider == client_param_type.provider:\n                # Validate the dict against the provider's client params\n                try:\n                    client_param_type.model_validate(self.client_params)\n                except ValidationError as e:\n                    raise ValidationError(\n                        f\"Client parameters do not match the expected format for provider '{self.provider}': {e}\"\n                    )\n\n    def _validate_temperature(self):\n        \"\"\"\n        Validate temperature against provider-specific ranges.\n        \"\"\"\n        if self.temperature is None:\n            return\n        # Find the temperature range for the provider\n        temperature_range: Optional[tuple[float, float]] = None\n        for client_param_type in ClientParamsModels:\n            if self.provider == client_param_type.provider:\n                temperature_range = client_param_type.temperature_range\n        if not temperature_range:\n            raise ValueError(\n                f\"Temperature range not found for provider: {self.provider}\"\n            )\n        # Check if the temperature is within the range\n        if temperature_range[0] &lt;= self.temperature &lt;= temperature_range[1]:\n            return\n        else:\n            raise ValidationError(\n                f\"Temperature {self.temperature} is out of range {temperature_range} for provider: {self.provider}\"\n            )\n\n    def _validate_model(self):\n        \"\"\"\n        Validate that the model is supported by at least one provider.\n        \"\"\"\n        from conduit.model.models.modelstore import ModelStore\n\n        if not ModelStore.is_supported(self.model):\n            raise ValidationError(f\"Model '{self.model}' is not supported.\")\n        return\n\n    # For Caching\n    def normalize_json(self, obj):\n        \"\"\"\n        Nested dicts and lists are normalized to ensure consistent ordering.\n        This is crucial for consistent caching and comparison.\n        \"\"\"\n        if isinstance(obj, dict):\n            return {k: self.normalize_json(obj[k]) for k in sorted(obj)}\n        elif isinstance(obj, list):\n            return [self.normalize_json(item) for item in obj]\n        else:\n            return obj\n\n    def generate_cache_key(self) -&gt; str:\n        \"\"\"\n        Generate a reliable cache key for the Request instance.\n        Only includes fields that would affect the LLM response.\n        \"\"\"\n        from hashlib import sha256\n        import json\n\n        # Use sort_keys for deterministic JSON ordering\n        messages_str: str = json.dumps(self.messages.to_cache_dict(), sort_keys=True)\n        # Include parser since it affects response format\n        schema_str: str = (\n            Parser.as_string(self.response_model) if self.response_model else \"none\"\n        )\n        # Handle None temperature gracefully\n        temp_str = str(self.temperature) if self.temperature is not None else \"none\"\n        # Include client_params if they are set; the normalize_json ensures consistent ordering, recursively\n        if self.client_params:\n            client_params_str = json.dumps(\n                self.normalize_json(self.client_params), sort_keys=True\n            )\n        else:\n            client_params_str = \"none\"\n        params_str = \"|\".join(\n            [\n                self.model,\n                messages_str,\n                temp_str,\n                schema_str,\n                self.provider or \"none\",\n                client_params_str,\n            ]\n        )\n        return sha256(params_str.encode(\"utf-8\")).hexdigest()\n\n    # Dunder methods for string representation\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Generate a string representation of the Request instance.\n        \"\"\"\n        return f\"Request(model={self.model}, messages={self.messages}, temperature={self.temperature}, provider={self.provider})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Generate a detailed string representation of the Request instance.\n        \"\"\"\n        return (\n            f\"Request(model={self.model!r}, messages={self.messages!r}, \"\n            f\"temperature={self.temperature!r}, provider={self.provider!r}, \"\n            f\"client_params={self.client_params!r}, response_model={self.response_model!r})\"\n        )\n\n    # Serialization methods (for Cache, MessageStore, etc.)\n    def to_cache_dict(self) -&gt; dict:\n        \"\"\"\n        Required attributes:\n        - model: str\n        - messages: list[dict]\n        - temperature: Optional[float]\n        - response_model: Optional[dict]\n        - provider: Optional[str]\n        - client_params: Optional[dict]\n        - stream: bool\n        - verbose: Verbosity\n        \"\"\"\n        cache_dict = {\n            \"model\": self.model,\n            \"messages\": self.messages.to_cache_dict(),\n            \"temperature\": self.temperature,\n            \"response_model\": (\n                Parser.as_string(self.response_model)\n                if self.response_model and isinstance(self.response_model, type)\n                else None\n            ),\n            \"provider\": self.provider,\n            \"client_params\": self.client_params,\n            \"stream\": self.stream,\n            \"verbose\": self.verbose.value,\n        }\n        return cache_dict\n\n    @classmethod\n    def from_cache_dict(cls, cache_dict: dict) -&gt; \"Request\":\n        model = cache_dict.get(\"model\")\n        messages = cache_dict.get(\"messages\")\n        temperature = cache_dict.get(\"temperature\", None)\n        response_model = cache_dict.get(\"response_model\", None)\n        if response_model and isinstance(response_model, str):\n            # Find the matching Pydantic class in Parser._response_models\n            import json\n\n            try:\n                schema_dict = json.loads(response_model)\n                title = schema_dict.get(\"title\")\n\n                for model_class in Parser._response_models:\n                    if model_class.__name__ == title:\n                        response_model = model_class\n                else:\n                    # If not found, set to None to avoid validation error\n                    response_model = None\n            except (json.JSONDecodeError, KeyError):\n                response_model = None\n\n        client_params = cache_dict.get(\"client_params\", None)\n        stream = cache_dict.get(\"stream\", False)\n        verbose = cache_dict.get(\"verbose\", Verbosity.PROGRESS)\n\n        return cls(\n            model=str(model),\n            messages=Messages.from_cache_dict(messages) if messages else [],\n            temperature=temperature,\n            response_model=response_model,\n            client_params=client_params,\n            stream=stream,\n            verbose=verbose,\n        )\n\n    # Each customization method first operates on client_params, then constructs base parameters.\n    def _to_openai_spec(self) -&gt; dict:\n        \"\"\"\n        We use OpenAI spec with OpenAI, Gemini, Ollama, and Perplexity clients.\n        \"\"\"\n        if self.client_params:\n            assert OpenAIParams.model_validate(self.client_params), (\n                f\"OpenAIParams expected for OpenAI client, not {type(self.client_params)}.\"\n            )\n\n        # Convert messages to OpenAI format based on the provider\n        match self.provider:\n            case \"openai\":\n                converted_messages = self.messages.to_openai()  # type: ignore\n            case \"ollama\":\n                converted_messages = self.messages.to_ollama()  # type: ignore\n            case \"google\":\n                converted_messages = self.messages.to_google()  # type: ignore\n            case \"perplexity\":\n                converted_messages = self.messages.to_perplexity()  # type: ignore\n            case _:\n                raise ValueError(\n                    f\"Unsupported provider '{self.provider}' for OpenAI spec conversion.\"\n                )\n        assert len(converted_messages) &gt; 0, (\n            \"converted_messages is empty. Unable to convert to OpenAI spec.\"\n        )\n\n        base_params = {\n            \"model\": self.model,\n            \"messages\": converted_messages,\n            \"response_model\": self.response_model,\n            \"temperature\": self.temperature,\n            \"stream\": self.stream,\n            \"max_tokens\": self.max_tokens,\n        }\n\n        # OpenAI has different max token parameters for different models, irritatingly.\n        if self.max_tokens is not None:\n            if self.provider == \"openai\" and (\n                self.model.startswith(\"o1-\")\n                or self.model.startswith(\"o3-\")\n                or self.model.startswith(\"gpt-5\")\n            ):\n                base_params[\"max_completion_tokens\"] = self.max_tokens\n                base_params.pop(\"max_tokens\", None)  # Remove max_tokens if set\n            else:\n                # All other providers (google, ollama, perplexity) and\n                # older OpenAI models use max_tokens\n                base_params[\"max_tokens\"] = self.max_tokens\n\n        # Automatically include all client params\n        if self.provider == \"ollama\":\n            if self.client_params:\n                # Ollama expects options to be nested under \"extra_body\"\n                base_params.update({\"extra_body\": {\"options\": self.client_params}})\n        else:\n            if self.client_params:\n                base_params.update(self.client_params)\n\n        # Filter out None values and return\n        return {\n            k: v\n            for k, v in base_params.items()\n            if v is not None or k == \"response_model\"\n        }  # Actually filter None values EXCEPT for response_model, as Instructor expects it to be present\n\n    def to_openai(self) -&gt; dict:\n        if self.client_params:\n            assert OpenAIParams.model_validate(self.client_params), (\n                f\"OpenAIParams expected for OpenAI client, not {type(self.client_params)}.\"\n            )\n        return self._to_openai_spec()\n\n    def to_ollama(self) -&gt; dict:\n        \"\"\"\n        We should set num_ctx so we have maximal context window.\n        Recall that Ollama with Instructor/OpenAI spec expects options to be nested under extra_body, so we have a special case within to_openai_spec.\n        \"\"\"\n        from conduit.model.models.modelstore import ModelStore\n\n        if self.client_params:\n            assert OllamaParams.model_validate(self.client_params), (\n                f\"OllamaParams expected for Ollama client, not {type(self.client_params)}.\"\n            )\n        # Set num_ctx to the maximum context window for the model.\n        num_ctx = ModelStore.get_num_ctx(self.model)\n        if num_ctx is None:\n            raise ValueError(\n                f\"Model '{self.model}' does not have a defined context window.\"\n            )\n        if self.client_params:\n            self.client_params[\"num_ctx\"] = num_ctx\n        else:\n            self.client_params = {\"num_ctx\": num_ctx}\n        return self._to_openai_spec()\n\n    def to_anthropic(self) -&gt; dict:\n        \"\"\"\n        Convert parameters to Anthropic format.\n        Key differences from OpenAI:\n        1. System messages become a separate 'system' parameter\n        2. max_tokens is required\n        3. No response_model in the API call params\n        \"\"\"\n        if self.client_params:\n            assert AnthropicParams.model_validate(self.client_params), (\n                f\"AnthropicParams expected for Anthropic client, not {type(self.client_params)}.\"\n            )\n        # Start with converted messages\n        converted_messages = self.messages.to_anthropic()  # type: ignore\n\n        # Extract system message if present\n        system_content = \"\"\n        filtered_messages = []\n\n        for message in converted_messages:\n            if message.get(\"role\") == \"system\":\n                system_content = message.get(\"content\", \"\")\n            else:\n                filtered_messages.append(message)\n\n        # Also check if any remaining messages have system role (Anthropic quirk)\n        for message in filtered_messages:\n            if message.get(\"role\") == \"system\":\n                # Convert system role to user role (as per your AnthropicClient logic)\n                message[\"role\"] = \"user\"\n\n        # Build base parameters\n        base_params = {\n            \"model\": self.model,\n            \"messages\": filtered_messages,\n            \"max_retries\": 0,\n            \"response_model\": self.response_model,\n            \"temperature\": (\n                self.temperature if self.temperature is not None else 1.0\n            ),  # Default to 1.0 if not set\n            \"max_tokens\": self.max_tokens if self.max_tokens is not None else 4000,\n        }\n\n        # Add system parameter if we have system content\n        if system_content:\n            base_params[\"system\"] = system_content\n\n        # Remove message_type as it's not used in Anthropic API\n        for message in filtered_messages:\n            if message.get(\"message_type\"):\n                message.pop(\"message_type\")\n\n        # Add temperature if specified and validate range\n        if self.temperature is not None:\n            if not (0 &lt;= self.temperature &lt;= 1):\n                raise ValueError(\n                    \"Temperature for Anthropic models needs to be between 0 and 1.\"\n                )\n            base_params[\"temperature\"] = self.temperature\n\n        # Add client_params to base_params\n        if self.client_params:\n            base_params.update(self.client_params)\n\n        return {\n            k: v\n            for k, v in base_params.items()\n            if v is not None or k == \"response_model\"\n        }\n\n    def to_google(self) -&gt; dict:\n        if self.client_params:\n            assert GoogleParams.model_validate(self.client_params), (\n                f\"GoogleParams expected for Google client, not {type(self.client_params)}.\"\n            )\n            # Remove \"frequency_penalty\" key if it exists, as Google does not use it (unlike OpenAI).\n            self.client_params.pop(\"frequency_penalty\", None)\n        return self._to_openai_spec()\n\n    def to_perplexity(self) -&gt; dict:\n        if self.client_params:\n            assert PerplexityParams.model_validate(self.client_params), (\n                f\"PerplexityParams expected for Perplexity client, not {type(self.client_params)}.\"\n            )\n            # Remove \"stop\" key if it exists, as Perplexity does not use it (unlike OpenAI).\n            self.client_params.pop(\"stop\", None)\n        return self._to_openai_spec()\n</file>\n    <file path=\"result/response.py\">\"\"\"\nA successful Result.\n\"\"\"\n\nfrom conduit.message.message import Message\nfrom conduit.message.textmessage import TextMessage\nfrom conduit.message.audiomessage import AudioMessage\nfrom conduit.message.imagemessage import ImageMessage\nfrom conduit.message.messages import Messages\nfrom conduit.request.request import Request\nfrom conduit.logs.logging_config import get_logger\nfrom conduit.progress.display_mixins import (\n    RichDisplayResponseMixin,\n    PlainDisplayResponseMixin,\n)\nfrom pydantic import BaseModel, Field, model_validator\nfrom datetime import datetime\n\nlogger = get_logger(__name__)\n\n\nMessageUnion = TextMessage | AudioMessage | ImageMessage\n\n\nclass Response(BaseModel, RichDisplayResponseMixin, PlainDisplayResponseMixin):\n    \"\"\"\n    Our class for a successful Result.\n    We mixin display modules so that Responses can to_plain, to_rich as part of our progress tracking / verbosity system.\n    \"\"\"\n\n    # Core attributes\n    message: MessageUnion\n    request: Request\n    input_tokens: int\n    output_tokens: int\n    duration: float\n\n    # Control flag -- if this is rehydrated from cache, we don't want to emit token events again\n    emit_tokens: bool = Field(default=True, exclude=True)  # Exclude from serialization\n\n    # Initialization attributes\n    timestamp: str = Field(\n        default_factory=lambda: datetime.now().isoformat(),\n        description=\"Timestamp of the response creation\",\n    )\n\n    @model_validator(mode=\"after\")\n    def emit_token_event_if_enabled(self):\n        if self.emit_tokens:\n            try:\n                self.emit_token_event()\n            except Exception as e:\n                logger.error(f\"Failed to emit token event: {e}\")\n        return self\n\n    def emit_token_event(self):\n        \"\"\"\n        Emit a TokenEvent to the OdometerRegistry if it exists.\n        \"\"\"\n        from conduit.odometer.TokenEvent import TokenEvent\n        from conduit.model.model import Model\n\n        assert self.request.provider, \"Provider must be set in the request\"\n\n        # Get hostname\n        import socket\n\n        try:\n            host = socket.gethostname()\n        except Exception as e:\n            logger.error(f\"Failed to get hostname: {e}\")\n            host = \"unknown\"\n\n        event = TokenEvent(\n            provider=self.request.provider,\n            model=self.request.model,\n            input_tokens=self.input_tokens,\n            output_tokens=self.output_tokens,\n            timestamp=int(datetime.fromisoformat(self.timestamp).timestamp()),\n            host=host,\n        )\n        Model._odometer_registry.emit_token_event(event)\n\n    def to_cache_dict(self) -&gt; dict:\n        \"\"\"\n        Serialize Response to cache-friendly dictionary.\n        \"\"\"\n        return {\n            \"message\": self.message.to_cache_dict(),\n            \"request\": self.request.to_cache_dict(),\n            \"input_tokens\": self.input_tokens,\n            \"output_tokens\": self.output_tokens,\n            \"duration\": self.duration,\n            \"timestamp\": self.timestamp,\n        }\n\n    @classmethod\n    def from_cache_dict(cls, cache_dict: dict) -&gt; \"Response\":\n        \"\"\"\n        Deserialize Response from cache dictionary.\n        \"\"\"\n        return cls(\n            message=Message.from_cache_dict(cache_dict[\"message\"]),\n            request=Request.from_cache_dict(cache_dict[\"request\"]),\n            input_tokens=cache_dict[\"input_tokens\"],\n            output_tokens=cache_dict[\"output_tokens\"],\n            duration=cache_dict[\"duration\"],\n            timestamp=cache_dict.get(\"timestamp\", datetime.now().isoformat()),\n            emit_tokens=False,  # Do not emit tokens for cached responses\n        )\n\n    @property\n    def prompt(self) -&gt; str | None:\n        \"\"\"\n        This is the last user message.\n        \"\"\"\n        return self.request.messages[-1].content\n\n    @property\n    def messages(self) -&gt; Messages:\n        return Messages(messages=self.request.messages + [self.message])\n\n    @property\n    def total_tokens(self) -&gt; int:\n        return self.input_tokens + self.output_tokens\n\n    @property\n    def content(self) -&gt; str | BaseModel | list[BaseModel] | list[str]:\n        \"\"\"\n        This is the last assistant message content.\n        \"\"\"\n        return self.message.content\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"\n        This is the model used for the response.\n        \"\"\"\n        return self.request.model\n\n    def display(self):\n        \"\"\"\n        Display image if the latest message is an ImageMessage.\n        \"\"\"\n        from conduit.message.imagemessage import ImageMessage\n\n        if isinstance(self.message, ImageMessage):\n            self.message.display()\n        else:\n            raise TypeError(\n                f\"Cannot display message of type {self.message.__class__.__name__}. \"\n                \"Only ImageMessage can be displayed.\"\n            )\n\n    def play(self):\n        \"\"\"\n        Play audio if the latest message is an AudioMessage.\n        \"\"\"\n        from conduit.message.audiomessage import AudioMessage\n\n        if isinstance(self.message, AudioMessage):\n            self.message.play()\n        else:\n            raise TypeError(\n                f\"Cannot play message of type {self.message.__class__.__name__}. \"\n                \"Only AudioMessage can be played.\"\n            )\n\n    def __repr__(self):\n        attr_list = []\n        for k, v in self.__dict__.items():\n            if k == \"message\" and hasattr(v, \"content\"):\n                # Special handling for message content\n                content = v.content\n                if isinstance(content, str):\n                    word_count = len(content.split())\n                    if len(content) &gt; 20:\n                        truncated_content = (\n                            content[:20] + f\"... [{word_count} words total]\"\n                        )\n                    else:\n                        truncated_content = content\n                elif isinstance(content, BaseModel):\n                    word_count = len(content.model_dump_json().split())\n                    truncated_content = f\"{content.__class__.__name__}(...)\"\n                elif isinstance(content, list):\n                    word_count = sum(\n                        len(item.split()) if isinstance(item, str) else 0\n                        for item in content\n                    )\n                    truncated_content = f\"List[{len(content)} items] (...)\"\n\n                # Build the message repr manually with truncated content\n                message_repr = (\n                    f\"TextMessage(role='{v.role}', content='{truncated_content}')\"\n                )\n                attr_list.append(f\"{k}={message_repr}\")\n            else:\n                attr_list.append(f\"{k}={repr(v)[:50]}\")\n\n        attributes = \", \".join(attr_list)\n        return f\"{self.__class__.__name__}({attributes})\"\n\n    def __str__(self):\n        \"\"\"\n        We want to pass as string when possible.\n        Allow json objects (dict) to be pretty printed.\n        \"\"\"\n        content = self.content\n        if content == None or content == \"\":\n            return \"\"\n        if content.__class__.__name__ == \"PerplexityContent\":\n            output = content.text + \"\\n\\n\"\n            for index, citation in enumerate(content.citations):\n                output += f\"{index + 1}. - [{citation.title}]({citation.url})\\n\"\n        else:\n            output = content\n        return output\n\n    def __len__(self):\n        \"\"\"\n        We want to be able to check the length of the content.\n        \"\"\"\n        return len(self.__str__())\n</file>\n    <file path=\"result/result.py\">from typing import Union\nfrom conduit.result.response import Response\nfrom conduit.result.error import ConduitError\n\nConduitResult = Union[Response, ConduitError]\n</file>\n    <file path=\"result/__init__.py\"/>\n    <file path=\"result/error.py\">from conduit.logs.logging_config import get_logger\nfrom conduit.progress.display_mixins import (\n    RichDisplayConduitErrorMixin,\n    PlainDisplayConduitErrorMixin,\n)\nfrom typing import Optional, Any\nfrom pydantic import BaseModel, Field\nimport traceback\nfrom datetime import datetime\n\nlogger = get_logger(__name__)\n\n\nclass ErrorInfo(BaseModel):\n    \"\"\"Simple error information\"\"\"\n\n    code: str = Field(\n        ..., description=\"Error code like 'validation_error', 'api_error', etc.\"\n    )\n    message: str = Field(..., description=\"Human-readable error message\")\n    category: str = Field(\n        ..., description=\"Error category: 'client', 'server', 'network', 'parsing'\"\n    )\n    timestamp: datetime = Field(..., description=\"When the error occurred\")\n\n    model_config = {\"frozen\": True}  # Equivalent to dataclass(frozen=True)\n\n\nclass ErrorDetail(BaseModel):\n    \"\"\"Detailed error information for debugging\"\"\"\n\n    exception_type: str = Field(\n        ..., description=\"Type of exception like 'ValidationError', 'APIException'\"\n    )\n    stack_trace: Optional[str] = Field(\n        None, description=\"Full stack trace if available\"\n    )\n    raw_response: Optional[Any] = Field(\n        None, description=\"Original response that caused error\"\n    )\n    request_params: Optional[dict] = Field(None, description=\"Params that led to error\")\n    retry_count: Optional[int] = Field(None, description=\"If retries were attempted\")\n\n    model_config = {\"frozen\": True}  # Equivalent to dataclass(frozen=True)\n\n\nclass ConduitError(\n    BaseModel, RichDisplayConduitErrorMixin, PlainDisplayConduitErrorMixin\n):\n    \"\"\"\n    An unsuccessful Result.\n    Complete error information.\n    We mixin our display classes to provide to_plain and to_rich methods in service of our Progress / Verbosity implementation.\n    \"\"\"\n\n    info: ErrorInfo = Field(..., description=\"Core error information\")\n    detail: Optional[ErrorDetail] = Field(\n        None, description=\"Detailed debugging information\"\n    )\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Print this like a normal error message + stack trace if available.\n        \"\"\"\n        base_message = f\"[{self.info.timestamp}] {self.info.category.upper()} - {self.info.code}: {self.info.message}\"\n        if self.detail and self.detail.stack_trace:\n            return f\"{base_message}\\nStack Trace:\\n{self.detail.stack_trace}\"\n        return base_message\n\n    @classmethod\n    def from_exception(\n        cls, exc: Exception, code: str, category: str, **context\n    ) -&gt; \"ConduitError\":\n        \"\"\"Create ConduitError from an exception with full context\"\"\"\n        info = ErrorInfo(\n            code=code, message=str(exc), category=category, timestamp=datetime.now()\n        )\n\n        detail = ErrorDetail(\n            exception_type=type(exc).__name__,\n            stack_trace=traceback.format_exc(),\n            raw_response=context.get(\"raw_response\"),\n            request_params=context.get(\"request_params\"),\n            retry_count=context.get(\"retry_count\"),\n        )\n\n        return cls(info=info, detail=detail)\n\n    @classmethod\n    def simple(cls, code: str, message: str, category: str) -&gt; \"ConduitError\":\n        \"\"\"Create simple error without exception details\"\"\"\n        info = ErrorInfo(\n            code=code, message=message, category=category, timestamp=datetime.now()\n        )\n        return cls(info=info, detail=None)\n\n    def to_cache_dict(self) -&gt; dict:\n        return self.model_dump()\n\n    @classmethod\n    def from_cache_dict(cls, cache_dict: dict) -&gt; \"ConduitError\":\n        return cls(**cache_dict)\n</file>\n    <file path=\"progress/handlers.py\">\"\"\"\nEnhanced progress handlers with verbosity support.\nMaintains backwards compatibility while adding verbosity-aware methods.\n\"\"\"\n\nfrom conduit.logs.logging_config import get_logger\nfrom conduit.progress.verbosity import Verbosity\nfrom datetime import datetime\nimport time\n\nlogger = get_logger(__name__)\n\n\nclass RichProgressHandler:\n    \"\"\"Rich-based progress handler with spinners and colors - now verbosity-aware\"\"\"\n\n    def __init__(self, console):\n        self.console = console\n        self.concurrent_mode = False\n        self.concurrent_line_printed = False\n\n    # Enhanced individual operation methods with verbosity support\n    def show_spinner(self, model_name, query_preview, verbosity=Verbosity.PROGRESS):\n        \"\"\"Show Rich spinner with live status - verbosity aware\"\"\"\n        if verbosity == Verbosity.SILENT:\n            return\n\n        if self.concurrent_mode:\n            return  # Suppress individual operations during concurrent mode\n\n        if verbosity == Verbosity.PROGRESS:\n            # Current behavior for backwards compatibility\n            self.console.print(\n                f\"\u280b {model_name} | {query_preview}\",\n                end=\"\\r\",\n                highlight=False,\n                soft_wrap=True,\n            )\n        # Higher verbosity levels will be handled in future phases\n\n    def show_complete(\n        self,\n        model_name,\n        query_preview,\n        duration,\n        verbosity=Verbosity.PROGRESS,\n        response_obj=None,\n    ):\n        \"\"\"Update same line with green checkmark - verbosity aware\"\"\"\n        if verbosity == Verbosity.SILENT:\n            return\n\n        if self.concurrent_mode:\n            return  # Suppress individual operations during concurrent mode\n\n        if verbosity == Verbosity.PROGRESS:\n            # Current behavior for backwards compatibility\n            self.console.print(\n                f\"\u2713 {model_name} | {query_preview} | ({duration:.1f}s)\", style=\"green\"\n            )\n        elif verbosity &gt;= Verbosity.SUMMARY and response_obj != None:\n            # New behavior - use object display methods\n            # First show the progress line\n            self.console.print(\n                f\"\u2713 {model_name} | {query_preview} | ({duration:.1f}s)\", style=\"green\"\n            )\n            # Then show the detailed response\n            rich_content = response_obj.to_rich(verbosity)\n            if rich_content:\n                self.console.print()  # Add spacing\n                self.console.print(rich_content)\n\n    def show_canceled(self, model_name, query_preview, verbosity=Verbosity.PROGRESS):\n        \"\"\"Update same line with warning - verbosity aware\"\"\"\n        if verbosity == Verbosity.SILENT:\n            return\n\n        if self.concurrent_mode:\n            return\n\n        if verbosity == Verbosity.PROGRESS:\n            # Current behavior for backwards compatibility\n            self.console.print(\n                f\"\u26a0 {model_name} | {query_preview} | Canceled\", style=\"yellow\"\n            )\n        # Higher verbosity levels will be handled in future phases\n\n    def show_cached(\n        self, model_name, query_preview, duration, verbosity=Verbosity.PROGRESS\n    ):\n        \"\"\"Show cache hit with lightning symbol - verbosity aware\"\"\"\n        if verbosity == Verbosity.SILENT:\n            return\n\n        if self.concurrent_mode:\n            return  # Suppress individual operations during concurrent mode\n\n        if verbosity == Verbosity.PROGRESS:\n            # Current behavior for backwards compatibility\n            self.console.print(\n                f\"\u26a1 {model_name} | {query_preview} | Cached ({duration:.1f}s)\",\n                style=\"cyan\",\n            )\n        # Higher verbosity levels will be handled in future phases\n\n    def show_failed(\n        self,\n        model_name,\n        query_preview,\n        error,\n        verbosity=Verbosity.PROGRESS,\n        error_obj=None,\n    ):\n        \"\"\"Update same line with error - verbosity aware\"\"\"\n        if verbosity == Verbosity.SILENT:\n            return\n\n        if self.concurrent_mode:\n            return\n\n        if verbosity == Verbosity.PROGRESS:\n            # Current behavior for backwards compatibility\n            self.console.print(\n                f\"\u2717 {model_name} | {query_preview} | Failed: {error}\", style=\"red\"\n            )\n        elif verbosity &gt;= Verbosity.SUMMARY and error_obj:\n            # New behavior - use object display methods\n            # First show the progress line\n            self.console.print(\n                f\"\u2717 {model_name} | {query_preview} | Failed: {error}\", style=\"red\"\n            )\n            # Then show the detailed error\n            rich_content = error_obj.to_rich(verbosity)\n            if rich_content:\n                self.console.print()  # Add spacing\n                self.console.print(rich_content)\n\n    # Concurrent operation methods (unchanged for now)\n    def handle_concurrent_start(self, total: int):\n        \"\"\"Handle start of concurrent operations\"\"\"\n        self.concurrent_mode = True\n        self.concurrent_line_printed = False\n        self.console.print(\n            f\"\u280b Running {total} concurrent requests...\", end=\"\\r\", highlight=False\n        )\n\n    def update_concurrent_progress(\n        self, completed: int, total: int, running: int, failed: int, elapsed: float\n    ):\n        \"\"\"Update live concurrent progress\"\"\"\n        if not self.concurrent_mode:\n            return\n\n        # Only show progress updates every 0.5 seconds to avoid spam\n        current_time = time.time()\n        if not hasattr(self, \"_last_update\") or current_time - self._last_update &gt; 0.5:\n            self._last_update = current_time\n\n            progress_text = f\"\u280b Progress: {completed}/{total} complete | {running} running | {failed} failed | {elapsed:.1f}s elapsed\"\n            self.console.print(progress_text, end=\"\\r\", highlight=False)\n\n    def handle_concurrent_complete(self, successful: int, total: int, duration: float):\n        \"\"\"Handle completion of all concurrent operations\"\"\"\n        self.concurrent_mode = False\n\n        if successful == total:\n            self.console.print(\n                f\"[green]\u2713[/green] All requests complete: {successful}/{total} successful in {duration:.1f}s\"\n            )\n        else:\n            failed = total - successful\n            self.console.print(\n                f\"[yellow]\u2713[/yellow] All requests complete: {successful}/{total} successful, {failed} failed in {duration:.1f}s\"\n            )\n\n    # Backwards compatibility methods (now verbosity-aware with defaults)\n    def emit_started(self, model_name, query_preview, verbosity=Verbosity.PROGRESS):\n        self.show_spinner(model_name, query_preview, verbosity=verbosity)\n\n    def emit_complete(\n        self, model_name, query_preview, duration, verbosity=Verbosity.PROGRESS\n    ):\n        self.show_complete(model_name, query_preview, duration, verbosity=verbosity)\n\n    def emit_canceled(self, model_name, query_preview, verbosity=Verbosity.PROGRESS):\n        self.show_canceled(model_name, query_preview, verbosity=verbosity)\n\n    def emit_cached(\n        self, model_name, query_preview, duration, verbosity=Verbosity.PROGRESS\n    ):\n        self.show_cached(model_name, query_preview, duration, verbosity=verbosity)\n\n    def emit_failed(\n        self, model_name, query_preview, error, verbosity=Verbosity.PROGRESS\n    ):\n        self.show_failed(model_name, query_preview, error, verbosity=verbosity)\n\n\nclass PlainProgressHandler:\n    \"\"\"Simple progress handler for environments without Rich - now verbosity-aware\"\"\"\n\n    def __init__(self):\n        self.concurrent_mode = False\n\n    # Enhanced individual operation methods with verbosity support\n    def show_spinner(self, model_name, query_preview, verbosity=Verbosity.PROGRESS):\n        \"\"\"Show starting state (plain text - no spinner) - verbosity aware\"\"\"\n        if verbosity == Verbosity.SILENT:\n            return\n\n        if self.concurrent_mode:\n            return  # Suppress individual operations during concurrent mode\n\n        if verbosity == Verbosity.PROGRESS:\n            # Current behavior for backwards compatibility\n            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n            print(f\"[{timestamp}] [{model_name}] Starting: {query_preview}\")\n        # Higher verbosity levels will be handled in future phases\n\n    def show_complete(\n        self,\n        model_name,\n        query_preview,\n        duration,\n        verbosity=Verbosity.PROGRESS,\n        response_obj=None,\n    ):\n        \"\"\"Show completion on new line - verbosity aware\"\"\"\n        if verbosity == Verbosity.SILENT:\n            return\n\n        if self.concurrent_mode:\n            return\n\n        if verbosity == Verbosity.PROGRESS:\n            # Current behavior for backwards compatibility\n            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n            print(f\"[{timestamp}] [{model_name}] Complete: ({duration:.1f}s)\")\n        elif verbosity &gt;= Verbosity.SUMMARY and response_obj:\n            # New behavior - use object display methods\n            # First show the progress line\n            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n            print(f\"[{timestamp}] [{model_name}] Complete: ({duration:.1f}s)\")\n            # Then show the detailed response\n            plain_content = response_obj.to_plain(verbosity)\n            if plain_content:\n                print(plain_content)\n\n    def show_canceled(self, model_name, query_preview, verbosity=Verbosity.PROGRESS):\n        \"\"\"Show cancellation on new line - verbosity aware\"\"\"\n        if verbosity == Verbosity.SILENT:\n            return\n\n        if self.concurrent_mode:\n            return\n\n        if verbosity == Verbosity.PROGRESS:\n            # Current behavior for backwards compatibility\n            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n            print(f\"[{timestamp}] [{model_name}] Canceled\")\n        # Higher verbosity levels will be handled in future phases\n\n    def show_cached(\n        self, model_name, query_preview, duration, verbosity=Verbosity.PROGRESS\n    ):\n        \"\"\"Show cache hit in plain text - verbosity aware\"\"\"\n        if verbosity == Verbosity.SILENT:\n            return\n\n        if self.concurrent_mode:\n            return\n\n        if verbosity == Verbosity.PROGRESS:\n            # Current behavior for backwards compatibility\n            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n            print(f\"[{timestamp}] [{model_name}] Cache hit: {query_preview}\")\n        # Higher verbosity levels will be handled in future phases\n\n    def show_failed(\n        self,\n        model_name,\n        query_preview,\n        error,\n        verbosity=Verbosity.PROGRESS,\n        error_obj=None,\n    ):\n        \"\"\"Show failure on new line - verbosity aware\"\"\"\n        if verbosity == Verbosity.SILENT:\n            return\n\n        if self.concurrent_mode:\n            return\n\n        if verbosity == Verbosity.PROGRESS:\n            # Current behavior for backwards compatibility\n            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n            print(f\"[{timestamp}] [{model_name}] Failed: {error}\")\n        elif verbosity &gt;= Verbosity.SUMMARY and error_obj:\n            # New behavior - use object display methods\n            # First show the progress line\n            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n            print(f\"[{timestamp}] [{model_name}] Failed: {error}\")\n            # Then show the detailed error\n            plain_content = error_obj.to_plain(verbosity)\n            if plain_content:\n                print(plain_content)\n\n    # Concurrent operation methods (unchanged for now)\n    def handle_concurrent_start(self, total: int):\n        \"\"\"Handle start of concurrent operations\"\"\"\n        self.concurrent_mode = True\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        print(f\"[{timestamp}] Starting: {total} concurrent requests\")\n\n    def update_concurrent_progress(\n        self, completed: int, total: int, running: int, failed: int, elapsed: float\n    ):\n        \"\"\"Plain console doesn't show live updates - too noisy\"\"\"\n        pass  # Plain console shows only start/end messages\n\n    def handle_concurrent_complete(self, successful: int, total: int, duration: float):\n        \"\"\"Handle completion of all concurrent operations\"\"\"\n        self.concurrent_mode = False\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n\n        if successful == total:\n            print(\n                f\"[{timestamp}] All requests complete: {successful}/{total} successful in {duration:.1f}s\"\n            )\n        else:\n            failed = total - successful\n            print(\n                f\"[{timestamp}] All requests complete: {successful}/{total} successful, {failed} failed in {duration:.1f}s\"\n            )\n\n    # Backwards compatibility methods (now verbosity-aware with defaults)\n    def emit_started(self, model_name, query_preview, verbosity=Verbosity.PROGRESS):\n        self.show_spinner(model_name, query_preview, verbosity=verbosity)\n\n    def emit_complete(\n        self, model_name, query_preview, duration, verbosity=Verbosity.PROGRESS\n    ):\n        self.show_complete(model_name, query_preview, duration, verbosity=verbosity)\n\n    def emit_canceled(self, model_name, query_preview, verbosity=Verbosity.PROGRESS):\n        self.show_canceled(model_name, query_preview, verbosity=verbosity)\n\n    def emit_cached(\n        self, model_name, query_preview, duration, verbosity=Verbosity.PROGRESS\n    ):\n        self.show_cached(model_name, query_preview, duration, verbosity=verbosity)\n\n    def emit_failed(\n        self, model_name, query_preview, error, verbosity=Verbosity.PROGRESS\n    ):\n        self.show_failed(model_name, query_preview, error, verbosity=verbosity)\n</file>\n    <file path=\"progress/display_mixins.py\">\"\"\"\nDisplay mixins for Conduit verbosity system.\n\nThese mixins provide rich and plain text formatting capabilities to data objects\nlike Params, Response, and ConduitError. Each mixin handles the specific display\nlogic for its object type across different verbosity levels.\n\"\"\"\n\nfrom conduit.progress.verbosity import Verbosity\nfrom typing import TYPE_CHECKING, Any\nimport json\n\n# TYPE_CHECKING imports to avoid circular dependencies\nif TYPE_CHECKING:\n    from rich.console import RenderableType\n    from rich.panel import Panel\n    from rich.syntax import Syntax\n\n\ndef extract_user_prompt(request) -&gt; str:\n    \"\"\"Extract the user prompt from request messages for display\"\"\"\n    if not hasattr(request, \"messages\") or not request.messages:\n        return \"\"\n\n    # Get the last user message (most recent prompt)\n    for message in reversed(request.messages):\n        if hasattr(message, \"role\") and message.role == \"user\":\n            content = str(message.content)\n            # Handle different message types\n            if hasattr(message, \"message_type\"):\n                if message.message_type == \"image\":\n                    return f\"{getattr(message, 'text_content', '')} [Image attached]\"\n                elif message.message_type == \"audio\":\n                    return f\"{getattr(message, 'text_content', '')} [Audio attached]\"\n            return content\n    return \"\"\n\n\ndef safe_json_serialize(obj: Any) -&gt; Any:\n    \"\"\"\n    Recursively handle JSON serialization of objects that may contain Verbosity enums.\n\n    Args:\n        obj: Object to serialize (dict, list, or primitive)\n\n    Returns:\n        JSON-serializable version of the object\n    \"\"\"\n    if isinstance(obj, Verbosity):\n        return obj.to_json_serializable()\n    elif hasattr(obj, \"to_json_serializable\"):\n        return obj.to_json_serializable()\n    elif hasattr(obj, \"model_dump\"):\n        # Pydantic model - recursively serialize\n        try:\n            dumped = obj.model_dump()\n            return safe_json_serialize(dumped)\n        except:\n            return str(obj)\n    elif hasattr(obj, \"__dict__\"):\n        # Object with attributes - recursively serialize\n        try:\n            return safe_json_serialize(obj.__dict__)\n        except:\n            return str(obj)\n    elif isinstance(obj, dict):\n        # Dictionary - recursively serialize values\n        return {key: safe_json_serialize(value) for key, value in obj.items()}\n    elif isinstance(obj, (list, tuple)):\n        # List/tuple - recursively serialize items\n        return [safe_json_serialize(item) for item in obj]\n    elif hasattr(obj, \"name\") and hasattr(obj, \"value\"):\n        # Enum-like object\n        return f\"{obj.name} ({obj.value})\"\n    else:\n        # Primitive or unknown type\n        try:\n            json.dumps(obj)  # Test if it's JSON serializable\n            return obj\n        except (TypeError, ValueError):\n            return str(obj)\n\n\nclass RichDisplayMixin:\n    \"\"\"Base mixin for Rich console display functionality.\"\"\"\n\n    def to_rich(self, verbosity: Verbosity) -&gt; \"RenderableType\":\n        \"\"\"\n        Convert object to Rich renderable based on verbosity level.\n\n        Args:\n            verbosity: The verbosity level for display\n\n        Returns:\n            Rich renderable object (Panel, Syntax, Text, etc.)\n        \"\"\"\n        if verbosity == Verbosity.SILENT:\n            return \"\"\n        elif verbosity == Verbosity.PROGRESS:\n            # PROGRESS level handled by existing progress system\n            return \"\"\n        else:\n            # Delegate to specific implementation\n            return self._to_rich_impl(verbosity)\n\n    def _to_rich_impl(self, verbosity: Verbosity) -&gt; \"RenderableType\":\n        \"\"\"Override in subclasses for specific Rich formatting logic.\"\"\"\n        raise NotImplementedError(\"Subclasses must implement _to_rich_impl\")\n\n\nclass PlainDisplayMixin:\n    \"\"\"Base mixin for plain text display functionality.\"\"\"\n\n    def to_plain(self, verbosity: Verbosity) -&gt; str:\n        \"\"\"\n        Convert object to plain text based on verbosity level.\n\n        Args:\n            verbosity: The verbosity level for display\n\n        Returns:\n            Plain text string representation\n        \"\"\"\n        if verbosity == Verbosity.SILENT:\n            return \"\"\n        elif verbosity == Verbosity.PROGRESS:\n            # PROGRESS level handled by existing progress system\n            return \"\"\n        else:\n            # Delegate to specific implementation\n            return self._to_plain_impl(verbosity)\n\n    def _to_plain_impl(self, verbosity: Verbosity) -&gt; str:\n        \"\"\"Override in subclasses for specific plain text formatting logic.\"\"\"\n        raise NotImplementedError(\"Subclasses must implement _to_plain_impl\")\n\n\nclass RichDisplayParamsMixin(RichDisplayMixin):\n    \"\"\"Rich display mixin for Params objects.\"\"\"\n\n    def _to_rich_impl(self, verbosity: Verbosity) -&gt; \"RenderableType\":\n        \"\"\"Format Params object for Rich console display.\"\"\"\n        from rich.panel import Panel\n        from rich.syntax import Syntax\n        from rich.text import Text\n        from rich.table import Table\n\n        if verbosity == Verbosity.SUMMARY:\n            return self._format_params_summary_rich()\n        elif verbosity == Verbosity.DETAILED:\n            return self._format_params_detailed_rich()\n        elif verbosity == Verbosity.COMPLETE:\n            return self._format_params_complete_rich()\n        elif verbosity == Verbosity.DEBUG:\n            return self._format_params_debug_rich()\n        else:\n            return Text(\"\")\n\n    def _format_params_summary_rich(self) -&gt; \"Panel\":\n        \"\"\"Format basic request info for SUMMARY level.\"\"\"\n        from rich.panel import Panel\n        from rich.text import Text\n        from datetime import datetime\n\n        # Build content text\n        content = Text()\n\n        # Show user message(s)\n        if hasattr(self, \"messages\") and self.messages:\n            for msg in self.messages:\n                if hasattr(msg, \"role\") and msg.role == \"user\":\n                    content.append(f\"user: {str(msg.content)}\\n\", style=\"yellow\")\n        elif hasattr(self, \"query_input\") and self.query_input:\n            content.append(f\"user: {str(self.query_input)}\\n\", style=\"yellow\")\n\n        # Show parameters\n        params_line = []\n        if hasattr(self, \"temperature\") and self.temperature is not None:\n            params_line.append(f\"Temperature: {self.temperature}\")\n        if hasattr(self, \"parser\") and self.parser:\n            parser_name = getattr(self.parser, \"pydantic_model\", {})\n            if hasattr(parser_name, \"__name__\"):\n                params_line.append(f\"Parser: {parser_name.__name__}\")\n\n        if params_line:\n            content.append(\" \u2022 \".join(params_line), style=\"dim\")\n\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        return Panel(\n            content,\n            title=f\"\u25ba REQUEST {getattr(self, 'model', 'unknown')}\",\n            title_align=\"left\",\n            subtitle=f\"[dim]{timestamp}[/dim]\",\n            subtitle_align=\"right\",\n        )\n\n    def _format_params_detailed_rich(self) -&gt; \"Panel\":\n        \"\"\"Format truncated messages for DETAILED level.\"\"\"\n        from rich.panel import Panel\n        from rich.table import Table\n        from rich.text import Text\n        from datetime import datetime\n\n        # Create messages table\n        table = Table.grid(padding=(0, 1))\n        table.add_column(\"Role\", style=\"bold\", width=8)\n        table.add_column(\"Content\")\n\n        if hasattr(self, \"messages\") and self.messages:\n            for msg in self.messages:\n                if hasattr(msg, \"role\") and hasattr(msg, \"content\"):\n                    content = str(msg.content)\n                    # Truncate to ~100 characters\n                    if len(content) &gt; 100:\n                        content = content[:100] + \"...\"\n\n                    role_color = {\n                        \"system\": \"green\",\n                        \"user\": \"yellow\",\n                        \"assistant\": \"blue\",\n                    }.get(str(msg.role), \"white\")\n\n                    table.add_row(f\"[{role_color}]{msg.role}[/{role_color}]\", content)\n        elif hasattr(self, \"query_input\") and self.query_input:\n            content = str(self.query_input)\n            if len(content) &gt; 100:\n                content = content[:100] + \"...\"\n            table.add_row(\"[yellow]user[/yellow]\", content)\n\n        # Add parameters row\n        params_line = []\n        if hasattr(self, \"temperature\") and self.temperature is not None:\n            params_line.append(f\"Temperature: {self.temperature}\")\n        if hasattr(self, \"parser\") and self.parser:\n            parser_name = getattr(self.parser, \"pydantic_model\", {})\n            if hasattr(parser_name, \"__name__\"):\n                params_line.append(f\"Parser: {parser_name.__name__}\")\n\n        if params_line:\n            table.add_row(\"\", f\"[dim]{' \u2022 '.join(params_line)}[/dim]\")\n\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        return Panel(\n            table,\n            title=f\"\u25ba REQUEST {getattr(self, 'model', 'unknown')} (Detailed)\",\n            title_align=\"left\",\n            subtitle=f\"[dim]{timestamp}[/dim]\",\n            subtitle_align=\"right\",\n        )\n\n    def _format_params_complete_rich(self) -&gt; \"Panel\":\n        \"\"\"Format complete messages for COMPLETE level.\"\"\"\n        from rich.panel import Panel\n        from rich.table import Table\n        from rich.text import Text\n        from datetime import datetime\n\n        # Create messages table with full content\n        table = Table.grid(padding=(0, 1))\n        table.add_column(\"Role\", style=\"bold\", width=8)\n        table.add_column(\"Content\")\n\n        if hasattr(self, \"messages\") and self.messages:\n            for msg in self.messages:\n                if hasattr(msg, \"role\") and hasattr(msg, \"content\"):\n                    content = str(msg.content)\n                    # No truncation - show full content with word wrapping\n\n                    role_color = {\n                        \"system\": \"green\",\n                        \"user\": \"yellow\",\n                        \"assistant\": \"blue\",\n                    }.get(str(msg.role), \"white\")\n\n                    table.add_row(f\"[{role_color}]{msg.role}[/{role_color}]\", content)\n        elif hasattr(self, \"query_input\") and self.query_input:\n            content = str(self.query_input)\n            table.add_row(\"[yellow]user[/yellow]\", content)\n\n        # Add parameters row\n        params_line = []\n        if hasattr(self, \"temperature\") and self.temperature is not None:\n            params_line.append(f\"Temperature: {self.temperature}\")\n        if hasattr(self, \"parser\") and self.parser:\n            parser_name = getattr(self.parser, \"pydantic_model\", {})\n            if hasattr(parser_name, \"__name__\"):\n                params_line.append(f\"Parser: {parser_name.__name__}\")\n        if hasattr(self, \"stream\"):\n            params_line.append(f\"Stream: {self.stream}\")\n\n        if params_line:\n            table.add_row(\"\", f\"[dim]{' \u2022 '.join(params_line)}[/dim]\")\n\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        return Panel(\n            table,\n            title=f\"\u25ba REQUEST {getattr(self, 'model', 'unknown')} (Complete)\",\n            title_align=\"left\",\n            subtitle=f\"[dim]{timestamp}[/dim]\",\n            subtitle_align=\"right\",\n        )\n\n    def _format_params_debug_rich(self) -&gt; \"Panel\":\n        \"\"\"Format full JSON debug for DEBUG level.\"\"\"\n        from rich.panel import Panel\n        from rich.syntax import Syntax\n        from datetime import datetime\n        import json\n\n        # Create debug dictionary with all available information\n        debug_data = {\n            \"model\": getattr(self, \"model\", \"unknown\"),\n            \"timestamp\": datetime.now().isoformat(),\n        }\n\n        # Add messages\n        if hasattr(self, \"messages\") and self.messages:\n            debug_data[\"messages\"] = []\n            for msg in self.messages:\n                if hasattr(msg, \"role\") and hasattr(msg, \"content\"):\n                    debug_data[\"messages\"].append(\n                        {\"role\": str(msg.role), \"content\": str(msg.content)}\n                    )\n        elif hasattr(self, \"query_input\") and self.query_input:\n            debug_data[\"messages\"] = [\n                {\"role\": \"user\", \"content\": str(self.query_input)}\n            ]\n\n        # Add all other parameters\n        for attr in [\"temperature\", \"stream\", \"verbose\"]:\n            if hasattr(self, attr):\n                value = getattr(self, attr)\n                if value is not None:\n                    # Handle Verbosity enum serialization\n                    if hasattr(value, \"to_json_serializable\"):\n                        debug_data[attr] = value.to_json_serializable()\n                    elif hasattr(value, \"name\"):  # Other enums\n                        debug_data[attr] = f\"{value.name} ({value.value})\"\n                    else:\n                        debug_data[attr] = (\n                            str(value)\n                            if not isinstance(value, (int, float, bool))\n                            else value\n                        )\n\n        # Add parser info\n        if hasattr(self, \"parser\") and self.parser:\n            parser_name = getattr(self.parser, \"pydantic_model\", {})\n            if hasattr(parser_name, \"__name__\"):\n                debug_data[\"parser\"] = parser_name.__name__\n\n        # Add client params if available\n        if hasattr(self, \"client_params\") and self.client_params:\n            debug_data[\"client_params\"] = safe_json_serialize(self.client_params)\n\n        # Make the entire debug_data object safe for JSON serialization\n        safe_debug_data = safe_json_serialize(debug_data)\n\n        json_content = json.dumps(safe_debug_data, indent=2, ensure_ascii=False)\n        syntax = Syntax(json_content, \"json\", line_numbers=True, theme=\"monokai\")\n\n        return Panel(\n            syntax,\n            title=\"\ud83d\udc1b FULL DEBUG REQUEST\",\n            title_align=\"left\",\n            subtitle=f\"[dim]{datetime.now().strftime('%H:%M:%S')}[/dim]\",\n            subtitle_align=\"right\",\n        )\n\n\nclass PlainDisplayParamsMixin(PlainDisplayMixin):\n    \"\"\"Plain text display mixin for Params objects.\"\"\"\n\n    def _to_plain_impl(self, verbosity: Verbosity) -&gt; str:\n        \"\"\"Format Params object for plain text display.\"\"\"\n        if verbosity == Verbosity.SUMMARY:\n            return self._format_params_summary_plain()\n        elif verbosity == Verbosity.DETAILED:\n            return self._format_params_detailed_plain()\n        elif verbosity == Verbosity.COMPLETE:\n            return self._format_params_complete_plain()\n        elif verbosity == Verbosity.DEBUG:\n            return self._format_params_debug_plain()\n        else:\n            return \"\"\n\n    def _format_params_summary_plain(self) -&gt; str:\n        \"\"\"Format basic request info for SUMMARY level.\"\"\"\n        from datetime import datetime\n\n        lines = []\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n\n        # Header\n        lines.append(f\"[{timestamp}] REQUEST {getattr(self, 'model', 'unknown')}\")\n\n        # Show user message\n        if hasattr(self, \"messages\") and self.messages:\n            for msg in self.messages:\n                if hasattr(msg, \"role\") and msg.role == \"user\":\n                    lines.append(f\"user: {str(msg.content)}\")\n        elif hasattr(self, \"query_input\") and self.query_input:\n            lines.append(f\"user: {str(self.query_input)}\")\n\n        # Show parameters\n        params_line = []\n        if hasattr(self, \"temperature\") and self.temperature is not None:\n            params_line.append(f\"Temperature: {self.temperature}\")\n        if hasattr(self, \"parser\") and self.parser:\n            parser_name = getattr(self.parser, \"pydantic_model\", {})\n            if hasattr(parser_name, \"__name__\"):\n                params_line.append(f\"Parser: {parser_name.__name__}\")\n\n        if params_line:\n            lines.append(\" \u2022 \".join(params_line))\n\n        return \"\\n\".join(lines)\n\n    def _format_params_detailed_plain(self) -&gt; str:\n        \"\"\"Format truncated messages for DETAILED level.\"\"\"\n        from datetime import datetime\n\n        lines = []\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n\n        # Header\n        lines.append(\n            f\"[{timestamp}] REQUEST {getattr(self, 'model', 'unknown')} (Detailed)\"\n        )\n        lines.append(\"Messages:\")\n\n        # Show messages with truncation\n        if hasattr(self, \"messages\") and self.messages:\n            for msg in self.messages:\n                if hasattr(msg, \"role\") and hasattr(msg, \"content\"):\n                    content = str(msg.content)\n                    if len(content) &gt; 100:\n                        content = content[:100] + \"...\"\n                    lines.append(f\"  {msg.role}: {content}\")\n        elif hasattr(self, \"query_input\") and self.query_input:\n            content = str(self.query_input)\n            if len(content) &gt; 100:\n                content = content[:100] + \"...\"\n            lines.append(f\"  user: {content}\")\n\n        # Show parameters\n        params_line = []\n        if hasattr(self, \"temperature\") and self.temperature is not None:\n            params_line.append(f\"Temperature: {self.temperature}\")\n        if hasattr(self, \"parser\") and self.parser:\n            parser_name = getattr(self.parser, \"pydantic_model\", {})\n            if hasattr(parser_name, \"__name__\"):\n                params_line.append(f\"Parser: {parser_name.__name__}\")\n\n        if params_line:\n            lines.append(\"Parameters: \" + \" \u2022 \".join(params_line))\n\n        return \"\\n\".join(lines)\n\n    def _format_params_complete_plain(self) -&gt; str:\n        \"\"\"Format complete messages for COMPLETE level.\"\"\"\n        from datetime import datetime\n\n        lines = []\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n\n        # Header\n        lines.append(\n            f\"[{timestamp}] REQUEST {getattr(self, 'model', 'unknown')} (Complete)\"\n        )\n        lines.append(\"Complete Messages:\")\n\n        # Show full messages\n        if hasattr(self, \"messages\") and self.messages:\n            for msg in self.messages:\n                if hasattr(msg, \"role\") and hasattr(msg, \"content\"):\n                    content = str(msg.content)\n                    lines.append(f\"  {msg.role}: {content}\")\n        elif hasattr(self, \"query_input\") and self.query_input:\n            content = str(self.query_input)\n            lines.append(f\"  user: {content}\")\n\n        # Show all parameters\n        params_line = []\n        if hasattr(self, \"temperature\") and self.temperature is not None:\n            params_line.append(f\"Temperature: {self.temperature}\")\n        if hasattr(self, \"parser\") and self.parser:\n            parser_name = getattr(self.parser, \"pydantic_model\", {})\n            if hasattr(parser_name, \"__name__\"):\n                params_line.append(f\"Parser: {parser_name.__name__}\")\n        if hasattr(self, \"stream\"):\n            params_line.append(f\"Stream: {self.stream}\")\n\n        if params_line:\n            lines.append(\"Parameters: \" + \" \u2022 \".join(params_line))\n\n        return \"\\n\".join(lines)\n\n    def _format_params_debug_plain(self) -&gt; str:\n        \"\"\"Format full JSON debug for DEBUG level.\"\"\"\n        from datetime import datetime\n        import json\n\n        lines = []\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n\n        # Header\n        lines.append(f\"[{timestamp}] DEBUG REQUEST {getattr(self, 'model', 'unknown')}\")\n\n        # Create debug dictionary\n        debug_data = {\n            \"model\": getattr(self, \"model\", \"unknown\"),\n            \"timestamp\": datetime.now().isoformat(),\n        }\n\n        # Add messages\n        if hasattr(self, \"messages\") and self.messages:\n            debug_data[\"messages\"] = []\n            for msg in self.messages:\n                if hasattr(msg, \"role\") and hasattr(msg, \"content\"):\n                    debug_data[\"messages\"].append(\n                        {\"role\": str(msg.role), \"content\": str(msg.content)}\n                    )\n        elif hasattr(self, \"query_input\") and self.query_input:\n            debug_data[\"messages\"] = [\n                {\"role\": \"user\", \"content\": str(self.query_input)}\n            ]\n\n        # Add all other parameters\n        for attr in [\"temperature\", \"stream\", \"verbose\"]:\n            if hasattr(self, attr):\n                value = getattr(self, attr)\n                if value is not None:\n                    # Handle Verbosity enum serialization\n                    if hasattr(value, \"to_json_serializable\"):\n                        debug_data[attr] = value.to_json_serializable()\n                    elif hasattr(value, \"name\"):  # Other enums\n                        debug_data[attr] = f\"{value.name} ({value.value})\"\n                    else:\n                        debug_data[attr] = (\n                            str(value)\n                            if not isinstance(value, (int, float, bool))\n                            else value\n                        )\n\n        # Add parser info\n        if hasattr(self, \"parser\") and self.parser:\n            parser_name = getattr(self.parser, \"pydantic_model\", {})\n            if hasattr(parser_name, \"__name__\"):\n                debug_data[\"parser\"] = parser_name.__name__\n\n        # Add client params if available\n        if hasattr(self, \"client_params\") and self.client_params:\n            debug_data[\"client_params\"] = safe_json_serialize(self.client_params)\n\n        # Make the entire debug_data object safe for JSON serialization\n        safe_debug_data = safe_json_serialize(debug_data)\n\n        # Format as JSON\n        json_content = json.dumps(safe_debug_data, indent=2, ensure_ascii=False)\n        lines.append(\"Complete Parameters:\")\n        lines.append(json_content)\n\n        return \"\\n\".join(lines)\n\n\nclass RichDisplayResponseMixin(RichDisplayMixin):\n    \"\"\"Rich display mixin for Response objects.\"\"\"\n\n    def _to_rich_impl(self, verbosity: Verbosity) -&gt; \"RenderableType\":\n        \"\"\"Format Response object for Rich console display.\"\"\"\n        from rich.panel import Panel\n        from rich.syntax import Syntax\n        from rich.text import Text\n\n        if verbosity == Verbosity.SUMMARY:\n            return self._format_response_summary_rich()\n        elif verbosity == Verbosity.DETAILED:\n            return self._format_response_detailed_rich()\n        elif verbosity == Verbosity.COMPLETE:\n            return self._format_response_complete_rich()\n        elif verbosity == Verbosity.DEBUG:\n            return self._format_response_debug_rich()\n        else:\n            return Text(\"\")\n\n    def _format_response_summary_rich(self) -&gt; \"Panel\":\n        \"\"\"Format basic response info for SUMMARY level.\"\"\"\n        from rich.panel import Panel\n        from rich.text import Text\n        from datetime import datetime\n\n        content = Text()\n        duration = getattr(self, \"duration\", 0)\n\n        # Show response content (truncated)\n        response_content = str(getattr(self, \"content\", \"No content\"))\n        if len(response_content) &gt; 100:\n            response_content = response_content[:100] + \"...\"\n\n        content.append(response_content, style=\"blue\")\n\n        # Show metadata if available\n        if hasattr(self, \"model\"):\n            content.append(f\"\\nModel: {self.model}\", style=\"dim\")\n\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        return Panel(\n            content,\n            title=f\"\u2713 RESPONSE {duration:.1f}s\",\n            title_align=\"left\",\n            subtitle=f\"[dim]{timestamp}[/dim]\",\n            subtitle_align=\"right\",\n        )\n\n    def _format_response_detailed_rich(self) -&gt; \"Panel\":\n        \"\"\"Format truncated response for DETAILED level.\"\"\"\n        from rich.panel import Panel\n        from rich.text import Text\n        from datetime import datetime\n\n        content = Text()\n        duration = getattr(self, \"duration\", 0)\n\n        # NEW: Show user prompt first at DETAILED level and above\n        if hasattr(self, \"request\"):\n            user_prompt = extract_user_prompt(self.request)\n            if user_prompt:\n                content.append(\"User: \", style=\"bold yellow\")\n                # Truncate user prompt to ~200 chars for detailed view\n                if len(user_prompt) &gt; 200:\n                    user_prompt = user_prompt[:200] + \"...\"\n                content.append(f\"{user_prompt}\\n\\n\", style=\"yellow\")\n\n        # Show response content (truncated to ~200 chars for detailed)\n        content.append(\"Assistant: \", style=\"bold blue\")\n        response_content = str(getattr(self, \"content\", \"No content\"))\n        if len(response_content) &gt; 200:\n            response_content = response_content[:200] + \"...\"\n        content.append(response_content, style=\"blue\")\n\n        # Show metadata\n        content.append(\"\\n\", style=\"\")\n        metadata_lines = []\n        if hasattr(self, \"model\"):\n            metadata_lines.append(f\"Model: {self.model}\")\n        if hasattr(self, \"request\") and self.request:\n            if (\n                hasattr(self.request, \"temperature\")\n                and self.request.temperature is not None\n            ):\n                metadata_lines.append(f\"Temperature: {self.request.temperature}\")\n\n        if metadata_lines:\n            content.append(\" \u2022 \".join(metadata_lines), style=\"dim\")\n\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        return Panel(\n            content,\n            title=f\"\u2713 CONVERSATION {duration:.1f}s\",\n            title_align=\"left\",\n            subtitle=f\"[dim]{timestamp}[/dim]\",\n            subtitle_align=\"right\",\n        )\n\n    def _format_response_complete_rich(self) -&gt; \"Panel\":\n        \"\"\"Format complete response for COMPLETE level.\"\"\"\n        from rich.panel import Panel\n        from rich.text import Text\n        from datetime import datetime\n\n        content = Text()\n        duration = getattr(self, \"duration\", 0)\n\n        # NEW: Show full user prompt at COMPLETE level\n        if hasattr(self, \"request\"):\n            user_prompt = extract_user_prompt(self.request)\n            if user_prompt:\n                content.append(\"User: \", style=\"bold yellow\")\n                content.append(f\"{user_prompt}\\n\\n\", style=\"yellow\")\n\n        # Show full response content (no truncation)\n        content.append(\"Assistant: \", style=\"bold blue\")\n        response_content = str(getattr(self, \"content\", \"No content\"))\n        content.append(response_content, style=\"blue\")\n\n        # Show detailed metadata\n        content.append(\"\\n\\n\", style=\"\")\n        metadata_lines = []\n        if hasattr(self, \"model\"):\n            metadata_lines.append(f\"Model: {self.model}\")\n        if hasattr(self, \"request\") and self.request:\n            if (\n                hasattr(self.request, \"temperature\")\n                and self.request.temperature is not None\n            ):\n                metadata_lines.append(f\"Temperature: {self.request.temperature}\")\n            if hasattr(self.request, \"response_model\") and self.request.response_model:\n                parser_name = getattr(\n                    self.request.response_model,\n                    \"__name__\",\n                    str(self.request.response_model),\n                )\n                metadata_lines.append(f\"Parser: {parser_name}\")\n\n        if hasattr(self, \"timestamp\"):\n            metadata_lines.append(f\"Timestamp: {self.timestamp}\")\n\n        if metadata_lines:\n            content.append(\" \u2022 \".join(metadata_lines), style=\"dim\")\n\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        return Panel(\n            content,\n            title=f\"\u2713 FULL CONVERSATION {duration:.1f}s\",\n            title_align=\"left\",\n            subtitle=f\"[dim]{timestamp}[/dim]\",\n            subtitle_align=\"right\",\n        )\n\n    def _format_response_debug_rich(self) -&gt; \"Panel\":\n        \"\"\"Format full JSON debug for DEBUG level.\"\"\"\n        from rich.panel import Panel\n        from rich.syntax import Syntax\n        from datetime import datetime\n        import json\n\n        duration = getattr(self, \"duration\", 0)\n\n        # Create debug dictionary with conversation context\n        debug_data = {\n            \"type\": \"ConversationDebug\",\n            \"timestamp\": datetime.now().isoformat(),\n            \"duration\": duration,\n            \"model\": getattr(self, \"model\", \"unknown\"),\n        }\n\n        # Add user prompt\n        if hasattr(self, \"request\"):\n            user_prompt = extract_user_prompt(self.request)\n            if user_prompt:\n                debug_data[\"user_prompt\"] = user_prompt\n\n        # Add response content\n        content = getattr(self, \"content\", None)\n        if content is not None:\n            try:\n                if hasattr(content, \"model_dump\"):\n                    debug_data[\"assistant_response\"] = content.model_dump()\n                elif hasattr(content, \"__dict__\"):\n                    debug_data[\"assistant_response\"] = content.__dict__\n                else:\n                    debug_data[\"assistant_response\"] = str(content)\n            except:\n                debug_data[\"assistant_response\"] = str(content)\n\n        # Add full request if available\n        if hasattr(self, \"request\") and self.request:\n            debug_data[\"full_request\"] = safe_json_serialize(self.request)\n\n        # Make the entire debug_data object safe for JSON serialization\n        safe_debug_data = safe_json_serialize(debug_data)\n\n        json_content = json.dumps(safe_debug_data, indent=2, ensure_ascii=False)\n        syntax = Syntax(json_content, \"json\", line_numbers=True, theme=\"monokai\")\n\n        return Panel(\n            syntax,\n            title=f\"\ud83d\udc1b CONVERSATION DEBUG {duration:.1f}s\",\n            title_align=\"left\",\n            subtitle=f\"[dim]{datetime.now().strftime('%H:%M:%S')}[/dim]\",\n            subtitle_align=\"right\",\n        )\n\n\nclass PlainDisplayResponseMixin(PlainDisplayMixin):\n    \"\"\"Plain text display mixin for Response objects.\"\"\"\n\n    def _to_plain_impl(self, verbosity: Verbosity) -&gt; str:\n        \"\"\"Format Response object for plain text display.\"\"\"\n        if verbosity == Verbosity.SUMMARY:\n            return self._format_response_summary_plain()\n        elif verbosity == Verbosity.DETAILED:\n            return self._format_response_detailed_plain()\n        elif verbosity == Verbosity.COMPLETE:\n            return self._format_response_complete_plain()\n        elif verbosity == Verbosity.DEBUG:\n            return self._format_response_debug_plain()\n        else:\n            return \"\"\n\n    def _format_response_summary_plain(self) -&gt; str:\n        \"\"\"Format basic response info for SUMMARY level.\"\"\"\n        from datetime import datetime\n\n        lines = []\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        duration = getattr(self, \"duration\", 0)\n\n        # Header\n        lines.append(f\"[{timestamp}] RESPONSE {duration:.1f}s\")\n\n        # Show response content (truncated)\n        response_content = str(getattr(self, \"content\", \"No content\"))\n        if len(response_content) &gt; 100:\n            response_content = response_content[:100] + \"...\"\n        lines.append(response_content)\n\n        return \"\\n\".join(lines)\n\n    def _format_response_detailed_plain(self) -&gt; str:\n        \"\"\"Format truncated response for DETAILED level.\"\"\"\n        from datetime import datetime\n\n        lines = []\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        duration = getattr(self, \"duration\", 0)\n\n        # Header\n        lines.append(f\"[{timestamp}] CONVERSATION {duration:.1f}s (Detailed)\")\n\n        # NEW: Show user prompt\n        if hasattr(self, \"request\"):\n            user_prompt = extract_user_prompt(self.request)\n            if user_prompt:\n                if len(user_prompt) &gt; 200:\n                    user_prompt = user_prompt[:200] + \"...\"\n                lines.append(f\"User: {user_prompt}\")\n\n        # Show response content (truncated to ~200 chars)\n        response_content = str(getattr(self, \"content\", \"No content\"))\n        if len(response_content) &gt; 200:\n            response_content = response_content[:200] + \"...\"\n        lines.append(f\"Assistant: {response_content}\")\n\n        # Show metadata\n        metadata_lines = []\n        if hasattr(self, \"model\"):\n            metadata_lines.append(f\"Model: {self.model}\")\n        if hasattr(self, \"request\") and self.request:\n            if (\n                hasattr(self.request, \"temperature\")\n                and self.request.temperature is not None\n            ):\n                metadata_lines.append(f\"Temperature: {self.request.temperature}\")\n\n        if metadata_lines:\n            lines.append(\"Metadata: \" + \" \u2022 \".join(metadata_lines))\n\n        return \"\\n\".join(lines)\n\n    def _format_response_complete_plain(self) -&gt; str:\n        \"\"\"Format complete response for COMPLETE level.\"\"\"\n        from datetime import datetime\n\n        lines = []\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        duration = getattr(self, \"duration\", 0)\n\n        # Header\n        lines.append(f\"[{timestamp}] FULL CONVERSATION {duration:.1f}s\")\n\n        # NEW: Show full user prompt\n        if hasattr(self, \"request\"):\n            user_prompt = extract_user_prompt(self.request)\n            if user_prompt:\n                lines.append(f\"User: {user_prompt}\")\n\n        # Show full response content\n        response_content = str(getattr(self, \"content\", \"No content\"))\n        lines.append(f\"Assistant: {response_content}\")\n\n        # Show detailed metadata\n        metadata_lines = []\n        if hasattr(self, \"model\"):\n            metadata_lines.append(f\"Model: {self.model}\")\n        if hasattr(self, \"request\") and self.request:\n            if (\n                hasattr(self.request, \"temperature\")\n                and self.request.temperature is not None\n            ):\n                metadata_lines.append(f\"Temperature: {self.request.temperature}\")\n            if hasattr(self.request, \"response_model\") and self.request.response_model:\n                parser_name = getattr(\n                    self.request.response_model,\n                    \"__name__\",\n                    str(self.request.response_model),\n                )\n                metadata_lines.append(f\"Parser: {parser_name}\")\n\n        if hasattr(self, \"timestamp\"):\n            metadata_lines.append(f\"Timestamp: {self.timestamp}\")\n\n        if metadata_lines:\n            lines.append(\"Metadata: \" + \" \u2022 \".join(metadata_lines))\n\n        return \"\\n\".join(lines)\n\n    def _format_response_debug_plain(self) -&gt; str:\n        \"\"\"Format full JSON debug for DEBUG level.\"\"\"\n        from datetime import datetime\n        import json\n\n        lines = []\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        duration = getattr(self, \"duration\", 0)\n\n        # Header\n        lines.append(f\"[{timestamp}] CONVERSATION DEBUG {duration:.1f}s\")\n\n        # Create debug dictionary\n        debug_data = {\n            \"type\": \"ConversationDebug\",\n            \"timestamp\": datetime.now().isoformat(),\n            \"duration\": duration,\n            \"model\": getattr(self, \"model\", \"unknown\"),\n        }\n\n        # Add user prompt\n        if hasattr(self, \"request\"):\n            user_prompt = extract_user_prompt(self.request)\n            if user_prompt:\n                debug_data[\"user_prompt\"] = user_prompt\n\n        # Add response content\n        content = getattr(self, \"content\", None)\n        if content is not None:\n            try:\n                if hasattr(content, \"model_dump\"):\n                    debug_data[\"assistant_response\"] = content.model_dump()\n                elif hasattr(content, \"__dict__\"):\n                    debug_data[\"assistant_response\"] = content.__dict__\n                else:\n                    debug_data[\"assistant_response\"] = str(content)\n            except:\n                debug_data[\"assistant_response\"] = str(content)\n\n        # Add full request if available\n        if hasattr(self, \"request\") and self.request:\n            debug_data[\"full_request\"] = safe_json_serialize(self.request)\n\n        # Make the entire debug_data object safe for JSON serialization\n        safe_debug_data = safe_json_serialize(debug_data)\n\n        # Format as JSON\n        json_content = json.dumps(safe_debug_data, indent=2, ensure_ascii=False)\n        lines.append(\"Complete Conversation Object:\")\n        lines.append(json_content)\n\n        return \"\\n\".join(lines)\n\n\nclass RichDisplayConduitErrorMixin(RichDisplayMixin):\n    \"\"\"Rich display mixin for ConduitError objects.\"\"\"\n\n    def _to_rich_impl(self, verbosity: Verbosity) -&gt; \"RenderableType\":\n        \"\"\"Format ConduitError object for Rich console display.\"\"\"\n        from rich.panel import Panel\n        from rich.syntax import Syntax\n        from rich.text import Text\n\n        if verbosity == Verbosity.SUMMARY:\n            return self._format_error_summary_rich()\n        elif verbosity == Verbosity.DETAILED:\n            return self._format_error_detailed_rich()\n        elif verbosity == Verbosity.COMPLETE:\n            return self._format_error_complete_rich()\n        elif verbosity == Verbosity.DEBUG:\n            return self._format_error_debug_rich()\n        else:\n            return Text(\"\")\n\n    def _format_error_summary_rich(self) -&gt; \"Panel\":\n        \"\"\"Format basic error info for SUMMARY level.\"\"\"\n        from rich.panel import Panel\n        from rich.text import Text\n        from datetime import datetime\n\n        content = Text()\n\n        # Show error info\n        if hasattr(self, \"info\"):\n            content.append(f\"Error: {self.info.code}\\n\", style=\"bold red\")\n            content.append(f\"{self.info.message}\\n\", style=\"red\")\n            content.append(f\"Category: {self.info.category}\", style=\"dim red\")\n        else:\n            content.append(\"Unknown error\", style=\"red\")\n\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        return Panel(\n            content,\n            title=\"\u2717 ERROR\",\n            title_align=\"left\",\n            subtitle=f\"[dim]{timestamp}[/dim]\",\n            subtitle_align=\"right\",\n            border_style=\"red\",\n        )\n\n    def _format_error_detailed_rich(self) -&gt; \"Panel\":\n        \"\"\"Format detailed error for DETAILED level.\"\"\"\n        from rich.panel import Panel\n        from rich.text import Text\n        from datetime import datetime\n\n        content = Text()\n\n        # Show error info with more details\n        if hasattr(self, \"info\"):\n            content.append(f\"Error Code: {self.info.code}\\n\", style=\"bold red\")\n            content.append(f\"Message: {self.info.message}\\n\", style=\"red\")\n            content.append(f\"Category: {self.info.category}\\n\", style=\"dim red\")\n            content.append(f\"Timestamp: {self.info.timestamp}\\n\", style=\"dim\")\n        else:\n            content.append(\"Unknown error\", style=\"red\")\n\n        # Show additional detail if available\n        if hasattr(self, \"detail\") and self.detail:\n            content.append(\n                f\"\\nException Type: {self.detail.exception_type}\", style=\"dim red\"\n            )\n            if self.detail.request_params:\n                content.append(\n                    f\"\\nRequest had {len(self.detail.request_params)} parameters\",\n                    style=\"dim\",\n                )\n\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        return Panel(\n            content,\n            title=\"\u2717 ERROR (Detailed)\",\n            title_align=\"left\",\n            subtitle=f\"[dim]{timestamp}[/dim]\",\n            subtitle_align=\"right\",\n            border_style=\"red\",\n        )\n\n    def _format_error_complete_rich(self) -&gt; \"Panel\":\n        \"\"\"Format complete error for COMPLETE level.\"\"\"\n        from rich.panel import Panel\n        from rich.text import Text\n        from datetime import datetime\n\n        content = Text()\n\n        # Show comprehensive error info\n        if hasattr(self, \"info\"):\n            content.append(f\"Error Code: {self.info.code}\\n\", style=\"bold red\")\n            content.append(f\"Message: {self.info.message}\\n\", style=\"red\")\n            content.append(f\"Category: {self.info.category}\\n\", style=\"dim red\")\n            content.append(f\"Timestamp: {self.info.timestamp}\\n\", style=\"dim\")\n        else:\n            content.append(\"Unknown error\\n\", style=\"red\")\n\n        # Show complete detail if available\n        if hasattr(self, \"detail\") and self.detail:\n            content.append(f\"\\nException Details:\\n\", style=\"bold\")\n            content.append(f\"Type: {self.detail.exception_type}\\n\", style=\"dim red\")\n\n            if self.detail.request_params:\n                content.append(\n                    f\"Request Parameters: {len(self.detail.request_params)} items\\n\",\n                    style=\"dim\",\n                )\n\n            if self.detail.retry_count is not None:\n                content.append(f\"Retry Count: {self.detail.retry_count}\\n\", style=\"dim\")\n\n            # Show truncated stack trace if available\n            if self.detail.stack_trace:\n                stack_lines = self.detail.stack_trace.split(\"\\n\")\n                # Show first few and last few lines\n                if len(stack_lines) &gt; 10:\n                    shown_lines = (\n                        stack_lines[:3] + [\"  ... (truncated) ...\"] + stack_lines[-3:]\n                    )\n                else:\n                    shown_lines = stack_lines\n                content.append(\"\\nStack Trace (truncated):\\n\", style=\"bold dim\")\n                content.append(\"\\n\".join(shown_lines), style=\"dim red\")\n\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        return Panel(\n            content,\n            title=\"\u2717 ERROR (Complete)\",\n            title_align=\"left\",\n            subtitle=f\"[dim]{timestamp}[/dim]\",\n            subtitle_align=\"right\",\n            border_style=\"red\",\n        )\n\n    def _format_error_debug_rich(self) -&gt; \"Panel\":\n        \"\"\"Format full JSON debug for DEBUG level.\"\"\"\n        from rich.panel import Panel\n        from rich.syntax import Syntax\n        from datetime import datetime\n        import json\n\n        # Create debug dictionary with all error information\n        debug_data = {\n            \"type\": \"ConduitError\",\n            \"timestamp\": datetime.now().isoformat(),\n        }\n\n        # Add error info\n        if hasattr(self, \"info\"):\n            debug_data[\"info\"] = {\n                \"code\": self.info.code,\n                \"message\": self.info.message,\n                \"category\": self.info.category,\n                \"timestamp\": self.info.timestamp.isoformat()\n                if hasattr(self.info.timestamp, \"isoformat\")\n                else str(self.info.timestamp),\n            }\n\n        # Add error detail\n        if hasattr(self, \"detail\") and self.detail:\n            debug_data[\"detail\"] = {\n                \"exception_type\": self.detail.exception_type,\n                \"has_stack_trace\": bool(self.detail.stack_trace),\n                \"stack_trace_lines\": len(self.detail.stack_trace.split(\"\\n\"))\n                if self.detail.stack_trace\n                else 0,\n                \"request_params\": self.detail.request_params,\n                \"retry_count\": self.detail.retry_count,\n                \"raw_response\": str(self.detail.raw_response)\n                if self.detail.raw_response\n                else None,\n            }\n\n            # Include full stack trace in debug mode\n            if self.detail.stack_trace:\n                debug_data[\"detail\"][\"full_stack_trace\"] = self.detail.stack_trace\n\n        # Make the entire debug_data object safe for JSON serialization\n        safe_debug_data = safe_json_serialize(debug_data)\n\n        json_content = json.dumps(safe_debug_data, indent=2, ensure_ascii=False)\n        syntax = Syntax(json_content, \"json\", line_numbers=True, theme=\"monokai\")\n\n        return Panel(\n            syntax,\n            title=\"\ud83d\udc1b FULL DEBUG ERROR\",\n            title_align=\"left\",\n            subtitle=f\"[dim]{datetime.now().strftime('%H:%M:%S')}[/dim]\",\n            subtitle_align=\"right\",\n            border_style=\"red\",\n        )\n\n\nclass PlainDisplayConduitErrorMixin(PlainDisplayMixin):\n    \"\"\"Plain text display mixin for ConduitError objects.\"\"\"\n\n    def _to_plain_impl(self, verbosity: Verbosity) -&gt; str:\n        \"\"\"Format ConduitError object for plain text display.\"\"\"\n        if verbosity == Verbosity.SUMMARY:\n            return self._format_error_summary_plain()\n        elif verbosity == Verbosity.DETAILED:\n            return self._format_error_detailed_plain()\n        elif verbosity == Verbosity.COMPLETE:\n            return self._format_error_complete_plain()\n        elif verbosity == Verbosity.DEBUG:\n            return self._format_error_debug_plain()\n        else:\n            return \"\"\n\n    def _format_error_summary_plain(self) -&gt; str:\n        \"\"\"Format basic error info for SUMMARY level.\"\"\"\n        from datetime import datetime\n\n        lines = []\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n\n        # Header\n        lines.append(f\"[{timestamp}] ERROR\")\n\n        # Show error info\n        if hasattr(self, \"info\"):\n            lines.append(f\"Error: {self.info.code}\")\n            lines.append(f\"{self.info.message}\")\n            lines.append(f\"Category: {self.info.category}\")\n        else:\n            lines.append(\"Unknown error\")\n\n        return \"\\n\".join(lines)\n\n    def _format_error_detailed_plain(self) -&gt; str:\n        \"\"\"Format detailed error for DETAILED level.\"\"\"\n        from datetime import datetime\n\n        lines = []\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n\n        # Header\n        lines.append(f\"[{timestamp}] ERROR (Detailed)\")\n\n        # Show error info with more details\n        if hasattr(self, \"info\"):\n            lines.append(f\"Error Code: {self.info.code}\")\n            lines.append(f\"Message: {self.info.message}\")\n            lines.append(f\"Category: {self.info.category}\")\n            lines.append(f\"Timestamp: {self.info.timestamp}\")\n        else:\n            lines.append(\"Unknown error\")\n\n        # Show additional detail if available\n        if hasattr(self, \"detail\") and self.detail:\n            lines.append(f\"Exception Type: {self.detail.exception_type}\")\n            if self.detail.request_params:\n                lines.append(\n                    f\"Request had {len(self.detail.request_params)} parameters\"\n                )\n\n        return \"\\n\".join(lines)\n\n    def _format_error_complete_plain(self) -&gt; str:\n        \"\"\"Format complete error for COMPLETE level.\"\"\"\n        from datetime import datetime\n\n        lines = []\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n\n        # Header\n        lines.append(f\"[{timestamp}] ERROR (Complete)\")\n\n        # Show comprehensive error info\n        if hasattr(self, \"info\"):\n            lines.append(f\"Error Code: {self.info.code}\")\n            lines.append(f\"Message: {self.info.message}\")\n            lines.append(f\"Category: {self.info.category}\")\n            lines.append(f\"Timestamp: {self.info.timestamp}\")\n        else:\n            lines.append(\"Unknown error\")\n\n        # Show complete detail if available\n        if hasattr(self, \"detail\") and self.detail:\n            lines.append(\"\\nException Details:\")\n            lines.append(f\"Type: {self.detail.exception_type}\")\n\n            if self.detail.request_params:\n                lines.append(\n                    f\"Request Parameters: {len(self.detail.request_params)} items\"\n                )\n\n            if self.detail.retry_count is not None:\n                lines.append(f\"Retry Count: {self.detail.retry_count}\")\n\n            # Show truncated stack trace if available\n            if self.detail.stack_trace:\n                lines.append(\"\\nStack Trace (truncated):\")\n                stack_lines = self.detail.stack_trace.split(\"\\n\")\n                # Show first few and last few lines\n                if len(stack_lines) &gt; 10:\n                    shown_lines = (\n                        stack_lines[:3] + [\"  ... (truncated) ...\"] + stack_lines[-3:]\n                    )\n                else:\n                    shown_lines = stack_lines\n                lines.extend(shown_lines)\n\n        return \"\\n\".join(lines)\n\n    def _format_error_debug_plain(self) -&gt; str:\n        \"\"\"Format full JSON debug for DEBUG level.\"\"\"\n        from datetime import datetime\n        import json\n\n        lines = []\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n\n        # Header\n        lines.append(f\"[{timestamp}] DEBUG ERROR\")\n\n        # Create debug dictionary\n        debug_data = {\n            \"type\": \"ConduitError\",\n            \"timestamp\": datetime.now().isoformat(),\n        }\n\n        # Add error info\n        if hasattr(self, \"info\"):\n            debug_data[\"info\"] = {\n                \"code\": self.info.code,\n                \"message\": self.info.message,\n                \"category\": self.info.category,\n                \"timestamp\": self.info.timestamp.isoformat()\n                if hasattr(self.info.timestamp, \"isoformat\")\n                else str(self.info.timestamp),\n            }\n\n        # Add error detail\n        if hasattr(self, \"detail\") and self.detail:\n            debug_data[\"detail\"] = {\n                \"exception_type\": self.detail.exception_type,\n                \"has_stack_trace\": bool(self.detail.stack_trace),\n                \"stack_trace_lines\": len(self.detail.stack_trace.split(\"\\n\"))\n                if self.detail.stack_trace\n                else 0,\n                \"request_params\": self.detail.request_params,\n                \"retry_count\": self.detail.retry_count,\n                \"raw_response\": str(self.detail.raw_response)\n                if self.detail.raw_response\n                else None,\n            }\n\n            # Include full stack trace in debug mode\n            if self.detail.stack_trace:\n                debug_data[\"detail\"][\"full_stack_trace\"] = self.detail.stack_trace\n\n        # Make the entire debug_data object safe for JSON serialization\n        safe_debug_data = safe_json_serialize(debug_data)\n\n        # Format as JSON\n        json_content = json.dumps(safe_debug_data, indent=2, ensure_ascii=False)\n        lines.append(\"Complete Error Object:\")\n        lines.append(json_content)\n\n        return \"\\n\".join(lines)\n</file>\n    <file path=\"progress/wrappers.py\">\"\"\"\nEnhanced progress wrappers with verbosity support.\nMaintains backwards compatibility while adding new verbosity parameter.\n\"\"\"\n\nfrom conduit.logs.logging_config import get_logger\nfrom conduit.progress.verbosity import Verbosity\nfrom functools import wraps\nimport time, sys, inspect\n\nlogger = get_logger(__name__)\n\n\ndef extract_query_preview(input_data, max_length=30):\n    \"\"\"Extract a preview of the query for display purposes\"\"\"\n    if isinstance(input_data, str):\n        # Strip whitespace and replace newlines with spaces\n        preview = input_data.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n        return preview[:max_length] + \"...\" if len(preview) &gt; max_length else preview\n    elif isinstance(input_data, list):\n        # Find last message with role=\"user\"\n        for message in reversed(input_data):\n            if hasattr(message, \"role\") and message.role == \"user\":\n                content = message.content\n                # Handle Pydantic objects\n                if hasattr(content, \"model_dump_json\"):\n                    content = content.model_dump_json()\n                else:\n                    content = str(content)\n                # Strip whitespace and replace newlines with spaces\n                content = content.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n                return (\n                    content[:max_length] + \"...\"\n                    if len(content) &gt; max_length\n                    else content\n                )\n        return \"No user message found\"\n    else:\n        preview = str(input_data).strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n        return preview[:max_length] + \"...\"\n\n\ndef sync_wrapper(\n    model_instance,\n    func,\n    handler,\n    query_preview,\n    index,\n    total,\n    verbosity,\n    *args,\n    **kwargs,\n):\n    \"\"\"Synchronous wrapper for progress display with verbosity support\"\"\"\n    model_name = model_instance.model\n    display_preview = (\n        f\"[{index}/{total}] {query_preview}\" if index is not None else query_preview\n    )\n\n    # Show starting state (only for PROGRESS and above)\n    if verbosity &gt;= Verbosity.PROGRESS:\n        if hasattr(handler, \"show_spinner\"):\n            handler.show_spinner(model_name, display_preview, verbosity=verbosity)\n        else:\n            handler.emit_started(model_name, display_preview, verbosity=verbosity)\n\n    start_time = time.time()\n    try:\n        result = func(model_instance, *args, **kwargs)\n        duration = time.time() - start_time\n\n        # Only show completion for PROGRESS and above\n        if verbosity &gt;= Verbosity.PROGRESS:\n            # Check if result is an error\n            from conduit.result.error import ConduitError\n\n            if isinstance(result, ConduitError):\n                if hasattr(handler, \"show_failed\"):\n                    handler.show_failed(\n                        model_name,\n                        display_preview,\n                        str(result.info.message),\n                        verbosity=verbosity,\n                        error_obj=result,\n                    )\n                else:\n                    handler.emit_failed(\n                        model_name,\n                        display_preview,\n                        str(result.info.message),\n                        verbosity=verbosity,\n                    )\n            else:\n                # Check for cache hit (very fast response)\n                is_cache_hit = duration &lt; 0.05  # Less than 50ms indicates cache hit\n\n                if is_cache_hit:\n                    if hasattr(handler, \"show_cached\"):\n                        handler.show_cached(\n                            model_name, display_preview, duration, verbosity=verbosity\n                        )\n                    else:\n                        handler.emit_cached(\n                            model_name, display_preview, duration, verbosity=verbosity\n                        )\n                else:\n                    # Normal success case\n                    if hasattr(handler, \"show_complete\"):\n                        handler.show_complete(\n                            model_name,\n                            display_preview,\n                            duration,\n                            verbosity=verbosity,\n                            response_obj=result,\n                        )\n                    else:\n                        handler.emit_complete(\n                            model_name, display_preview, duration, verbosity=verbosity\n                        )\n\n        return result\n    except KeyboardInterrupt:\n        if verbosity &gt;= Verbosity.PROGRESS:\n            if hasattr(handler, \"show_canceled\"):\n                handler.show_canceled(model_name, display_preview, verbosity=verbosity)\n            else:\n                handler.emit_canceled(model_name, display_preview, verbosity=verbosity)\n\n        # Exit gracefully without stack trace\n        sys.exit(130)  # Standard exit code for Ctrl+C\n\n    except Exception as e:\n        if verbosity &gt;= Verbosity.PROGRESS:\n            if hasattr(handler, \"show_failed\"):\n                handler.show_failed(\n                    model_name, display_preview, str(e), verbosity=verbosity\n                )\n            else:\n                handler.emit_failed(\n                    model_name, display_preview, str(e), verbosity=verbosity\n                )\n        raise\n\n\nasync def async_wrapper(\n    model_instance, func, handler, query_preview, verbosity, *args, **kwargs\n):\n    \"\"\"Asynchronous wrapper for progress display with verbosity support\"\"\"\n    model_name = model_instance.model\n\n    # Show starting state (only for PROGRESS and above)\n    if verbosity &gt;= Verbosity.PROGRESS:\n        if hasattr(handler, \"show_spinner\"):\n            handler.show_spinner(model_name, query_preview, verbosity=verbosity)\n        else:\n            handler.emit_started(model_name, query_preview, verbosity=verbosity)\n\n    start_time = time.time()\n    try:\n        result = await func(model_instance, *args, **kwargs)\n        duration = time.time() - start_time\n\n        # Only show completion for PROGRESS and above\n        if verbosity &gt;= Verbosity.PROGRESS:\n            # Check if result is an error\n            from conduit.result.error import ConduitError\n\n            if isinstance(result, ConduitError):\n                if hasattr(handler, \"show_failed\"):\n                    handler.show_failed(\n                        model_name,\n                        query_preview,\n                        str(result.info.message),\n                        verbosity=verbosity,\n                        error_obj=result,\n                    )\n                else:\n                    handler.emit_failed(\n                        model_name,\n                        query_preview,\n                        str(result.info.message),\n                        verbosity=verbosity,\n                    )\n            else:\n                # Check for cache hit (very fast response)\n                is_cache_hit = duration &lt; 0.05  # Less than 50ms indicates cache hit\n\n                if is_cache_hit:\n                    if hasattr(handler, \"show_cached\"):\n                        handler.show_cached(\n                            model_name, query_preview, duration, verbosity=verbosity\n                        )\n                    else:\n                        handler.emit_cached(\n                            model_name, query_preview, duration, verbosity=verbosity\n                        )\n                else:\n                    # Normal success case\n                    if hasattr(handler, \"show_complete\"):\n                        handler.show_complete(\n                            model_name,\n                            query_preview,\n                            duration,\n                            verbosity=verbosity,\n                            response_obj=result,\n                        )\n                    else:\n                        handler.emit_complete(\n                            model_name, query_preview, duration, verbosity=verbosity\n                        )\n\n        return result\n\n    except KeyboardInterrupt:\n        if verbosity &gt;= Verbosity.PROGRESS:\n            if hasattr(handler, \"show_canceled\"):\n                handler.show_canceled(model_name, query_preview, verbosity=verbosity)\n            else:\n                handler.emit_canceled(model_name, query_preview, verbosity=verbosity)\n\n        # Exit gracefully without stack trace\n        sys.exit(130)  # Standard exit code for Ctrl+C\n\n    except Exception as e:\n        if verbosity &gt;= Verbosity.PROGRESS:\n            if hasattr(handler, \"show_failed\"):\n                handler.show_failed(\n                    model_name, query_preview, str(e), verbosity=verbosity\n                )\n            else:\n                handler.emit_failed(\n                    model_name, query_preview, str(e), verbosity=verbosity\n                )\n        raise\n\n\ndef progress_display(func):\n    \"\"\"\n    Decorator that adds progress display to Model.query() methods with verbosity support.\n    Automatically detects sync vs async and uses appropriate wrapper.\n    \"\"\"\n\n    @wraps(func)\n    def sync_decorator(self, *args, **kwargs):\n        # Extract and convert verbosity parameter\n        verbose_input = kwargs.pop(\"verbose\", True)\n        verbosity = Verbosity.from_input(verbose_input)\n\n        index = kwargs.pop(\"index\", None)\n        total = kwargs.pop(\"total\", None)\n\n        # Validate index/total parameters\n        if (index is None) != (total is None):\n            raise ValueError(\n                \"Must provide both 'index' and 'total' parameters or neither\"\n            )\n\n        # For SILENT, bypass all progress display\n        if verbosity == Verbosity.SILENT:\n            return func(self, *args, **kwargs)\n\n        # Extract query preview from first argument\n        query_preview = extract_query_preview(args[0] if args else \"\")\n\n        if self.console:\n            # Lazy import Rich components only when needed\n            from conduit.progress.handlers import RichProgressHandler\n\n            handler = RichProgressHandler(self.console)\n        else:\n            # Built-in PlainProgressHandler - no imports\n            from conduit.progress.handlers import PlainProgressHandler\n\n            handler = PlainProgressHandler()\n\n        return sync_wrapper(\n            self, func, handler, query_preview, index, total, verbosity, *args, **kwargs\n        )\n\n    @wraps(func)\n    async def async_decorator(self, *args, **kwargs):\n        # Extract and convert verbosity parameter\n        verbose_input = kwargs.pop(\"verbose\", True)\n        verbosity = Verbosity.from_input(verbose_input)\n\n        # For SILENT, bypass all progress display\n        if verbosity == Verbosity.SILENT:\n            return await func(self, *args, **kwargs)\n\n        # Extract query preview from first argument\n        query_preview = extract_query_preview(args[0] if args else \"\")\n\n        if self.console:\n            # Lazy import Rich components only when needed\n            from conduit.progress.handlers import RichProgressHandler\n\n            handler = RichProgressHandler(self.console)\n        else:\n            # Built-in PlainProgressHandler - no imports\n            from conduit.progress.handlers import PlainProgressHandler\n\n            handler = PlainProgressHandler()\n\n        return await async_wrapper(\n            self, func, handler, query_preview, verbosity, *args, **kwargs\n        )\n\n    # Return the appropriate decorator based on function type\n    if inspect.iscoroutinefunction(func):\n        return async_decorator\n    else:\n        return sync_decorator\n\n\nasync def concurrent_wrapper(operation, tracker):\n    \"\"\"Wrap individual async operations for concurrent tracking\"\"\"\n    try:\n        tracker.operation_started()\n        result = await operation\n        tracker.operation_completed()\n        return result\n    except Exception as e:\n        tracker.operation_failed()\n        raise  # Re-raise the exception\n\n\ndef create_concurrent_progress_tracker(console, total: int):\n    \"\"\"Factory function to create appropriate concurrent tracker\"\"\"\n    if console:\n        from conduit.progress.handlers import RichProgressHandler\n\n        handler = RichProgressHandler(console)\n    else:\n        from conduit.progress.handlers import PlainProgressHandler\n\n        handler = PlainProgressHandler()\n\n    from conduit.progress.tracker import ConcurrentTracker\n\n    return ConcurrentTracker(handler, total)\n</file>\n    <file path=\"progress/tracker.py\">from conduit.logs.logging_config import get_logger\nfrom datetime import datetime\nfrom pydantic import BaseModel\nfrom typing import Protocol\nfrom datetime import datetime\nimport time\n\nlogger = get_logger(__name__)\n\n\nclass ProgressEvent(BaseModel):\n    pass\n\n\nclass SyncEvent(ProgressEvent):\n    event_type: str  # \"started\", \"complete\", \"failed\", \"canceled\"\n    timestamp: datetime\n    model: str\n    query_preview: str  # First ~30 chars of the query\n    duration: float | None = None  # Duration in seconds\n    error: str | None = None  # Error message if any\n\n\nclass AsyncEvent(ProgressEvent):\n    request_id: int\n    event_type: str  # \"started\", \"complete\", \"failed\", \"canceled\"\n    timestamp: datetime\n    model: str\n    query_preview: str  # First ~30 chars of the query\n    duration: float | None = None  # Duration in seconds\n    error: str | None = None  # Error message if any\n\n\nclass ProgressHandler(Protocol):\n    def handle_event(self, event: ProgressEvent) -&gt; None:\n        \"\"\"Handle a progress event.\"\"\"\n        ...\n\n\nclass ProgressTracker:\n    def __init__(self, handler: ProgressHandler):\n        self.handler = handler\n\n    def emit_event(self, event: ProgressEvent):\n        self.handler.handle_event(event)\n\n\nclass ConcurrentSummaryEvent(ProgressEvent):\n    \"\"\"Event for concurrent operation summaries\"\"\"\n\n    event_type: str  # \"concurrent_start\", \"concurrent_complete\"\n    timestamp: datetime\n    total: int\n    successful: int = 0\n    failed: int = 0\n    duration: float | None = None\n\n\nclass ConcurrentTracker:\n    \"\"\"Tracks progress of concurrent operations with real-time updates\"\"\"\n\n    def __init__(self, handler, total: int):\n        self.handler = handler\n        self.total = total\n        self.completed = 0\n        self.failed = 0\n        self.start_time = time.time()\n\n    def emit_concurrent_start(self):\n        \"\"\"Emit start event for concurrent operations\"\"\"\n        event = ConcurrentSummaryEvent(\n            event_type=\"concurrent_start\", timestamp=datetime.now(), total=self.total\n        )\n        if hasattr(self.handler, \"handle_concurrent_start\"):\n            self.handler.handle_concurrent_start(self.total)\n        else:\n            # Fallback for handlers without concurrent support\n            if hasattr(self.handler, \"show_spinner\"):\n                self.handler.show_spinner(\n                    \"concurrent\", f\"Running {self.total} concurrent requests...\"\n                )\n            else:\n                self.handler.emit_started(\n                    \"concurrent\", f\"Starting: {self.total} concurrent requests\"\n                )\n\n    def operation_started(self):\n        \"\"\"Called when an individual operation starts\"\"\"\n        # For now, we don't need to track individual starts\n        # Could be extended later for more detailed progress\n        pass\n\n    def operation_completed(self):\n        \"\"\"Called when an individual operation completes successfully\"\"\"\n        self.completed += 1\n        self._update_progress()\n\n    def operation_failed(self):\n        \"\"\"Called when an individual operation fails\"\"\"\n        self.failed += 1\n        self._update_progress()\n\n    def _update_progress(self):\n        \"\"\"Update live progress display\"\"\"\n        if hasattr(self.handler, \"update_concurrent_progress\"):\n            elapsed = time.time() - self.start_time\n            running = self.total - self.completed - self.failed\n            self.handler.update_concurrent_progress(\n                completed=self.completed,\n                total=self.total,\n                running=running,\n                failed=self.failed,\n                elapsed=elapsed,\n            )\n\n    def emit_concurrent_complete(self):\n        \"\"\"Emit completion event for all concurrent operations\"\"\"\n        duration = time.time() - self.start_time\n        successful = self.completed\n\n        if hasattr(self.handler, \"handle_concurrent_complete\"):\n            self.handler.handle_concurrent_complete(successful, self.total, duration)\n        else:\n            # Fallback for handlers without concurrent support\n            if hasattr(self.handler, \"show_complete\"):\n                self.handler.show_complete(\n                    \"concurrent\",\n                    f\"All requests complete: {successful}/{self.total} successful\",\n                    duration,\n                )\n            else:\n                self.handler.emit_complete(\n                    \"concurrent\",\n                    f\"All requests complete: {successful}/{self.total} successful\",\n                    duration,\n                )\n</file>\n    <file path=\"progress/__init__.py\">\n</file>\n    <file path=\"progress/verbosity.py\">from enum import Enum\n\n\nclass Verbosity(Enum):\n    \"\"\"\n    SILENT - Obviously nothing shown\n    PROGRESS - Just the spinner/completion (your current default)\n    SUMMARY - Basic request/response info (level 2 in your spec)\n    DETAILED - Truncated messages in panels (level 3)\n    COMPLETE - Full messages in panels (level 4)\n    DEBUG - Full JSON with syntax highlighting (level 5)\n    \"\"\"\n\n    SILENT = 0\n    PROGRESS = 1\n    SUMMARY = 2\n    DETAILED = 3\n    COMPLETE = 4\n    DEBUG = 5\n\n    @classmethod\n    def from_input(cls, value):\n        \"\"\"\n        Converts various input types to a Verbosity instance.\n        \"\"\"\n        if value is False:\n            return cls.SILENT\n        elif value is True:\n            return cls.PROGRESS\n        elif isinstance(value, cls):\n            return value\n        elif isinstance(value, str):\n            # Map string values to enum members\n            string_map = {\n                \"\": cls.SILENT,\n                \"v\": cls.PROGRESS,\n                \"vv\": cls.SUMMARY,\n                \"vvv\": cls.DETAILED,\n                \"vvvv\": cls.COMPLETE,\n                \"vvvvv\": cls.DEBUG,\n            }\n            if value in string_map:\n                return string_map[value]\n            raise ValueError(f\"Invalid verbosity: {value}\")\n        else:\n            raise ValueError(f\"Invalid verbosity type: {type(value)}\")\n\n    def __bool__(self) -&gt; bool:\n        \"\"\"\n        Returns True if the verbosity level is not SILENT.\n        \"\"\"\n        return self != Verbosity.SILENT\n\n    def __lt__(self, other):\n        \"\"\"Less than comparison based on enum values.\"\"\"\n        if self.__class__ is other.__class__:\n            return self.value &lt; other.value\n        return NotImplemented\n\n    def __le__(self, other):\n        \"\"\"Less than or equal comparison based on enum values.\"\"\"\n        if self.__class__ is other.__class__:\n            return self.value &lt;= other.value\n        return NotImplemented\n\n    def __gt__(self, other):\n        \"\"\"Greater than comparison based on enum values.\"\"\"\n        if self.__class__ is other.__class__:\n            return self.value &gt; other.value\n        return NotImplemented\n\n    def __ge__(self, other):\n        \"\"\"Greater than or equal comparison based on enum values.\"\"\"\n        if self.__class__ is other.__class__:\n            return self.value &gt;= other.value\n        return NotImplemented\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation for logging and display.\"\"\"\n        return self.name\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Detailed representation for debugging.\"\"\"\n        return f\"Verbosity.{self.name}\"\n</file>\n  </file_contents>\n</project>\n\n</code>"
      }
    ],
    "temperature": null,
    "response_model": null,
    "provider": "google",
    "client_params": null,
    "stream": false,
    "verbose": 1
  },
  "input_tokens": 107247,
  "output_tokens": 772,
  "duration": 32.92867875099182,
  "timestamp": "2025-10-17T23:39:26.240885"
}